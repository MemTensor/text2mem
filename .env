# Text2Mem Environment Configuration / 环境配置
# Provider: mock (default for testing)

# Database Settings
TEXT2MEM_DB_PATH=./text2mem.db
TEXT2MEM_DB_WAL=true
TEXT2MEM_DB_TIMEOUT=30

# Provider Configuration
# Options: mock | ollama | openai
TEXT2MEM_PROVIDER=mock
TEXT2MEM_EMBEDDING_PROVIDER=mock
TEXT2MEM_GENERATION_PROVIDER=mock

# Model Settings (adjust based on provider)
TEXT2MEM_EMBEDDING_MODEL=nomic-embed-text
TEXT2MEM_GENERATION_MODEL=qwen2:0.5b
TEXT2MEM_TEMPERATURE=0.7
TEXT2MEM_MAX_TOKENS=512
TEXT2MEM_TOP_P=0.9

# OpenAI Settings (required if TEXT2MEM_PROVIDER=openai)
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_ORGANIZATION=

# Ollama Settings (required if TEXT2MEM_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434

# Request Settings
TEXT2MEM_REQUEST_TIMEOUT=60
TEXT2MEM_MAX_RETRIES=3
TEXT2MEM_BATCH_SIZE=10

# Hybrid Search Settings
TEXT2MEM_SEARCH_ALPHA=0.7
TEXT2MEM_SEARCH_BETA=0.3
TEXT2MEM_SEARCH_PHRASE_BONUS=0.2
TEXT2MEM_SEARCH_DEFAULT_LIMIT=10
TEXT2MEM_SEARCH_MAX_LIMIT=100
TEXT2MEM_SEARCH_DEFAULT_K=5

# Bench Test Settings
TEXT2MEM_BENCH_TIMEOUT=90
TEXT2MEM_BENCH_SPLIT=basic
TEXT2MEM_BENCH_MODE=auto
TEXT2MEM_BENCH_VERBOSE=false

# Bench Generation Settings
TEXT2MEM_BENCH_GEN_PROVIDER=openai
TEXT2MEM_BENCH_GEN_MODEL=gpt-4o-mini
TEXT2MEM_BENCH_GEN_TEMPERATURE=0.7
TEXT2MEM_BENCH_GEN_MAX_TOKENS=4000
TEXT2MEM_BENCH_GEN_TIMEOUT=120

# Async Bench Generation Settings
TEXT2MEM_BENCH_GEN_USE_ASYNC=true
TEXT2MEM_BENCH_GEN_MAX_CONCURRENT=5
TEXT2MEM_BENCH_GEN_RETRY_MAX=3
TEXT2MEM_BENCH_GEN_RETRY_DELAY=2
TEXT2MEM_BENCH_GEN_CHECKPOINT_BATCH=10

# Logging
TEXT2MEM_LOG_LEVEL=INFO
TEXT2MEM_LANG=en

# Advanced: JSON model configuration (optional)
# TEXT2MEM_MODELS={"openai":{"embedding":"text-embedding-3-small","generation":"gpt-4o"},"ollama":{"embedding":"nomic-embed-text","generation":"qwen2:0.5b","base_url":"http://localhost:11434"}}

