# moved from text2mem/models_service_providers.py
from __future__ import annotations
import logging
import os
from typing import Dict, Any, Optional, Union, List

from .models_service import (
    BaseEmbeddingModel,
    BaseGenerationModel,
    EmbeddingResult,
    GenerationResult,
    ModelsService,
)
from text2mem.core.config import Text2MemConfig, ModelConfig

logger = logging.getLogger("text2mem.models_service_providers")


class MockEmbeddingModel(BaseEmbeddingModel):
    def __init__(self, model_name: str = "mock-embedding"):
        self.model_name = model_name
        self.dimension = 384
        logger.info(f"âœ… MockåµŒå…¥æ¨¡åž‹åˆå§‹åŒ–: {model_name}")
    def embed_text(self, text: str) -> EmbeddingResult:
        import hashlib, random
        seed = int(hashlib.md5(text.encode()).hexdigest(), 16) % 10000
        random.seed(seed)
        embedding = [random.uniform(-1, 1) for _ in range(self.dimension)]
        length = sum(x * x for x in embedding) ** 0.5
        embedding = [x / length for x in embedding]
        tokens = len(text.split())
        return EmbeddingResult(text=text, embedding=embedding, model=self.model_name, tokens_used=tokens)
    def embed_batch(self, texts: List[str]) -> List[EmbeddingResult]:
        return [self.embed_text(text) for text in texts]
    def get_dimension(self) -> int:
        return self.dimension


class MockGenerationModel(BaseGenerationModel):
    def __init__(self, model_name: str = "mock-generation"):
        self.model_name = model_name
        logger.info(f"âœ… Mockç”Ÿæˆæ¨¡åž‹åˆå§‹åŒ–: {model_name}")
    def generate(self, prompt: str, **kwargs) -> GenerationResult:
        responses = {
            "summarize": "è¿™æ˜¯å¯¹æ–‡æœ¬çš„æ€»ç»“ï¼šæ–‡æœ¬è®¨è®ºäº†é‡è¦æ¦‚å¿µå’Œå…³é”®æ€æƒ³ã€‚",
            "label": "æŠ€æœ¯, åˆ›æ–°, ç ”ç©¶",
            "question": "è¿™æ˜¯å¯¹æ‚¨é—®é¢˜çš„æ¨¡æ‹Ÿå›žç­”ã€‚æˆ‘æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡åž‹ï¼Œä¸æä¾›çœŸå®žçš„AIåŠŸèƒ½ã€‚",
            "hello": "ä½ å¥½ï¼æˆ‘æ˜¯Text2Memçš„æ¨¡æ‹Ÿæ¨¡åž‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ¼”ç¤ºåŠŸèƒ½ï¼Œä½†ä¸æä¾›å®žé™…çš„AIèƒ½åŠ›ã€‚",
        }
        response = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ï¼Œç”¨äºŽæ¼”ç¤ºåŠŸèƒ½ã€‚åœ¨å®žé™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯çœŸå®žçš„AIç”Ÿæˆå†…å®¹ã€‚"
        for key, text in responses.items():
            if key.lower() in prompt.lower():
                response = text
                break
        prompt_tokens = len(prompt.split())
        completion_tokens = len(response.split())
        return GenerationResult(
            text=response,
            model=self.model_name,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens,
        )
    def generate_structured(self, prompt: str, schema: Dict[str, Any], **kwargs) -> GenerationResult:
        import json
        if "æ ‡ç­¾" in prompt or "label" in prompt.lower():
            result = {"labels": ["æŠ€æœ¯", "åˆ›æ–°", "ç ”ç©¶"]}
        elif "æ‘˜è¦" in prompt or "summary" in prompt.lower():
            result = {"summary": "è¿™æ˜¯ä¸€æ®µæ¨¡æ‹Ÿçš„æ–‡æœ¬æ‘˜è¦ã€‚"}
        else:
            if "type" in schema and schema["type"] == "object" and "properties" in schema:
                result = {}
                for key, prop in schema["properties"].items():
                    if prop.get("type") == "string":
                        result[key] = f"æ¨¡æ‹Ÿ{key}å€¼"
                    elif prop.get("type") == "number":
                        result[key] = 42
                    elif prop.get("type") == "boolean":
                        result[key] = True
                    elif prop.get("type") == "array":
                        result[key] = ["æ¨¡æ‹Ÿé¡¹ç›®1", "æ¨¡æ‹Ÿé¡¹ç›®2"]
                    else:
                        result[key] = None
            else:
                result = {"result": "æ¨¡æ‹Ÿç»“æž„åŒ–å“åº”"}
        response = json.dumps(result, ensure_ascii=False)
        prompt_tokens = len(prompt.split())
        completion_tokens = len(response.split())
        return GenerationResult(
            text=response,
            model=self.model_name,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens,
        )


class MockModelsService(ModelsService):
    def __init__(self, config: Optional[ModelConfig] = None):
        self.embedding_model = MockEmbeddingModel()
        self.generation_model = MockGenerationModel()
        logger.info("âœ… Mockæ¨¡åž‹æœåŠ¡åˆå§‹åŒ–å®Œæˆ")


def create_mock_models_service(config: Optional[Union[Text2MemConfig, Dict[str, Any]]] = None) -> MockModelsService:
    if config is None:
        config = {}
    elif isinstance(config, Text2MemConfig):
        config = config.model
    return MockModelsService(config)  # config currently unused


def create_ollama_models_service(config: Optional[Union[Text2MemConfig, ModelConfig, Dict[str, Any]]] = None) -> 'ModelsService':
    from .models_service_ollama import create_models_service_from_config
    if config is None:
        model_cfg: ModelConfig = ModelConfig.load_ollama_config()
    elif isinstance(config, Text2MemConfig):
        model_cfg = config.model
    elif isinstance(config, ModelConfig):
        model_cfg = config
    else:
        model_cfg = ModelConfig.load_ollama_config()
    return create_models_service_from_config(model_cfg)


def create_openai_models_service(config: Optional[Union[Text2MemConfig, ModelConfig, Dict[str, Any]]] = None) -> 'ModelsService':
    from .models_service_openai import create_openai_models_service as _create_openai_service
    if config is None:
        model_cfg: ModelConfig = ModelConfig.load_openai_config()
    elif isinstance(config, Text2MemConfig):
        model_cfg = config.model
    elif isinstance(config, ModelConfig):
        model_cfg = config
    else:
        model_cfg = ModelConfig.load_openai_config()
    return _create_openai_service(model_cfg)


def create_models_service(mode: str = "auto", config: Optional[Union[Text2MemConfig, ModelConfig, Dict[str, Any]]] = None) -> 'ModelsService':
    if isinstance(config, Text2MemConfig):
        model_cfg: ModelConfig = config.model
    elif isinstance(config, ModelConfig):
        model_cfg = config
    else:
        model_cfg = ModelConfig.from_env()
    if mode == "auto":
        env_mode = os.getenv("MODEL_SERVICE", "").lower()
        if env_mode in ("mock", "ollama", "openai"):
            mode = env_mode
        else:
            if model_cfg.embedding_provider == "openai" or model_cfg.generation_provider == "openai":
                mode = "openai"
            elif model_cfg.embedding_provider == "ollama" or model_cfg.generation_provider == "ollama":
                mode = "ollama"
            else:
                mode = "mock"
    logger.info(f"ðŸ”„ åˆ›å»ºæ¨¡åž‹æœåŠ¡ï¼š{mode} æ¨¡å¼")
    if mode == "mock":
        return create_mock_models_service()
    if mode == "ollama":
        model_cfg.embedding_provider = "ollama"
        model_cfg.generation_provider = "ollama"
        return create_ollama_models_service(model_cfg)
    if mode == "openai":
        model_cfg.embedding_provider = "openai"
        model_cfg.generation_provider = "openai"
        return create_openai_models_service(model_cfg)
    raise ValueError(f"æœªçŸ¥çš„æ¨¡åž‹æœåŠ¡æ¨¡å¼: {mode}")


def create_models_service_from_env() -> 'ModelsService':
    from .models_service_ollama import create_models_service_from_config
    model_cfg = ModelConfig.from_env()
    return create_models_service_from_config(model_cfg)
