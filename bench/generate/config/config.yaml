# LLM Client Configuration
# LLM客户端配置

llm:
  # Provider: openai, ollama, anthropic, or custom
  provider: openai
  
  # Model name
  model: gpt-4-turbo-preview
  
  # API credentials
  api_key: ${OPENAI_API_KEY}  # Use environment variable
  base_url: https://api.openai.com/v1
  
  # Generation parameters
  temperature: 0.7        # 0.0-2.0, higher = more creative
  max_tokens: 4000        # Maximum tokens in response
  top_p: 1.0              # Nucleus sampling
  frequency_penalty: 0.0  # Penalize repetition
  presence_penalty: 0.0   # Encourage diversity

# Generation settings
generation:
  # Batch size for each LLM call
  batch_size: 10
  
  # Retry on error
  retry_on_error: 3
  retry_delay: 2  # seconds
  
  # Output validation
  validate_output: true
  
  # Save intermediate results
  save_intermediate: true
  
  # Verbose logging
  verbose: true

# Output settings
output:
  # Output directory
  dir: bench/generate/output
  
  # Filename pattern
  pattern: "{scenario}_{lang}_{timestamp}_{stage}.{ext}"
  
  # Pretty print JSON
  pretty: true
  indent: 2

# Scenario-specific settings
scenarios:
  pkm:
    default_count: 5
    default_lang: zh
  
  tpm:
    default_count: 5
    default_lang: zh
  
  work:
    default_count: 5
    default_lang: zh
  
  daily:
    default_count: 3
    default_lang: zh

# Operation-specific settings
operations:
  encode:
    min_content_length: 20
    max_content_length: 500
  
  retrieve:
    require_prerequisites: true
    min_prerequisites: 2
  
  update:
    require_prerequisites: true
  
  delete:
    require_prerequisites: true
  
  summarize:
    layer: both  # Needs LLM
    min_content_length: 100
