"""
å¼‚æ­¥ç”Ÿæˆæ§åˆ¶å™¨
æ”¯æŒå¹¶å‘ç”Ÿæˆã€åŠ¨æ€é™æµã€å®æ—¶ä¿å­˜
"""
from __future__ import annotations

import asyncio
import json
import os
import time
from asyncio import Semaphore, Queue
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from bench.generate.src.generation_controller import GenerationController
from bench.generate.src.llm_client_async import AsyncLLMClient, create_async_llm_client
from bench.generate.src.llm_client import LLMConfig
from bench.generate.src.stage2_generator import IRSample
from bench.generate.src.stage3_generator import CompleteSample


class AsyncGenerationController(GenerationController):
    """å¼‚æ­¥ç”Ÿæˆæ§åˆ¶å™¨"""
    
    def __init__(self, *args, max_concurrent: Optional[int] = None, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è¯»å–å¹¶å‘æ•°
        if max_concurrent is None:
            max_concurrent = int(os.getenv("TEXT2MEM_BENCH_GEN_MAX_CONCURRENT", "5"))
        
        self.max_concurrent = max_concurrent
        self.semaphore = Semaphore(max_concurrent)
        
        # å†™å…¥é˜Ÿåˆ—ï¼ˆç¡®ä¿é¡ºåºå†™å…¥ï¼‰
        self.write_queue: Queue = Queue()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_time": 0,
        }
        
        self._log(f"ğŸš€ å¼‚æ­¥æ¨¡å¼: æœ€å¤§å¹¶å‘æ•° = {self.max_concurrent}")
    
    def run_async(self):
        """è¿è¡Œå¼‚æ­¥ç”Ÿæˆæµç¨‹"""
        # åˆ›å»ºäº‹ä»¶å¾ªç¯å¹¶è¿è¡Œ
        try:
            asyncio.run(self._run_async_impl())
        except KeyboardInterrupt:
            self._log("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
            self._log("ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ°æ–­ç‚¹")
            raise
        except Exception as e:
            self._log(f"\n\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _run_async_impl(self):
        """å¼‚æ­¥è¿è¡Œå®ç°"""
        self._log("\n" + "=" * 60)
        self._log(f"ğŸš€ å¼€å§‹å¼‚æ­¥ç”Ÿæˆ: {self.plan.name}")
        self._log("=" * 60)
        
        start_time = time.time()
        
        # Stage 1: NLæŒ‡ä»¤ç”Ÿæˆ
        stage1_output = None
        if self._should_run_stage("stage1"):
            self._log("\nğŸ“ Stage 1: ç”ŸæˆNLæŒ‡ä»¤...")
            stage1_output = self.run_stage1()  # Stage1 ä»ä½¿ç”¨åŒæ­¥ï¼ˆæ‰¹é‡ç”Ÿæˆï¼‰
            self._log(f"âœ… Stage 1 å®Œæˆ: {stage1_output}")
        else:
            self._log("\nâ­ï¸  Stage 1: å·²å®Œæˆ")
            stage1_output = self.checkpoint.output_files.get("stage1")
        
        # Stage 2: IR Schemaç”Ÿæˆï¼ˆå¼‚æ­¥ï¼‰
        stage2_output = None
        if self._should_run_stage("stage2"):
            self._log("\nğŸ—ï¸  Stage 2: å¼‚æ­¥ç”ŸæˆIR Schema...")
            stage2_output = await self._run_stage2_async(stage1_output)
            self._log(f"âœ… Stage 2 å®Œæˆ: {stage2_output}")
        else:
            self._log("\nâ­ï¸  Stage 2: å·²å®Œæˆ")
            stage2_output = self.checkpoint.output_files.get("stage2")
        
        # Stage 3: Expectedç”Ÿæˆï¼ˆå¼‚æ­¥ï¼‰
        stage3_output = None
        if self._should_run_stage("stage3"):
            self._log("\nğŸ¯ Stage 3: å¼‚æ­¥ç”ŸæˆExpected...")
            stage3_output = await self._run_stage3_async(stage2_output)
            self._log(f"âœ… Stage 3 å®Œæˆ: {stage3_output}")
        else:
            self._log("\nâ­ï¸  Stage 3: å·²å®Œæˆ")
        
        # å®Œæˆ
        elapsed = time.time() - start_time
        self._log("\n" + "=" * 60)
        self._log(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}ç§’")
        self._log(self._format_stats())
        self._log("=" * 60)
        
        # æ‰“å°æ‘˜è¦
        self._log("\n" + self.checkpoint_mgr.get_progress_summary())
    
    async def _run_stage2_async(self, stage1_output: Optional[str]) -> Optional[str]:
        """å¼‚æ­¥è¿è¡Œ Stage 2"""
        if not stage1_output or not Path(stage1_output).exists():
            self._log("   âŒ Stage 1è¾“å‡ºæœªæ‰¾åˆ°")
            return None
        
        # åŠ è½½Stage 1è¾“å‡ºï¼ˆæ”¯æŒ JSON å’Œ JSONL æ ¼å¼ï¼‰
        stage1_path = Path(stage1_output)
        nl_instructions = []
        
        if stage1_path.suffix == '.jsonl':
            # JSONL æ ¼å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
            with open(stage1_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        nl_instructions.append(json.loads(line))
        else:
            # JSON æ ¼å¼ï¼ˆæ—§ç‰ˆæœ¬å…¼å®¹ï¼‰
            with open(stage1_path, 'r', encoding='utf-8') as f:
                nl_instructions = json.load(f)
        
        self._log(f"   ğŸ“¥ åŠ è½½ {len(nl_instructions)} æ¡NLæŒ‡ä»¤")
        
        stage_name = "stage2"
        stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        if not stage_progress:
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                status="running",
                total_batches=len(nl_instructions),
                completed_batches=0,
            )
            stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
        output_file = self.run_dir / "stage2.jsonl"
        
        # å¦‚æœæœ‰ç°æœ‰è¾“å‡ºæ–‡ä»¶ä¸”æ­£åœ¨æ¢å¤ï¼Œç»§ç»­ä½¿ç”¨è¯¥æ–‡ä»¶
        if stage_progress.output_file and Path(stage_progress.output_file).exists():
            output_file = Path(stage_progress.output_file)
            self._log(f"   ğŸ“‚ ç»§ç»­ä½¿ç”¨ç°æœ‰æ–‡ä»¶: {output_file}")
        else:
            self._log(f"   ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                output_file=str(output_file),
            )
        
        # åˆ›å»ºå¼‚æ­¥LLMå®¢æˆ·ç«¯
        llm_config = LLMConfig.from_dict(self.plan.llm)
        async with create_async_llm_client(llm_config) as async_client:
            # å¯åŠ¨å†™å…¥åç¨‹
            writer_task = asyncio.create_task(
                self._file_writer_worker(output_file, stage_name)
            )
            
            # åˆ›å»ºç”Ÿæˆä»»åŠ¡
            tasks = []
            for idx, nl_instruction in enumerate(nl_instructions):
                # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬
                if idx < stage_progress.completed_batches:
                    continue
                
                task = self._generate_stage2_sample(
                    async_client,
                    nl_instruction,
                    idx,
                    len(nl_instructions),
                    stage_name,
                )
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            self._log(f"   ğŸš€ å¼€å§‹å¹¶å‘ç”Ÿæˆ ({self.max_concurrent} å¹¶å‘)...")
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç­‰å¾…å†™å…¥å®Œæˆ
            await self.write_queue.put(None)  # ç»“æŸä¿¡å·
            await writer_task
            
            # ç»Ÿè®¡ç»“æœ
            successes = sum(1 for r in results if not isinstance(r, Exception))
            failures = len(results) - successes
            
            self._log(f"\n   âœ… Stage 2å®Œæˆ: {successes} æˆåŠŸ, {failures} å¤±è´¥")
        
        # æ ‡è®°é˜¶æ®µå®Œæˆ
        self.checkpoint_mgr.update_stage_progress(
            stage_name,
            status="completed",
        )
        
        return str(output_file)
    
    async def _run_stage3_async(self, stage2_output: Optional[str]) -> Optional[str]:
        """å¼‚æ­¥è¿è¡Œ Stage 3"""
        if not stage2_output or not Path(stage2_output).exists():
            self._log("   âŒ Stage 2è¾“å‡ºæœªæ‰¾åˆ°")
            return None
        
        # åŠ è½½Stage 2è¾“å‡º
        ir_samples_dict = []
        with open(stage2_output, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        ir_samples_dict.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
        
        self._log(f"   ğŸ“¥ åŠ è½½ {len(ir_samples_dict)} ä¸ªIRæ ·æœ¬")
        
        stage_name = "stage3"
        stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        if not stage_progress:
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                status="running",
                total_batches=len(ir_samples_dict),
                completed_batches=0,
            )
            stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
        output_file = self.run_dir / "stage3.jsonl"
        
        # å¦‚æœæœ‰ç°æœ‰è¾“å‡ºæ–‡ä»¶ä¸”æ­£åœ¨æ¢å¤ï¼Œç»§ç»­ä½¿ç”¨è¯¥æ–‡ä»¶
        if stage_progress.output_file and Path(stage_progress.output_file).exists():
            output_file = Path(stage_progress.output_file)
            self._log(f"   ğŸ“‚ ç»§ç»­ä½¿ç”¨ç°æœ‰æ–‡ä»¶: {output_file}")
        else:
            self._log(f"   ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                output_file=str(output_file),
            )
        
        # åˆ›å»ºå¼‚æ­¥LLMå®¢æˆ·ç«¯
        llm_config = LLMConfig.from_dict(self.plan.llm)
        async with create_async_llm_client(llm_config) as async_client:
            # å¯åŠ¨å†™å…¥åç¨‹
            writer_task = asyncio.create_task(
                self._file_writer_worker(output_file, stage_name)
            )
            
            # åˆ›å»ºç”Ÿæˆä»»åŠ¡
            tasks = []
            for idx, sample_dict in enumerate(ir_samples_dict):
                # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬
                if idx < stage_progress.completed_batches:
                    continue
                
                task = self._generate_stage3_sample(
                    async_client,
                    sample_dict,
                    idx,
                    len(ir_samples_dict),
                    stage_name,
                )
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            self._log(f"   ğŸš€ å¼€å§‹å¹¶å‘ç”Ÿæˆ ({self.max_concurrent} å¹¶å‘)...")
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç­‰å¾…å†™å…¥å®Œæˆ
            await self.write_queue.put(None)  # ç»“æŸä¿¡å·
            await writer_task
            
            # ç»Ÿè®¡ç»“æœ
            successes = sum(1 for r in results if not isinstance(r, Exception))
            failures = len(results) - successes
            
            self._log(f"\n   âœ… Stage 3å®Œæˆ: {successes} æˆåŠŸ, {failures} å¤±è´¥")
        
        # æ ‡è®°é˜¶æ®µå®Œæˆ
        self.checkpoint_mgr.update_stage_progress(
            stage_name,
            status="completed",
        )
        
        return str(output_file)
    
    async def _generate_stage2_sample(
        self,
        async_client: AsyncLLMClient,
        nl_instruction: Dict[str, Any],
        idx: int,
        total: int,
        stage_name: str,
    ) -> Optional[IRSample]:
        """å¼‚æ­¥ç”Ÿæˆå•ä¸ªStage 2æ ·æœ¬"""
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            start_time = time.time()
            
            try:
                self._log(f"   ğŸ“¦ [{idx + 1}/{total}] å¼€å§‹ç”Ÿæˆ...")
                
                # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆ
                sample = await self._call_stage2_generator_async(async_client, nl_instruction)
                
                if sample:
                    # æ·»åŠ åˆ°å†™å…¥é˜Ÿåˆ—
                    sample_dict = {
                        "id": sample.id,
                        "class": sample.class_info,
                        "nl": sample.nl,
                        "prerequisites": sample.prerequisites,
                        "schema_list": sample.schema_list,
                        "init_db": sample.init_db,
                        "notes": sample.notes,
                    }
                    
                    await self.write_queue.put((idx, sample_dict, None, stage_name))
                    
                    elapsed = time.time() - start_time
                    self._log(f"      âœ… [{idx + 1}/{total}] å®Œæˆ ({elapsed:.1f}s)")
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats["successful_requests"] += 1
                    self.stats["total_time"] += elapsed
                    
                    return sample
                    
            except Exception as e:
                self._log(f"      âŒ [{idx + 1}/{total}] å¤±è´¥: {e}")
                
                # è®°å½•é”™è¯¯
                await self.write_queue.put((idx, None, str(e), stage_name))
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats["failed_requests"] += 1
                
                return None
            finally:
                self.stats["total_requests"] += 1
    
    async def _generate_stage3_sample(
        self,
        async_client: AsyncLLMClient,
        sample_dict: Dict[str, Any],
        idx: int,
        total: int,
        stage_name: str,
    ) -> Optional[CompleteSample]:
        """å¼‚æ­¥ç”Ÿæˆå•ä¸ªStage 3æ ·æœ¬"""
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            start_time = time.time()
            
            try:
                self._log(f"   ğŸ“¦ [{idx + 1}/{total}] å¼€å§‹ç”Ÿæˆ...")
                
                # è½¬æ¢ä¸ºIRSample
                ir_sample = IRSample(
                    id=sample_dict.get("id", ""),
                    class_info=sample_dict.get("class", {}),
                    nl=sample_dict.get("nl", {}),
                    prerequisites=sample_dict.get("prerequisites", []),
                    schema_list=sample_dict.get("schema_list", []),
                    init_db=sample_dict.get("init_db"),
                    notes=sample_dict.get("notes", ""),
                )
                
                # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆ
                complete_sample = await self._call_stage3_generator_async(async_client, ir_sample)
                
                if complete_sample:
                    # æ·»åŠ åˆ°å†™å…¥é˜Ÿåˆ—
                    complete_dict = {
                        "id": complete_sample.id,
                        "class": complete_sample.class_info,
                        "nl": complete_sample.nl,
                        "prerequisites": complete_sample.prerequisites,
                        "schema_list": complete_sample.schema_list,
                        "init_db": complete_sample.init_db,
                        "expected": complete_sample.expected,
                        "notes": complete_sample.notes,
                    }
                    
                    await self.write_queue.put((idx, complete_dict, None, stage_name))
                    
                    elapsed = time.time() - start_time
                    self._log(f"      âœ… [{idx + 1}/{total}] å®Œæˆ ({elapsed:.1f}s)")
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats["successful_requests"] += 1
                    self.stats["total_time"] += elapsed
                    
                    return complete_sample
                    
            except Exception as e:
                self._log(f"      âŒ [{idx + 1}/{total}] å¤±è´¥: {e}")
                
                # è®°å½•é”™è¯¯
                await self.write_queue.put((idx, None, str(e), stage_name))
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats["failed_requests"] += 1
                
                return None
            finally:
                self.stats["total_requests"] += 1
    
    async def _call_stage2_generator_async(
        self,
        async_client: AsyncLLMClient,
        nl_instruction: Dict[str, Any],
    ) -> Optional[IRSample]:
        """è°ƒç”¨ Stage2 ç”Ÿæˆå™¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                # æ„å»ºpromptï¼ˆä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åï¼‰
                prompt = self.stage2_generator._build_single_prompt(nl_instruction)
                
                # å¼‚æ­¥è°ƒç”¨LLM
                response = await async_client.generate(prompt)
                
                # è§£æå“åº”ï¼ˆä¿®å¤ï¼šä¼ é€’æ­£ç¡®çš„å‚æ•°ï¼‰
                sample = self.stage2_generator._parse_response(response.content, nl_instruction)
                
                if sample:
                    # éªŒè¯æ ·æœ¬ï¼ˆä¿®å¤ï¼šä¼ é€’batchå‚æ•°ï¼ŒNoneè¡¨ç¤ºå•ä¸ªæ ·æœ¬éªŒè¯ï¼‰
                    errors = self.stage2_generator.validate_samples([sample], None)
                    if not errors:
                        return sample
                    
                    if attempt < max_attempts - 1:
                        await asyncio.sleep(1)
                        continue
                
            except Exception as e:
                if attempt < max_attempts - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                raise
        
        return None
    
    async def _call_stage3_generator_async(
        self,
        async_client: AsyncLLMClient,
        ir_sample: IRSample,
    ) -> Optional[CompleteSample]:
        """è°ƒç”¨ Stage3 ç”Ÿæˆå™¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                # æ„å»ºpromptï¼ˆä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åï¼‰
                prompt = self.stage3_generator._build_single_prompt(ir_sample)
                
                # å¼‚æ­¥è°ƒç”¨LLM
                response = await async_client.generate(prompt)
                
                # è§£æå“åº”
                complete_sample = self.stage3_generator._parse_response(
                    response.content,
                    ir_sample,
                )
                
                if complete_sample:
                    # éªŒè¯æ ·æœ¬ï¼ˆä¿®å¤ï¼šä¼ é€’batchå‚æ•°ï¼ŒNoneè¡¨ç¤ºå•ä¸ªæ ·æœ¬éªŒè¯ï¼‰
                    errors = self.stage3_generator.validate_samples([complete_sample], None)
                    if not errors:
                        return complete_sample
                    
                    if attempt < max_attempts - 1:
                        await asyncio.sleep(1)
                        continue
                
            except Exception as e:
                if attempt < max_attempts - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                raise
        
        return None
    
    async def _file_writer_worker(self, output_file: Path, stage_name: str):
        """æ–‡ä»¶å†™å…¥åç¨‹ï¼ˆç¡®ä¿é¡ºåºå†™å…¥ï¼Œæ‰¹é‡æ›´æ–°checkpointï¼‰"""
        checkpoint_batch_size = int(os.getenv("TEXT2MEM_BENCH_GEN_CHECKPOINT_BATCH", "10"))
        write_count = 0
        last_idx = -1
        
        with open(output_file, 'a', encoding='utf-8') as f:
            while True:
                item = await self.write_queue.get()
                
                if item is None:  # ç»“æŸä¿¡å·
                    # æœ€åæ›´æ–°checkpointï¼ˆå¦‚æœæœ‰æœªä¿å­˜çš„è¿›åº¦ï¼‰
                    if write_count > 0 and last_idx >= 0:
                        self.checkpoint_mgr.update_stage_progress(
                            stage_name,
                            completed_batches=last_idx + 1,
                        )
                    break
                
                idx, sample_dict, error, stage = item
                
                if sample_dict:
                    # å†™å…¥æ ·æœ¬
                    f.write(json.dumps(sample_dict, ensure_ascii=False) + '\n')
                    f.flush()
                    
                    write_count += 1
                    last_idx = idx
                    
                    # æ‰¹é‡æ›´æ–°checkpointï¼ˆå‡å°‘ç£ç›˜I/Oï¼‰
                    if write_count % checkpoint_batch_size == 0:
                        self.checkpoint_mgr.update_stage_progress(
                            stage,
                            completed_batches=idx + 1,
                        )
                elif error:
                    # é”™è¯¯ç«‹å³è®°å½•
                    self.checkpoint_mgr.record_error(stage, idx, error)
                
                self.write_queue.task_done()
    
    def _format_stats(self) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats["total_requests"] == 0:
            return ""
        
        avg_time = self.stats["total_time"] / self.stats["successful_requests"] \
            if self.stats["successful_requests"] > 0 else 0
        
        success_rate = (self.stats["successful_requests"] / self.stats["total_requests"]) * 100
        
        return f"""
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»è¯·æ±‚æ•°: {self.stats["total_requests"]}
   æˆåŠŸ: {self.stats["successful_requests"]}
   å¤±è´¥: {self.stats["failed_requests"]}
   æˆåŠŸç‡: {success_rate:.1f}%
   å¹³å‡è€—æ—¶: {avg_time:.2f}s/æ ·æœ¬
   æ€»è€—æ—¶: {self.stats["total_time"]:.1f}s
"""
