"""
Stage 3 Generator - Expected Result Generator
Adds the `expected` field (assertions, ranking, triggers) to IR samples.
"""
from __future__ import annotations

import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from bench.generate.src.llm_client import LLMClient, LLMConfig
from bench.generate.src.plan_loader import GenerationPlan
from bench.generate.src.stage2_generator import IRSample


@dataclass
class CompleteSample:
    """Complete test sample (output of Stage 3)"""
    id: str
    class_info: Dict[str, str]
    nl: Dict[str, str]
    prerequisites: List[Dict[str, Any]]
    schema_list: List[Dict[str, Any]]
    init_db: Optional[Any]
    expected: Dict[str, Any]
    notes: str


class Stage3Generator:
    """Stage 3: Expected Result Generator"""
    
    def __init__(
        self,
        llm_client: LLMClient,
        plan: GenerationPlan,
        prompts_dir: Path,
        llm_config: LLMConfig,
    ):
        self.llm_client = llm_client
        self.plan = plan
        self.prompts_dir = prompts_dir
        self.llm_config = llm_config
        
        # Load prompt templates (supports both Chinese and English)
        self.prompt_templates = {
            'zh': self._load_prompt_template("stage3_expected_generation.md"),
            'en': self._load_prompt_template("en_stage3_expected_generation.md"),
        }
    
    def _log(self, message: str, level: str = "INFO", verbose_only: bool = False):
        """Simple logging helper"""
        if verbose_only:
            # TODO: Add verbose flag
            return
        
        prefix = {
            "INFO": "‚ÑπÔ∏è ",
            "WARNING": "‚ö†Ô∏è ",
            "ERROR": "‚ùå",
            "SUCCESS": "‚úÖ"
        }.get(level, "")
        print(f"   {prefix} {message}")
    
    def _load_prompt_template(self, filename: str) -> str:
        """Load Stage 3 prompt template file
        
        Args:
            filename: Template file name.
            
        Returns:
            Template file content.
        """
        prompt_file = self.prompts_dir / filename
        
        if not prompt_file.exists():
            raise FileNotFoundError(f"Prompt template not found: {prompt_file}")
        
        return prompt_file.read_text(encoding="utf-8")
    
    def generate_single(self, ir_sample: IRSample) -> Optional[CompleteSample]:
        """
        Generate the `expected` field for a single IR sample.
        Supports multiple retry attempts.
        
        Args:
            ir_sample: An IR sample generated by Stage 2.
        
        Returns:
            A complete test sample, or None if generation failed.
        """
        max_attempts = 3  # Try up to 3 times
        
        for attempt in range(max_attempts):
            try:
                # Build prompt
                prompt = self._build_single_prompt(ir_sample)
                
                # Call LLM
                response = self.llm_client.generate(
                    prompt=prompt,
                    temperature=0.3,  # Lower temperature for consistency
                    max_tokens=3000,
                )
                
                # Parse response
                sample = self._parse_response(response.content, ir_sample)
                
                if sample:
                    # Validate generated sample
                    errors = self.validate_samples([sample], None)
                    if not errors:
                        if attempt > 0:
                            print(f"      ‚úÖ Attempt {attempt + 1} succeeded")
                        return sample
                    else:
                        if attempt < max_attempts - 1:
                            print(f"      ‚ö†Ô∏è  Attempt {attempt + 1} validation failed: {errors[0]}")
                            print(f"      üîÑ Retrying...")
                            continue
                else:
                    if attempt < max_attempts - 1:
                        print(f"      ‚ö†Ô∏è  Attempt {attempt + 1} failed to parse response")
                        print(f"      üîÑ Retrying...")
                        continue
                    
            except Exception as e:
                if attempt < max_attempts - 1:
                    print(f"      ‚ö†Ô∏è  Attempt {attempt + 1} encountered an error: {e}")
                    print(f"      üîÑ Retrying...")
                    import time
                    time.sleep(1)
                    continue
                else:
                    print(f"      ‚ùå All attempts failed")
        
        # All attempts failed
        return None
    
    def _build_single_prompt(self, ir_sample: IRSample) -> str:
        """Build prompt for a single IR sample using template file"""
        # Construct JSONL-style string for the IR sample
        ir_json = json.dumps({
            "id": ir_sample.id,
            "class": ir_sample.class_info,
            "nl": ir_sample.nl,
            "prerequisites": ir_sample.prerequisites,
            "schema_list": ir_sample.schema_list,
            "init_db": ir_sample.init_db,
            "notes": ir_sample.notes,
        }, ensure_ascii=False)
        
        # Determine main operation
        main_op = "unknown"
        if ir_sample.schema_list:
            main_op = ir_sample.schema_list[0].get("op", "unknown")
        
        # Determine language
        lang = ir_sample.class_info.get('lang', 'zh') if getattr(ir_sample, 'class_info', None) else 'zh'
        
        # Select template
        prompt_template = self.prompt_templates.get(lang, self.prompt_templates['zh'])
        
        # Replace placeholders
        prompt = prompt_template
        prompt = prompt.replace('{test_samples_jsonl}', ir_json)
        prompt = prompt.replace('{ir_json}', ir_json)
        prompt = prompt.replace('{main_op}', main_op)
        
        return prompt
    
    def _parse_response(self, content: str, ir_sample: IRSample) -> Optional[CompleteSample]:
        """Parse LLM response ‚Äî extract and repair JSON intelligently"""
        original_content = content
        content = content.strip()
        
        # Step 1: Clean markdown and wrappers
        content = re.sub(r'```json\s*', '', content)
        content = re.sub(r'```\s*', '', content)
        content = content.strip()
        
        # Remove common description prefixes
        patterns = [
            r'^[^{]*?(?:generate|ËæìÂá∫|ÁªìÊûú|sample|output|result)[^{]*?[:Ôºö]\s*',
            r'^[^{]*?(?:‰ª•‰∏ã|following|below)[^{]*?[:Ôºö]\s*',
        ]
        for pattern in patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        content = content.strip()
        
        # Step 2: Attempt to parse JSON
        data = None
        parse_method = None
        
        try:
            data = json.loads(content)
            parse_method = "direct"
        except json.JSONDecodeError:
            try:
                from json import JSONDecoder
                decoder = JSONDecoder()
                data, idx = decoder.raw_decode(content)
                parse_method = "raw_decode"
                
                if idx < len(content.strip()):
                    remaining = content[idx:].strip()
                    if remaining and len(remaining) > 10:
                        self._log(f"      ‚ö†Ô∏è  Extra content detected after JSON (ignored): {remaining[:80]}...", verbose_only=True)
            except (json.JSONDecodeError, ValueError):
                data = self._extract_json_by_braces(content)
                if data:
                    parse_method = "brace_matching"
        
        if data is None:
            print(f"      ‚ùå JSON parsing completely failed")
            print(f"      Original content length: {len(original_content)} characters")
            print(f"      First 200 chars: {original_content[:200]}")
            self._save_failed_response(original_content, ir_sample, "stage3")
            return None
        
        # Step 3: Validate 'expected' field
        if "expected" not in data:
            print(f"      ‚ö†Ô∏è  Missing 'expected' field in JSON")
            print(f"      Available keys: {list(data.keys())}")
            return None
        
        # Step 4: Build CompleteSample
        try:
            sample = CompleteSample(
                id=data.get("id", ir_sample.id),
                class_info=data.get("class", ir_sample.class_info),
                nl=data.get("nl", ir_sample.nl),
                prerequisites=data.get("prerequisites", ir_sample.prerequisites),
                schema_list=data.get("schema_list", ir_sample.schema_list),
                init_db=data.get("init_db", ir_sample.init_db),
                expected=data.get("expected", {}),
                notes=data.get("notes", ir_sample.notes),
            )
            
            self._log(f"      ‚úÖ Successfully parsed (method: {parse_method})", verbose_only=True)
            return sample
            
        except Exception as e:
            print(f"      ‚ùå Failed to construct CompleteSample: {e}")
            return None
    
    def _extract_json_by_braces(self, content: str) -> Optional[Dict]:
        """Extract JSON object by matching braces and attempt to repair"""
        start = content.find('{')
        if start == -1:
            return None
        
        json_str = self._extract_balanced_json(content, start)
        if not json_str:
            return None
        
        for attempt in range(6):
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                if attempt == 0:
                    json_str = re.sub(r'//.*', '', json_str)
                    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
                elif attempt == 1:
                    json_str = re.sub(r',\s*}', '}', json_str)
                    json_str = re.sub(r',\s*]', ']', json_str)
                elif attempt == 2:
                    if '}}]' in json_str and '"expected":[{' in json_str:
                        json_str = re.sub(r'(\}\})\]\s*$', r'\1}]', json_str)
                        print(f"      üîß Fixed missing closing bracket in 'expected'")
                elif attempt == 3:
                    json_str = self._auto_complete_braces(json_str)
                elif attempt == 4:
                    json_str = re.sub(r'}\s*{', '},{', json_str)
                else:
                    print(f"      ‚ö†Ô∏è  All JSON repair attempts failed: {e}")
                    if hasattr(e, "pos") and e.pos < len(json_str):
                        start_show = max(0, e.pos - 50)
                        end_show = min(len(json_str), e.pos + 50)
                        print(f"      Error near: ...{json_str[start_show:end_show]}...")
                    return None
        
        return None
    
    def _extract_balanced_json(self, content: str, start: int) -> Optional[str]:
        """Extract balanced JSON substring"""
        brace_count = 0
        bracket_count = 0
        in_string = False
        escape_next = False
        
        for i in range(start, len(content)):
            char = content[i]
            
            if escape_next:
                escape_next = False
                continue
            if char == '\\':
                escape_next = True
                continue
            if char == '"':
                in_string = not in_string
                continue
            
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                elif char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                
                if brace_count == 0 and bracket_count == 0 and i > start:
                    return content[start:i+1]
        
        return content[start:]
    
    def _auto_complete_braces(self, json_str: str) -> str:
        """Automatically complete missing braces/brackets"""
        brace_count = 0
        bracket_count = 0
        in_string = False
        escape_next = False
        
        for char in json_str:
            if escape_next:
                escape_next = False
                continue
            if char == '\\':
                escape_next = True
                continue
            if char == '"':
                in_string = not in_string
                continue
            
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                elif char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
        
        result = json_str
        if bracket_count > 0:
            result += ']' * bracket_count
            print(f"      üîß Auto-completed {bracket_count} closing square brackets ]")
        if brace_count > 0:
            result += '}' * brace_count
            print(f"      üîß Auto-completed {brace_count} closing curly braces }}")
        
        return result
    
    def _save_failed_response(self, content: str, ir_sample: IRSample, stage: str):
        """Save failed LLM response for debugging"""
        try:
            from pathlib import Path
            from datetime import datetime
            
            log_dir = Path("bench/generate/output/failed_responses")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            sample_id = ir_sample.id
            filename = f"failed_{stage}_{sample_id}_{timestamp}.txt"
            
            log_file = log_dir / filename
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"Failed to parse LLM response - {stage.upper()}\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"Sample ID: {sample_id}\n")
                f.write(f"Timestamp: {timestamp}\n")
                f.write(f"Content length: {len(content)} characters\n\n")
                f.write("=" * 80 + "\nOriginal response:\n")
                f.write("=" * 80 + "\n")
                f.write(content)
                f.write("\n\n")
                f.write("=" * 80 + "\nInput IR sample:\n")
                f.write("=" * 80 + "\n")
                f.write(json.dumps({
                    "id": ir_sample.id,
                    "class": ir_sample.class_info,
                    "nl": ir_sample.nl,
                    "prerequisites": ir_sample.prerequisites,
                    "schema_list": ir_sample.schema_list,
                }, ensure_ascii=False, indent=2))
            
            print(f"      üíæ Saved failed response to: {log_file}")
            
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Error while saving failed response: {e}")
    
    def validate_samples(
        self,
        samples: List[CompleteSample],
        batch: Any,
    ) -> List[str]:
        """Validate complete samples"""
        errors = []
        
        for idx, sample in enumerate(samples):
            if not sample.expected:
                errors.append(f"sample {idx}: 'expected' field is empty")
                continue
            
            if "assertions" not in sample.expected:
                errors.append(f"sample {idx}: missing 'assertions' in expected")
            
            if "ranking" not in sample.expected:
                errors.append(f"sample {idx}: missing 'ranking' in expected")
            
            if "triggers" not in sample.expected:
                errors.append(f"sample {idx}: missing 'triggers' in expected")
            
            assertions = sample.expected.get("assertions", [])
            if not isinstance(assertions, list):
                errors.append(f"sample {idx}: 'assertions' should be a list")
            
            for ass_idx, assertion in enumerate(assertions):
                if not isinstance(assertion, dict):
                    errors.append(f"sample {idx}, assertion {ass_idx}: should be an object")
                    continue
                
                required = ["name", "select", "expect"]
                for field in required:
                    if field not in assertion:
                        errors.append(f"sample {idx}, assertion {ass_idx}: missing '{field}' field")
        
        return errors
