"""
Stage 1 Generator - è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå™¨
æ ¹æ®åœºæ™¯å’Œæ“ä½œç”ŸæˆçœŸå®çš„ç”¨æˆ·æŒ‡ä»¤
"""
from __future__ import annotations

import json
import yaml
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from bench.generate.src.llm_client import LLMClient
from bench.generate.src.plan_loader import TaskBatch, GenerationPlan


@dataclass
class NLInstruction:
    """è‡ªç„¶è¯­è¨€æŒ‡ä»¤"""
    instruction: str
    context: str
    classification: Dict[str, str]
    scenario_info: Dict[str, Any]


class Stage1Generator:
    """Stage 1: NLæŒ‡ä»¤ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        llm_client: LLMClient,
        plan: GenerationPlan,
        seeds_dir: Path,
    ):
        self.llm_client = llm_client
        self.plan = plan
        self.seeds_dir = seeds_dir
        
        # åŠ è½½promptæ¨¡æ¿
        self.prompt_template = self._load_prompt_template()
        
        # åŠ è½½seedsæ•°æ®
        self.scenarios_config = self._load_scenarios()
        self.operations_config = self._load_operations()
    
    def _load_prompt_template(self) -> str:
        """åŠ è½½promptæ¨¡æ¿"""
        template_file = self.seeds_dir.parent / "prompts" / "stage1_nl_generation.md"
        
        if not template_file.exists():
            raise FileNotFoundError(f"Promptæ¨¡æ¿æœªæ‰¾åˆ°: {template_file}")
        
        with open(template_file, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_scenarios(self) -> Dict[str, Any]:
        """åŠ è½½åœºæ™¯é…ç½®"""
        scenarios_file = self.seeds_dir / "scenarios.yaml"
        
        if not scenarios_file.exists():
            return {}
        
        with open(scenarios_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        return data.get("scenarios", {})
    
    def _load_operations(self) -> Dict[str, Any]:
        """åŠ è½½æ“ä½œé…ç½®"""
        operations_file = self.seeds_dir / "operations.yaml"
        
        if not operations_file.exists():
            return {}
        
        with open(operations_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        return data.get("operations", {})
    
    def generate_batch(self, batch: TaskBatch) -> List[NLInstruction]:
        """
        ç”Ÿæˆä¸€æ‰¹NLæŒ‡ä»¤
        æ”¯æŒå¤šæ¬¡é‡è¯•
        
        Args:
            batch: ä»»åŠ¡æ‰¹æ¬¡
            
        Returns:
            NLæŒ‡ä»¤åˆ—è¡¨
        """
        max_attempts = 3  # æœ€å¤šå°è¯•3æ¬¡
        
        for attempt in range(max_attempts):
            try:
                # æ„å»ºprompt
                prompt = self._build_prompt(batch)
                
                # è°ƒç”¨LLM
                response = self.llm_client.generate(
                    prompt=prompt,
                    temperature=0.7,
                    max_tokens=4000,
                )
                
                # è§£æå“åº”
                instructions = self._parse_response(response.content, batch)
                
                if instructions:
                    # éªŒè¯
                    errors = self.validate_instructions(instructions, batch)
                    
                    # å¦‚æœæ²¡æœ‰ä¸¥é‡é”™è¯¯ï¼Œæˆ–è€…æ•°é‡è¶³å¤Ÿï¼Œå°±æ¥å—
                    if not errors or len(instructions) >= batch.count * 0.8:
                        if attempt > 0:
                            print(f"      âœ… ç¬¬{attempt + 1}æ¬¡å°è¯•æˆåŠŸ")
                        return instructions
                    else:
                        # éªŒè¯å¤±è´¥å¤ªå¤šï¼Œé‡è¯•
                        if attempt < max_attempts - 1:
                            print(f"      âš ï¸  ç¬¬{attempt + 1}æ¬¡å°è¯•è´¨é‡ä¸è¶³")
                            print(f"      ğŸ”„ é‡è¯•ä¸­...")
                            continue
                else:
                    # è§£æå¤±è´¥ï¼Œé‡è¯•
                    if attempt < max_attempts - 1:
                        print(f"      âš ï¸  ç¬¬{attempt + 1}æ¬¡å°è¯•è§£æå¤±è´¥")
                        print(f"      ğŸ”„ é‡è¯•ä¸­...")
                        continue
                
            except Exception as e:
                if attempt < max_attempts - 1:
                    print(f"      âš ï¸  ç¬¬{attempt + 1}æ¬¡å°è¯•å‡ºé”™: {e}")
                    print(f"      ğŸ”„ é‡è¯•ä¸­...")
                    import time
                    time.sleep(2)
                    continue
                else:
                    raise
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼‰
        return []
    
    def _build_prompt(self, batch: TaskBatch) -> str:
        """æ„å»ºç”Ÿæˆprompt"""
        # è·å–åœºæ™¯å’Œæ“ä½œé…ç½®
        scenario_config = self.scenarios_config.get(batch.scenario, {})
        operation_config = self.operations_config.get(batch.operation, {})
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºworkflowæ‰¹æ¬¡
        is_workflow = batch.structures and "workflow" in batch.structures
        workflow_count = batch.structures.count("workflow") if batch.structures else 0
        
        # æ„å»ºæ“ä½œè¡¨è¾¾æ–¹å¼
        expressions = operation_config.get("expressions_zh", [])
        operation_expressions = "\n".join([f"- {expr}" for expr in expressions[:5]])
        
        # å¡«å……æ¨¡æ¿
        prompt = self.prompt_template
        
        # æ›¿æ¢å˜é‡
        replacements = {
            "{count}": str(batch.count),
            "{operation}": batch.operation,
            "{operation_name}": operation_config.get("name", batch.operation),
            "{operation_description}": operation_config.get("description", ""),
            "{operation_expressions}": operation_expressions,
            "{scenario}": scenario_config.get("name", batch.scenario),
            "{scenario_description}": scenario_config.get("description", ""),
            "{lang}": batch.lang,
            "{min_context_length}": str(self.plan.min_context_length),
            "{max_context_length}": str(self.plan.max_context_length),
            "{context_length_range}": f"{self.plan.min_context_length}-{self.plan.max_context_length}",
        }
        
        for key, value in replacements.items():
            prompt = prompt.replace(key, value)
        
        # æ·»åŠ workflowç‰¹æ®Šè¯´æ˜
        if is_workflow and workflow_count > 0:
            prompt += f"\n\nâš ï¸ **ç‰¹åˆ«è¦æ±‚**: æœ¬æ‰¹æ¬¡éœ€è¦ç”Ÿæˆ {workflow_count} ä¸ªworkflowç±»å‹çš„æ ·æœ¬ï¼ˆæ˜ç¡®åŒ…å«3+æ­¥éª¤çš„æµç¨‹æŒ‡ä»¤ï¼‰ã€‚"
        
        return prompt
    
    def _parse_response(self, content: str, batch: TaskBatch) -> List[NLInstruction]:
        """è§£æLLMå“åº”"""
        # æå–JSONå†…å®¹
        content = content.strip()
        
        # å°è¯•æŸ¥æ‰¾JSONæ•°ç»„
        start = content.find('[')
        end = content.rfind(']')
        
        if start == -1 or end == -1:
            print(f"      âš ï¸  æœªæ‰¾åˆ°JSONæ•°ç»„")
            return []
        
        json_str = content[start:end+1]
        
        try:
            data = json.loads(json_str)
            
            if not isinstance(data, list):
                print(f"      âš ï¸  å“åº”ä¸æ˜¯æ•°ç»„")
                return []
            
            # è½¬æ¢ä¸ºNLInstructionå¯¹è±¡
            instructions = []
            for item in data:
                try:
                    instruction = NLInstruction(
                        instruction=item.get("instruction", ""),
                        context=item.get("context", ""),
                        classification=item.get("classification", {}),
                        scenario_info=item.get("scenario_info", {}),
                    )
                    instructions.append(instruction)
                except Exception as e:
                    print(f"      âš ï¸  è§£ææŒ‡ä»¤å¤±è´¥: {e}")
                    continue
            
            return instructions
            
        except json.JSONDecodeError as e:
            print(f"      âš ï¸  JSONè§£æå¤±è´¥: {e}")
            return []
    
    def validate_instructions(
        self,
        instructions: List[NLInstruction],
        batch: TaskBatch,
    ) -> List[str]:
        """
        éªŒè¯ç”Ÿæˆçš„æŒ‡ä»¤
        
        Returns:
            é”™è¯¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™éªŒè¯é€šè¿‡
        """
        errors = []
        
        for idx, instruction in enumerate(instructions):
            # éªŒè¯å¿…å¡«å­—æ®µ
            if not instruction.instruction:
                errors.append(f"æ ·æœ¬{idx}: instructionä¸ºç©º")
            
            if not instruction.context:
                errors.append(f"æ ·æœ¬{idx}: contextä¸ºç©º")
            
            # éªŒè¯contexté•¿åº¦
            context_len = len(instruction.context)
            if context_len < self.plan.min_context_length:
                errors.append(f"æ ·æœ¬{idx}: contexté•¿åº¦{context_len}å°äºæœ€å°å€¼{self.plan.min_context_length}")
            
            # éªŒè¯classification
            if not instruction.classification:
                errors.append(f"æ ·æœ¬{idx}: classificationä¸ºç©º")
            else:
                required_fields = ["instruction_type", "structure", "lang"]
                for field in required_fields:
                    if field not in instruction.classification:
                        errors.append(f"æ ·æœ¬{idx}: classificationç¼ºå°‘{field}")
            
            # éªŒè¯scenario_info
            if not instruction.scenario_info:
                errors.append(f"æ ·æœ¬{idx}: scenario_infoä¸ºç©º")
            else:
                # éªŒè¯operationå­—æ®µ
                if instruction.scenario_info.get("operation") != batch.operation:
                    errors.append(
                        f"æ ·æœ¬{idx}: operationä¸åŒ¹é…ï¼ŒæœŸæœ›{batch.operation}ï¼Œ"
                        f"å®é™…{instruction.scenario_info.get('operation')}"
                    )
        
        # éªŒè¯æ•°é‡
        if len(instructions) < batch.count * 0.8:  # å…è®¸20%çš„å®¹é”™
            errors.append(f"ç”Ÿæˆæ•°é‡ä¸è¶³: æœŸæœ›{batch.count}ï¼Œå®é™…{len(instructions)}")
        
        return errors
