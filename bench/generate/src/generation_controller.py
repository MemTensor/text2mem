"""
Generation Controller - ç”Ÿæˆæµç¨‹ä¸»æ§åˆ¶å™¨
ç¼–æ’æ•´ä¸ªä¸‰é˜¶æ®µç”Ÿæˆæµç¨‹ï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤
"""
from __future__ import annotations

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from bench.generate.src.llm_client import LLMClient, LLMConfig, create_llm_client
from bench.generate.src.plan_loader import PlanLoader, TaskAllocator, GenerationPlan, TaskBatch
from bench.generate.src.checkpoint_manager import CheckpointManager, Checkpoint, StageProgress
from bench.generate.src.stage1_generator import Stage1Generator
from bench.generate.src.stage2_generator import Stage2Generator, IRSample
from bench.generate.src.stage3_generator import Stage3Generator


class GenerationController:
    """ç”Ÿæˆæ§åˆ¶å™¨"""
    
    def __init__(
        self,
        plan_file: Path,
        resume: bool = True,
        verbose: bool = True,
    ):
        """
        Args:
            plan_file: ç”Ÿæˆè®¡åˆ’æ–‡ä»¶è·¯å¾„
            resume: æ˜¯å¦ä»æ–­ç‚¹æ¢å¤
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.verbose = verbose
        
        # åŠ è½½è®¡åˆ’
        self.plan = PlanLoader.load(plan_file)
        self._log(f"ğŸ“‹ åŠ è½½è®¡åˆ’: {self.plan.name}")
        
        # éªŒè¯è®¡åˆ’
        errors = PlanLoader.validate_plan(self.plan)
        if errors:
            raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥:\n  " + "\n  ".join(errors))
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_config = LLMConfig.from_dict(self.plan.llm)
        self.llm_client = create_llm_client(llm_config)
        self._log(f"ğŸ¤– LLM: {llm_config.provider} / {llm_config.model}")
        
        # æµ‹è¯•è¿æ¥
        if not self.llm_client.test_connection():
            raise ConnectionError("LLMè¿æ¥æµ‹è¯•å¤±è´¥")
        self._log("âœ… LLMè¿æ¥æ­£å¸¸")
        
        # åˆ›å»ºä»»åŠ¡åˆ†é…å™¨
        self.allocator = TaskAllocator(self.plan)
        
        # åˆå§‹åŒ–æ–­ç‚¹ç®¡ç†å™¨
        checkpoint_file = Path(self.plan.checkpoint_file.format(plan_name=self.plan.name))
        self.checkpoint_mgr = CheckpointManager(checkpoint_file)
        
        # åŠ è½½æˆ–åˆ›å»ºæ–­ç‚¹
        if resume and self.plan.resume_from_checkpoint:
            self.checkpoint = self.checkpoint_mgr.load()
            if self.checkpoint:
                self._log(f"ğŸ“¥ æ–­ç‚¹æ¢å¤: {self.checkpoint.progress_percentage:.1f}%")
            else:
                self.checkpoint = self._create_new_checkpoint()
        else:
            self.checkpoint = self._create_new_checkpoint()
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        seeds_dir = Path(__file__).parent.parent / "seeds"
        prompts_dir = Path(__file__).parent.parent / "prompts"
        
        self.stage1_generator = Stage1Generator(self.llm_client, self.plan, seeds_dir)
        self.stage2_generator = Stage2Generator(self.llm_client, self.plan, prompts_dir, llm_config)
        self.stage3_generator = Stage3Generator(self.llm_client, self.plan, prompts_dir, llm_config)
        
        # è¾“å‡ºç›®å½• - ç›´æ¥è¾“å‡ºåˆ° data/raw/
        base_dir = self.plan.output.get("base_dir", "bench/data/raw")
        self.output_dir = Path(base_dir)
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¿è¡Œç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.output_dir / timestamp
        self.run_dir.mkdir(parents=True, exist_ok=True)
    
    def _create_new_checkpoint(self) -> Checkpoint:
        """åˆ›å»ºæ–°æ–­ç‚¹"""
        self._log("ğŸ†• åˆ›å»ºæ–°æ–­ç‚¹")
        checkpoint = self.checkpoint_mgr.create_new(
            self.plan.name,
            self.plan.total_samples,
        )
        
        # åˆå§‹åŒ–é˜¶æ®µè¿›åº¦
        for stage_name in ["stage1", "stage2", "stage3"]:
            if self.plan.stages.get(stage_name, {}).get("enabled", True):
                # åˆ†é…ä»»åŠ¡æ¥è®¡ç®—æ‰¹æ¬¡æ•°
                if stage_name == "stage1":
                    batches = self.allocator.allocate_tasks(stage_name)
                    total_batches = len(batches)
                else:
                    # Stage2å’ŒStage3çš„æ‰¹æ¬¡æ•°åŸºäºæ ·æœ¬æ€»æ•°
                    total_batches = self.plan.total_samples
                
                checkpoint.stages[stage_name] = StageProgress(
                    stage_name=stage_name,
                    status="pending",
                    total_batches=total_batches,
                    completed_batches=0,
                )
        
        self.checkpoint_mgr.save(checkpoint)
        return checkpoint
    
    def _log(self, message: str):
        """è¾“å‡ºæ—¥å¿—"""
        if self.verbose:
            print(message)
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„ç”Ÿæˆæµç¨‹"""
        self._log("\n" + "=" * 60)
        self._log(f"ğŸš€ å¼€å§‹ç”Ÿæˆ: {self.plan.name}")
        self._log("=" * 60)
        
        start_time = time.time()
        
        try:
            # Stage 1: NLæŒ‡ä»¤ç”Ÿæˆ
            stage1_output = None
            if self._should_run_stage("stage1"):
                self._log("\nğŸ“ Stage 1: ç”ŸæˆNLæŒ‡ä»¤...")
                stage1_output = self.run_stage1()
                self._log(f"âœ… Stage 1 å®Œæˆ: {stage1_output}")
            else:
                self._log("\nâ­ï¸  Stage 1: å·²å®Œæˆ")
                stage1_output = self.checkpoint.output_files.get("stage1")
            
            # Stage 2: IR Schemaç”Ÿæˆ
            stage2_output = None
            if self._should_run_stage("stage2"):
                self._log("\nğŸ—ï¸  Stage 2: ç”ŸæˆIR Schema...")
                stage2_output = self._run_stage2(stage1_output)
                self._log(f"âœ… Stage 2 å®Œæˆ: {stage2_output}")
            else:
                self._log("\nâ­ï¸  Stage 2: å·²å®Œæˆ")
                stage2_output = self.checkpoint.output_files.get("stage2")
            
            # Stage 3: Expectedç”Ÿæˆ
            stage3_output = None
            if self._should_run_stage("stage3"):
                self._log("\nğŸ¯ Stage 3: ç”ŸæˆExpected...")
                stage3_output = self._run_stage3(stage2_output)
                self._log(f"âœ… Stage 3 å®Œæˆ: {stage3_output}")
            else:
                self._log("\nâ­ï¸  Stage 3: å·²å®Œæˆ")
            
            # ä¿å­˜è¿è¡Œå…ƒæ•°æ®
            self._save_metadata(stage1_output, stage2_output, stage3_output)
            
            # å®Œæˆ
            elapsed = time.time() - start_time
            self._log("\n" + "=" * 60)
            self._log(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}ç§’")
            self._log("=" * 60)
            
            # æ‰“å°æ‘˜è¦
            self._log("\n" + self.checkpoint_mgr.get_progress_summary())
            
        except KeyboardInterrupt:
            self._log("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
            self._log("ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ°æ–­ç‚¹")
            raise
        
        except Exception as e:
            self._log(f"\n\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _save_metadata(self, stage1_output: Optional[str], stage2_output: Optional[str], stage3_output: Optional[str]):
        """ä¿å­˜è¿è¡Œå…ƒæ•°æ®åˆ° metadata.json"""
        metadata = {
            "plan_name": self.plan.name,
            "timestamp": self.run_dir.name,  # ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¶é—´æˆ³
            "total_samples": self.plan.total_samples,
            "stages": {
                "stage1": {
                    "enabled": self.plan.stages.get("stage1", {}).get("enabled", True),
                    "output": str(Path(stage1_output).name) if stage1_output else None
                },
                "stage2": {
                    "enabled": self.plan.stages.get("stage2", {}).get("enabled", True),
                    "output": str(Path(stage2_output).name) if stage2_output else None
                },
                "stage3": {
                    "enabled": self.plan.stages.get("stage3", {}).get("enabled", True),
                    "output": str(Path(stage3_output).name) if stage3_output else None
                }
            },
            "llm": {
                "provider": self.plan.llm.get("provider"),
                "model": self.plan.llm.get("model")
            }
        }
        
        metadata_file = self.run_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        self._log(f"\n   ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    def _should_run_stage(self, stage_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿è¡ŒæŸä¸ªé˜¶æ®µ"""
        stage_config = self.plan.stages.get(stage_name, {})
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if not stage_config.get("enabled", True):
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        stage_progress = self.checkpoint.stages.get(stage_name)
        if stage_progress and stage_progress.status == "completed":
            return False
        
        return True
    
    def run_stage1(self) -> str:
        """è¿è¡ŒStage 1: NLæŒ‡ä»¤ç”Ÿæˆï¼ˆå¢é‡ä¿å­˜ç‰ˆæœ¬ï¼‰"""
        stage_name = "stage1"
        stage_progress = self.checkpoint.stages[stage_name]
        
        # æ›´æ–°çŠ¶æ€
        self.checkpoint_mgr.update_stage_progress(stage_name, status="running")
        
        # åˆ†é…ä»»åŠ¡
        batches = self.allocator.allocate_tasks(stage_name)
        self._log(f"   æ€»æ‰¹æ¬¡: {len(batches)}")
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶ï¼ˆä½¿ç”¨ JSONL æ ¼å¼ä»¥æ”¯æŒå¢é‡å†™å…¥ï¼‰
        # è¾“å‡ºåˆ° run_dir/stage1.jsonl
        output_file = self.run_dir / "stage1.jsonl"
        
        # å¦‚æœä»æ–­ç‚¹æ¢å¤ï¼Œæ£€æŸ¥æ˜¯å¦å·²æœ‰è¾“å‡ºæ–‡ä»¶
        existing_output = self.checkpoint.output_files.get(stage_name)
        if existing_output and Path(existing_output).exists():
            output_file = Path(existing_output)
            self._log(f"   ğŸ“¥ æ¢å¤è¾“å‡ºæ–‡ä»¶: {output_file}")
        else:
            # æ›´æ–°checkpointä¸­çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                output_file=str(output_file),
            )
        
        # æ‰“å¼€æ–‡ä»¶ç”¨äºè¿½åŠ 
        mode = 'a' if output_file.exists() else 'w'
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        for batch in batches:
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if batch.batch_id < stage_progress.completed_batches:
                self._log(f"   â­ï¸  æ‰¹æ¬¡ {batch.batch_id + 1}/{len(batches)}: å·²å®Œæˆ")
                continue
            
            self._log(f"\n   ğŸ“¦ æ‰¹æ¬¡ {batch.batch_id + 1}/{len(batches)}")
            self._log(f"      åœºæ™¯: {batch.scenario}, æ“ä½œ: {batch.operation}, æ•°é‡: {batch.count}")
            
            try:
                # ç”Ÿæˆ
                instructions = self.stage1_generator.generate_batch(batch)
                
                # éªŒè¯
                errors = self.stage1_generator.validate_instructions(instructions, batch)
                
                if errors:
                    validation_behavior = self.plan.validation.get("on_validation_error", "warn")
                    error_msg = "; ".join(errors[:3])
                    
                    if validation_behavior == "abort":
                        self._log(f"      âŒ éªŒè¯å¤±è´¥: {error_msg}")
                        raise ValueError(f"æ‰¹æ¬¡{batch.batch_id}éªŒè¯å¤±è´¥: {error_msg}")
                    elif validation_behavior == "warn":
                        self._log(f"      âš ï¸  éªŒè¯è­¦å‘Š: {error_msg}")
                
                # å®æ—¶å†™å…¥åˆ°æ–‡ä»¶ï¼ˆæ¯ä¸ªæ‰¹æ¬¡å®Œæˆåç«‹å³ä¿å­˜ï¼‰
                with open(output_file, mode, encoding='utf-8') as f:
                    for instruction in instructions:
                        sample_data = {
                            "instruction": instruction.instruction,
                            "context": instruction.context,
                            "classification": instruction.classification,
                            "scenario_info": instruction.scenario_info,
                            "batch_id": batch.batch_id,
                        }
                        f.write(json.dumps(sample_data, ensure_ascii=False) + '\n')
                
                # æ›´æ–°è¿›åº¦
                self.checkpoint_mgr.update_stage_progress(
                    stage_name,
                    completed_batches=batch.batch_id + 1,
                )
                
                self.checkpoint_mgr.add_completed_samples(
                    count=len(instructions),
                    scenario=batch.scenario,
                    operation=batch.operation,
                )
                
                self._log(f"      âœ… ç”Ÿæˆ {len(instructions)} æ¡æŒ‡ä»¤ï¼ˆå·²ä¿å­˜åˆ°æ–‡ä»¶ï¼‰")
                
                # åç»­æ‰¹æ¬¡ä½¿ç”¨è¿½åŠ æ¨¡å¼
                mode = 'a'
                
            except Exception as e:
                self._log(f"      âŒ å¤±è´¥: {e}")
                self.checkpoint_mgr.mark_batch_failed(stage_name, batch.batch_id, str(e))
                
                if self.plan.validation.get("on_validation_error") == "abort":
                    raise
                continue
        
        # æ›´æ–°checkpoint
        self.checkpoint_mgr.update_stage_progress(
            stage_name,
            status="completed",
            output_file=str(output_file),
        )
        
        self._log(f"\n   âœ… Stage 1 å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}")
        
        return str(output_file)
    
    def _run_stage2(self, stage1_output: Optional[str]) -> Optional[str]:
        """è¿è¡ŒStage 2: IR Schemaç”Ÿæˆï¼ˆå¢é‡ä¿å­˜ç‰ˆæœ¬ï¼‰"""
        if not stage1_output or not Path(stage1_output).exists():
            self._log("   âŒ Stage 1è¾“å‡ºæœªæ‰¾åˆ°")
            return None
        
        # åŠ è½½Stage 1è¾“å‡ºï¼ˆæ”¯æŒ JSON å’Œ JSONL æ ¼å¼ï¼‰
        stage1_path = Path(stage1_output)
        nl_instructions = []
        
        if stage1_path.suffix == '.jsonl':
            # JSONL æ ¼å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
            with open(stage1_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        nl_instructions.append(json.loads(line))
        else:
            # JSON æ ¼å¼ï¼ˆæ—§ç‰ˆæœ¬å…¼å®¹ï¼‰
            with open(stage1_path, 'r', encoding='utf-8') as f:
                nl_instructions = json.load(f)
        
        self._log(f"   ğŸ“¥ åŠ è½½ {len(nl_instructions)} æ¡NLæŒ‡ä»¤")
        
        stage_name = "stage2"
        stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        if not stage_progress:
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                status="running",
                total_batches=len(nl_instructions),
                completed_batches=0,
            )
            stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶ - è¾“å‡ºåˆ° run_dir/stage2.jsonl
        output_file = self.run_dir / "stage2.jsonl"
        
        # å¦‚æœæœ‰ç°æœ‰è¾“å‡ºæ–‡ä»¶ä¸”æ­£åœ¨æ¢å¤ï¼Œç»§ç»­ä½¿ç”¨è¯¥æ–‡ä»¶
        if stage_progress.output_file and Path(stage_progress.output_file).exists():
            output_file = Path(stage_progress.output_file)
            self._log(f"   ğŸ“‚ ç»§ç»­ä½¿ç”¨ç°æœ‰æ–‡ä»¶: {output_file}")
        else:
            self._log(f"   ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
            # æ›´æ–°è¾“å‡ºæ–‡ä»¶è·¯å¾„
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                output_file=str(output_file),
            )
        
        sample_count = 0
        
        # æ‰“å¼€æ–‡ä»¶ç”¨äºè¿½åŠ å†™å…¥ï¼ˆå¢é‡ä¿å­˜ï¼‰
        with open(output_file, 'a', encoding='utf-8') as f:
            for idx, nl_instruction in enumerate(nl_instructions):
                # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬
                if idx < stage_progress.completed_batches:
                    continue
                
                self._log(f"\n   ğŸ“¦ å¤„ç†æ ·æœ¬ {idx + 1}/{len(nl_instructions)}")
                
                try:
                    sample = self.stage2_generator.generate_single(nl_instruction)
                    
                    if sample:
                        # ç«‹å³å†™å…¥æ–‡ä»¶ï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                        sample_dict = {
                            "id": sample.id,
                            "class": sample.class_info,
                            "nl": sample.nl,
                            "prerequisites": sample.prerequisites,
                            "schema_list": sample.schema_list,
                            "init_db": sample.init_db,
                            "notes": sample.notes,
                        }
                        f.write(json.dumps(sample_dict, ensure_ascii=False) + '\n')
                        f.flush()  # å¼ºåˆ¶å†™å…¥ç£ç›˜
                        
                        sample_count += 1
                        self._log(f"      âœ… ç”Ÿæˆå¹¶ä¿å­˜IRæ ·æœ¬ (æ€»è®¡: {sample_count})")
                    
                    # æ›´æ–°checkpointè¿›åº¦
                    self.checkpoint_mgr.update_stage_progress(
                        stage_name,
                        completed_batches=idx + 1,
                    )
                    
                except Exception as e:
                    self._log(f"      âŒ å¤±è´¥: {e}")
                    self.checkpoint_mgr.record_error(stage_name, idx, str(e))
                    continue
        
        # æ ‡è®°é˜¶æ®µå®Œæˆ
        self.checkpoint_mgr.update_stage_progress(
            stage_name,
            status="completed",
        )
        
        self._log(f"\n   âœ… Stage 2å®Œæˆ: {sample_count} ä¸ªæ ·æœ¬å·²ä¿å­˜")
        
        return str(output_file)
    
    def _run_stage3(self, stage2_output: Optional[str]) -> Optional[str]:
        """è¿è¡ŒStage 3: Expectedç”Ÿæˆï¼ˆå¢é‡ä¿å­˜ç‰ˆæœ¬ï¼‰"""
        if not stage2_output or not Path(stage2_output).exists():
            self._log("   âŒ Stage 2è¾“å‡ºæœªæ‰¾åˆ°")
            return None
        
        # åŠ è½½Stage 2è¾“å‡º
        ir_samples_dict = []
        with open(stage2_output, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        ir_samples_dict.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
        
        self._log(f"   ğŸ“¥ åŠ è½½ {len(ir_samples_dict)} ä¸ªIRæ ·æœ¬")
        
        stage_name = "stage3"
        stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        if not stage_progress:
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                status="running",
                total_batches=len(ir_samples_dict),
                completed_batches=0,
            )
            stage_progress = self.checkpoint_mgr.get_stage_progress(stage_name)
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶ - è¾“å‡ºåˆ° run_dir/stage3.jsonl
        output_file = self.run_dir / "stage3.jsonl"
        
        # å¦‚æœæœ‰ç°æœ‰è¾“å‡ºæ–‡ä»¶ä¸”æ­£åœ¨æ¢å¤ï¼Œç»§ç»­ä½¿ç”¨è¯¥æ–‡ä»¶
        if stage_progress.output_file and Path(stage_progress.output_file).exists():
            output_file = Path(stage_progress.output_file)
            self._log(f"   ğŸ“‚ ç»§ç»­ä½¿ç”¨ç°æœ‰æ–‡ä»¶: {output_file}")
        else:
            self._log(f"   ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
            # æ›´æ–°è¾“å‡ºæ–‡ä»¶è·¯å¾„
            self.checkpoint_mgr.update_stage_progress(
                stage_name,
                output_file=str(output_file),
            )
        
        sample_count = 0
        
        # æ‰“å¼€æ–‡ä»¶ç”¨äºè¿½åŠ å†™å…¥ï¼ˆå¢é‡ä¿å­˜ï¼‰
        with open(output_file, 'a', encoding='utf-8') as f:
            for idx, sample_dict in enumerate(ir_samples_dict):
                # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬
                if idx < stage_progress.completed_batches:
                    continue
                
                self._log(f"\n   ğŸ“¦ å¤„ç†æ ·æœ¬ {idx + 1}/{len(ir_samples_dict)}")
                
                try:
                    # è½¬æ¢ä¸ºIRSample
                    ir_sample = IRSample(
                        id=sample_dict.get("id", ""),
                        class_info=sample_dict.get("class", {}),
                        nl=sample_dict.get("nl", {}),
                        prerequisites=sample_dict.get("prerequisites", []),
                        schema_list=sample_dict.get("schema_list", []),
                        init_db=sample_dict.get("init_db"),
                        notes=sample_dict.get("notes", ""),
                    )
                    
                    # ç”Ÿæˆexpected
                    complete_sample = self.stage3_generator.generate_single(ir_sample)
                    
                    if complete_sample:
                        # ç«‹å³å†™å…¥æ–‡ä»¶ï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                        sample_dict = {
                            "id": complete_sample.id,
                            "class": complete_sample.class_info,
                            "nl": complete_sample.nl,
                            "prerequisites": complete_sample.prerequisites,
                            "schema_list": complete_sample.schema_list,
                            "init_db": complete_sample.init_db,
                            "expected": complete_sample.expected,
                            "notes": complete_sample.notes,
                        }
                        f.write(json.dumps(sample_dict, ensure_ascii=False) + '\n')
                        f.flush()  # å¼ºåˆ¶å†™å…¥ç£ç›˜
                        
                        sample_count += 1
                        self._log(f"      âœ… ç”Ÿæˆå¹¶ä¿å­˜å®Œæ•´æ ·æœ¬ (æ€»è®¡: {sample_count})")
                    
                    # æ›´æ–°checkpointè¿›åº¦
                    self.checkpoint_mgr.update_stage_progress(
                        stage_name,
                        completed_batches=idx + 1,
                    )
                    
                except Exception as e:
                    self._log(f"      âŒ å¤±è´¥: {e}")
                    self.checkpoint_mgr.record_error(stage_name, idx, str(e))
                    continue
        
        # æ ‡è®°é˜¶æ®µå®Œæˆ
        self.checkpoint_mgr.update_stage_progress(
            stage_name,
            status="completed",
        )
        
        self._log(f"\n   âœ… Stage 3å®Œæˆ: {sample_count} ä¸ªå®Œæ•´æ ·æœ¬å·²ä¿å­˜")
        
        return str(output_file)
    
    def get_status(self) -> str:
        """è·å–å½“å‰çŠ¶æ€"""
        return self.checkpoint_mgr.get_progress_summary()
    
    def reset(self):
        """é‡ç½®ç”Ÿæˆè¿›åº¦"""
        self.checkpoint_mgr.delete()
        self._log("ğŸ—‘ï¸  æ–­ç‚¹å·²åˆ é™¤")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="Text2Mem Benchæµ‹è¯•æ ·æœ¬ç”Ÿæˆå™¨")
    parser.add_argument(
        "--plan",
        type=str,
        default="bench/generate/config/generation_plan.yaml",
        help="ç”Ÿæˆè®¡åˆ’æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="ä¸ä»æ–­ç‚¹æ¢å¤ï¼ˆé‡æ–°å¼€å§‹ï¼‰",
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="æ˜¾ç¤ºå½“å‰çŠ¶æ€å¹¶é€€å‡º",
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="é‡ç½®ç”Ÿæˆè¿›åº¦",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        default=True,
        help="è¯¦ç»†è¾“å‡º",
    )
    parser.add_argument(
        "--async",
        dest="use_async",
        action="store_true",
        help="ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œç”Ÿæˆï¼ˆæ¨èï¼Œé€Ÿåº¦æå‡5-10å€ï¼‰",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=None,
        help="æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼šä»ç¯å¢ƒå˜é‡ TEXT2MEM_BENCH_GEN_MAX_CONCURRENT è¯»å–æˆ–5ï¼‰",
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
    use_async = args.use_async or os.getenv("TEXT2MEM_BENCH_GEN_USE_ASYNC", "").lower() in ("true", "1", "yes")
    
    try:
        if use_async:
            # ä½¿ç”¨å¼‚æ­¥æ§åˆ¶å™¨
            try:
                from bench.generate.src.generation_controller_async import AsyncGenerationController
                
                controller = AsyncGenerationController(
                    plan_file=Path(args.plan),
                    resume=not args.no_resume,
                    verbose=args.verbose,
                    max_concurrent=args.max_concurrent,
                )
                
                if args.status:
                    print(controller.get_status())
                elif args.reset:
                    controller.reset()
                else:
                    controller.run_async()
            
            except ImportError as e:
                print(f"âŒ å¼‚æ­¥æ¨¡å¼éœ€è¦ aiohttp: pip install aiohttp")
                print(f"   æˆ–ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆç§»é™¤ --async å‚æ•°ï¼‰")
                exit(1)
        else:
            # ä½¿ç”¨åŒæ­¥æ§åˆ¶å™¨
            controller = GenerationController(
                plan_file=Path(args.plan),
                resume=not args.no_resume,
                verbose=args.verbose,
            )
            
            if args.status:
                print(controller.get_status())
            elif args.reset:
                controller.reset()
            else:
                controller.run()
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
