"""
Benchmark Builder - æž„å»ºå™¨

æ•´åˆç”Ÿæˆã€æµ‹è¯•ã€æ¸…æ´—æµç¨‹ï¼Œä¸€é”®ç”Ÿæˆ benchmark
"""
from __future__ import annotations

import json
import logging
import shutil
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import hashlib

from bench.core.benchmark_manager import BenchmarkManager, BenchmarkVersion

logger = logging.getLogger(__name__)


class BenchmarkBuilder:
    """Benchmark æž„å»ºå™¨"""
    
    def __init__(
        self,
        config_file: Optional[Path] = None,
        version_id: Optional[str] = None,
        keep_raw: bool = True,
        manager: Optional[BenchmarkManager] = None,
    ):
        """
        Args:
            config_file: ç”Ÿæˆé…ç½®æ–‡ä»¶è·¯å¾„
            version_id: ç‰ˆæœ¬ IDï¼ˆé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            keep_raw: æ˜¯å¦ä¿ç•™åŽŸå§‹ç”Ÿæˆæ•°æ®
            manager: BenchmarkManager å®žä¾‹
        """
        self.config_file = Path(config_file) if config_file else Path('bench/generate/config/generation_plan.yaml')
        self.version_id = version_id or datetime.now().strftime("%Y%m%d_%H%M%S")
        self.keep_raw = keep_raw
        self.manager = manager or BenchmarkManager()
        
        # åˆ›å»ºç‰ˆæœ¬ç›®å½•
        self.version = self.manager.create_version(self.version_id)
        
        # ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºŽç”Ÿæˆï¼‰
        self.temp_dir = Path(f'bench/data/.tmp_{self.version_id}')
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Benchmark Builder initialized: {self.version_id}")
        logger.info(f"Output directory: {self.version.version_dir}")
    
    def build(
        self,
        skip_generate: bool = False,
        from_raw: Optional[Path] = None,
        samples_override: Optional[int] = None,
    ) -> BenchmarkVersion:
        """
        å®Œæ•´æž„å»ºæµç¨‹
        
        Args:
            skip_generate: è·³è¿‡ç”Ÿæˆæ­¥éª¤
            from_raw: ä»ŽçŽ°æœ‰ raw æ•°æ®æž„å»º
            samples_override: è¦†ç›–é…ç½®ä¸­çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºŽå¿«é€Ÿæµ‹è¯•ï¼‰
        
        Returns:
            æž„å»ºå®Œæˆçš„ BenchmarkVersion
        """
        start_time = time.time()
        
        try:
            # é˜¶æ®µ 1: ç”Ÿæˆæ•°æ®
            if from_raw:
                logger.info("ðŸ“¦ ä½¿ç”¨çŽ°æœ‰ raw æ•°æ®")
                raw_dir = Path(from_raw)
                if not raw_dir.exists():
                    raise FileNotFoundError(f"Raw directory not found: {raw_dir}")
                stage3_file = raw_dir / "stage3.jsonl"
            elif skip_generate:
                logger.info("â© è·³è¿‡ç”Ÿæˆæ­¥éª¤")
                stage3_file = self.temp_dir / "stage3.jsonl"
            else:
                logger.info("ðŸ”„ é˜¶æ®µ 1/4: ç”Ÿæˆæµ‹è¯•æ•°æ®...")
                stage3_file = self._run_generation(samples_override)
            
            # é˜¶æ®µ 2: è¿è¡Œæµ‹è¯•
            logger.info("ðŸ”„ é˜¶æ®µ 2/4: è¿è¡Œæµ‹è¯•...")
            test_results = self._run_tests(stage3_file)
            
            # é˜¶æ®µ 3: æ¸…æ´—æ•°æ®
            logger.info("ðŸ”„ é˜¶æ®µ 3/4: æ¸…æ´—æ•°æ®...")
            cleaned_data, cleaning_report = self._clean_data(stage3_file, test_results)
            
            # é˜¶æ®µ 4: æž„å»º benchmark
            logger.info("ðŸ”„ é˜¶æ®µ 4/4: æž„å»º benchmark...")
            self._build_benchmark(cleaned_data)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = self._generate_metadata(test_results, cleaning_report)
            self.version.save_metadata(metadata)
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self._generate_stats(cleaned_data)
            self.version.save_stats(stats)
            
            # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
            self._save_test_report(test_results)
            
            # å¯é€‰ï¼šä¿ç•™ raw æ•°æ®
            if self.keep_raw and not from_raw:
                self._copy_raw_data(stage3_file.parent)
            
            # æ›´æ–° latest ç¬¦å·é“¾æŽ¥
            self.manager.create_link(self.version_id, 'latest')
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup()
            
            duration = time.time() - start_time
            
            # æ‰“å°æ‘˜è¦
            self._print_summary(metadata, duration)
            
            return self.version
            
        except Exception as e:
            logger.error(f"âŒ æž„å»ºå¤±è´¥: {e}")
            # æ¸…ç†å¤±è´¥çš„ç‰ˆæœ¬
            if self.version.version_dir.exists():
                shutil.rmtree(self.version.version_dir)
            self._cleanup()
            raise
    
    def _run_generation(self, samples_override: Optional[int] = None) -> Path:
        """è¿è¡Œç”Ÿæˆæµç¨‹"""
        # å¯¼å…¥ç”Ÿæˆå™¨
        sys.path.insert(0, str(Path('bench/generate').resolve()))
        
        from bench.generate.src.generation_controller import main as generate_main
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®ï¼ˆå¦‚æžœéœ€è¦ï¼‰
        config_backup = None
        if samples_override:
            import yaml
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # å¤‡ä»½åŽŸé…ç½®
            config_backup = config.copy()
            
            # ä¿®æ”¹æ ·æœ¬æ•°
            config['plan']['total_samples'] = samples_override
            
            # å†™å…¥ä¸´æ—¶é…ç½®
            temp_config = self.temp_dir / 'generation_plan.yaml'
            with open(temp_config, 'w', encoding='utf-8') as f:
                yaml.dump(config, f)
            
            config_file_to_use = temp_config
        else:
            config_file_to_use = self.config_file
        
        # è¿è¡Œç”Ÿæˆï¼ˆè¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•ï¼‰
        output_dir = self.temp_dir
        
        # è°ƒç”¨ç”Ÿæˆå™¨ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®žé™…å®žçŽ°è°ƒæ•´ï¼‰
        # æš‚æ—¶ä½¿ç”¨ subprocess è°ƒç”¨
        cmd = [
            sys.executable,
            'bench/generate/generate.py',
            '--config', str(config_file_to_use),
            '--output', str(output_dir),
        ]
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info(result.stdout)
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„ stage3.jsonl
        # ç”Ÿæˆå™¨åº”è¯¥è¾“å‡ºåˆ° output_dir/YYYYMMDD_HHMMSS/stage3.jsonl
        # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€æ–°çš„è¾“å‡ºç›®å½•
        stage3_file = output_dir / 'stage3.jsonl'
        
        if not stage3_file.exists():
            # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
            raw_dirs = [d for d in output_dir.iterdir() if d.is_dir() and d.name.replace('_', '').isdigit()]
            if raw_dirs:
                latest_raw = max(raw_dirs, key=lambda d: d.name)
                stage3_file = latest_raw / 'stage3.jsonl'
        
        if not stage3_file.exists():
            raise FileNotFoundError(f"Generated stage3.jsonl not found in {output_dir}")
        
        # ç»Ÿè®¡ç”Ÿæˆçš„æ ·æœ¬æ•°
        sample_count = sum(1 for _ in open(stage3_file, 'r', encoding='utf-8'))
        logger.info(f"âœ“ ç”Ÿæˆå®Œæˆ: {sample_count} samples")
        
        return stage3_file
    
    def _run_tests(self, stage3_file: Path) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•"""
        from bench.core.runner import BenchRunner, BenchConfig
        
        # è¯»å–æ ·æœ¬
        samples = []
        with open(stage3_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    samples.append(json.loads(line))
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = BenchConfig(
            db_root=self.temp_dir / 'db',
            output_dir=self.temp_dir / 'output',
            mode='auto',
        )
        
        # è¿è¡Œæµ‹è¯•
        runner = BenchRunner(config)
        
        passed = []
        failed = []
        
        start_time = time.time()
        
        for i, sample in enumerate(samples, 1):
            sample_id = sample.get('id', f'sample_{i}')
            
            try:
                result = runner.run_sample(sample, sample_id)
                
                if result.passed:
                    passed.append({
                        'sample_id': sample_id,
                        'passed': True,
                    })
                else:
                    failed.append({
                        'sample_id': sample_id,
                        'passed': False,
                        'errors': result.errors,
                    })
                
                # è¿›åº¦æ˜¾ç¤º
                if i % 10 == 0 or i == len(samples):
                    logger.info(f"  è¿›åº¦: {i}/{len(samples)} ({i/len(samples)*100:.1f}%)")
            
            except Exception as e:
                logger.warning(f"  æ ·æœ¬ {sample_id} æµ‹è¯•å¼‚å¸¸: {e}")
                failed.append({
                    'sample_id': sample_id,
                    'passed': False,
                    'errors': [str(e)],
                })
        
        duration = time.time() - start_time
        
        test_results = {
            'total_samples': len(samples),
            'passed': len(passed),
            'failed': len(failed),
            'pass_rate': len(passed) / len(samples) if samples else 0.0,
            'test_duration': duration,
            'passed_list': passed,
            'failed_list': failed,
        }
        
        logger.info(f"âœ“ æµ‹è¯•å®Œæˆ: {len(passed)}/{len(samples)} passed ({test_results['pass_rate']*100:.1f}%)")
        
        return test_results
    
    def _clean_data(
        self,
        stage3_file: Path,
        test_results: Dict[str, Any]
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """æ¸…æ´—æ•°æ®"""
        # è¯»å–æ‰€æœ‰æ ·æœ¬
        samples = []
        with open(stage3_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    samples.append(json.loads(line))
        
        # èŽ·å–é€šè¿‡æµ‹è¯•çš„æ ·æœ¬ ID
        passed_ids = {item['sample_id'] for item in test_results['passed_list']}
        
        # è¿‡æ»¤è§„åˆ™
        ALLOWED_OPERATIONS = {
            'Encode', 'Retrieve', 'Update', 'Delete', 'Summarize', 'Label',
            'Promote', 'Demote', 'Expire', 'Lock', 'Merge', 'Split',
        }
        
        cleaned_samples = []
        filter_stats = {
            'total': len(samples),
            'passed_test': 0,
            'has_unknown': 0,
            'invalid_operation': 0,
            'final': 0,
        }
        
        for sample in samples:
            sample_id = sample.get('id', '')
            
            # è§„åˆ™ 1: å¿…é¡»é€šè¿‡æµ‹è¯•
            if sample_id not in passed_ids:
                continue
            filter_stats['passed_test'] += 1
            
            # è§„åˆ™ 2: ä¸åŒ…å« 'unknown'
            sample_str = json.dumps(sample)
            if 'unknown' in sample_str.lower():
                filter_stats['has_unknown'] += 1
                continue
            
            # è§„åˆ™ 3: æ“ä½œå¿…é¡»åœ¨å…è®¸åˆ—è¡¨ä¸­
            schema_list = sample.get('schema_list', [])
            invalid_op = False
            for schema in schema_list:
                if schema.get('op') not in ALLOWED_OPERATIONS:
                    invalid_op = True
                    break
            
            if invalid_op:
                filter_stats['invalid_operation'] += 1
                continue
            
            # é€šè¿‡æ‰€æœ‰è¿‡æ»¤è§„åˆ™
            cleaned_samples.append(sample)
        
        filter_stats['final'] = len(cleaned_samples)
        
        cleaning_report = {
            'rules_applied': ['filter_failed', 'filter_unknown', 'filter_invalid_ops'],
            'samples_before': len(samples),
            'samples_after': len(cleaned_samples),
            'filter_stats': filter_stats,
        }
        
        logger.info(f"âœ“ æ¸…æ´—å®Œæˆ: {len(cleaned_samples)} samples retained")
        
        return cleaned_samples, cleaning_report
    
    def _build_benchmark(self, cleaned_samples: List[Dict[str, Any]]) -> None:
        """æž„å»ºæœ€ç»ˆ benchmark"""
        # é‡æ–°åˆ†é… ID
        id_counter = {}
        reassigned_samples = []
        
        for sample in cleaned_samples:
            # æå–åˆ†ç±»ä¿¡æ¯
            class_info = sample.get('class', {})
            lang = class_info.get('lang', 'en')
            instruction = class_info.get('instruction', 'direct')
            structure = class_info.get('structure', 'single')
            
            # èŽ·å–æ“ä½œç±»åž‹
            schema_list = sample.get('schema_list', [])
            if schema_list:
                op = schema_list[0].get('op', 'unknown').lower()
            else:
                op = 'unknown'
            
            # ç”Ÿæˆæ–° ID
            key = f"{lang}-{instruction}-{structure}-{op}"
            if key not in id_counter:
                id_counter[key] = 1
            else:
                id_counter[key] += 1
            
            new_id = f"t2m-{key}-{id_counter[key]:03d}"
            
            # ä¿å­˜åŽŸ ID
            sample['_original_id'] = sample.get('id')
            sample['id'] = new_id
            
            reassigned_samples.append(sample)
        
        # ä¿å­˜åˆ° benchmark.jsonl
        with open(self.version.benchmark_file, 'w', encoding='utf-8') as f:
            for sample in reassigned_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        logger.info(f"âœ“ Benchmark å·²ä¿å­˜: {self.version.benchmark_file}")
    
    def _generate_metadata(
        self,
        test_results: Dict[str, Any],
        cleaning_report: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        # è®¡ç®—é…ç½®æ–‡ä»¶å“ˆå¸Œ
        config_hash = self._hash_file(self.config_file)
        
        # è¯»å–é…ç½®ä¿¡æ¯
        import yaml
        with open(self.config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        llm_config = config.get('llm', {})
        plan_config = config.get('plan', {})
        
        metadata = {
            'id': self.version_id,
            'created_at': datetime.now().isoformat(),
            'status': 'draft',  # åˆå§‹çŠ¶æ€ä¸º draft
            
            'generation': {
                'config_file': str(self.config_file),
                'config_hash': config_hash,
                'total_samples': plan_config.get('total_samples', 0),
                'llm_provider': llm_config.get('provider', 'unknown'),
                'llm_model': llm_config.get('model', 'unknown'),
            },
            
            'test_results': test_results,
            'cleaning': cleaning_report,
            
            'tags': [],
            'notes': '',
        }
        
        return metadata
    
    def _generate_stats(self, cleaned_samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        from collections import Counter
        
        stats = {
            'total': len(cleaned_samples),
            'distribution': {
                'languages': {},
                'operations': {},
                'instruction_types': {},
                'structures': {},
            }
        }
        
        lang_counter = Counter()
        op_counter = Counter()
        instruction_counter = Counter()
        structure_counter = Counter()
        
        for sample in cleaned_samples:
            class_info = sample.get('class', {})
            lang_counter[class_info.get('lang', 'unknown')] += 1
            instruction_counter[class_info.get('instruction', 'unknown')] += 1
            structure_counter[class_info.get('structure', 'unknown')] += 1
            
            schema_list = sample.get('schema_list', [])
            for schema in schema_list:
                op_counter[schema.get('op', 'unknown')] += 1
        
        stats['distribution']['languages'] = dict(lang_counter)
        stats['distribution']['operations'] = dict(op_counter)
        stats['distribution']['instruction_types'] = dict(instruction_counter)
        stats['distribution']['structures'] = dict(structure_counter)
        
        return stats
    
    def _save_test_report(self, test_results: Dict[str, Any]) -> None:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        with open(self.version.test_report_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    def _copy_raw_data(self, raw_dir: Path) -> None:
        """å¤åˆ¶åŽŸå§‹æ•°æ®"""
        if raw_dir.exists():
            dest_raw_dir = self.version.raw_dir
            dest_raw_dir.mkdir(parents=True, exist_ok=True)
            
            for file in ['stage1.jsonl', 'stage2.jsonl', 'stage3.jsonl']:
                src = raw_dir / file
                if src.exists():
                    shutil.copy(src, dest_raw_dir / file)
            
            logger.info(f"âœ“ Raw æ•°æ®å·²ä¿å­˜åˆ°: {dest_raw_dir}")
    
    def _cleanup(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
            logger.debug(f"Cleaned up temp directory: {self.temp_dir}")
    
    def _print_summary(self, metadata: Dict[str, Any], duration: float) -> None:
        """æ‰“å°æž„å»ºæ‘˜è¦"""
        test_results = metadata['test_results']
        
        print("\n" + "=" * 80)
        print("âœ… Benchmark æž„å»ºå®Œæˆï¼")
        print("=" * 80)
        print(f"\nðŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  ç”Ÿæˆ: {metadata['generation']['total_samples']} samples")
        print(f"  æµ‹è¯•: {test_results['passed']}/{test_results['total_samples']} passed "
              f"({test_results['pass_rate']*100:.1f}%)")
        print(f"  æ¸…æ´—: {metadata['cleaning']['samples_after']} samples retained")
        print(f"  è€—æ—¶: {duration:.1f}s")
        
        print(f"\nðŸ“‚ è¾“å‡ºä½ç½®:")
        print(f"  Benchmark ID: {self.version_id}")
        print(f"  ç›®å½•: {self.version.version_dir}")
        print(f"  æ–‡ä»¶: benchmark.jsonl ({metadata['cleaning']['samples_after']} samples)")
        
        print(f"\nðŸ”— ç¬¦å·é“¾æŽ¥:")
        print(f"  latest -> {self.version_id}")
        
        print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  # éªŒè¯ benchmark")
        print(f"  bench-cli test {self.version_id} --verbose")
        print(f"  ")
        print(f"  # æ ‡è®°ä¸ºç¨³å®šç‰ˆæœ¬")
        print(f"  bench-cli link {self.version_id} stable")
        print()
    
    @staticmethod
    def _hash_file(file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            hasher.update(f.read())
        return hasher.hexdigest()[:16]
