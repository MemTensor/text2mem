#!/usr/bin/env python3
"""
Benchmarkå®Œæ•´æµç¨‹å·¥å…· v3.0

Features:
fromrawdatatoæœ€ç»ˆbenchmarkçš„å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹ï¼š
1. testï¼ˆcreaterunï¼‰
2. æ¸…æ´—å¹¶buildbenchmarkï¼ˆåˆå¹¶stepï¼‰

Usage:
    # å¤„ç†latestraw
    python -m bench.tools.pipeline --raw latest --version v2
    
    # å¤„ç†specifiedraw
    python -m bench.tools.pipeline --raw 20251015_131147 --version v2
    
    # skipteststep
    python -m bench.tools.pipeline --raw latest --version v2 --skip-tests
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BenchmarkPipeline:
    """Benchmarkå®Œæ•´æµç¨‹"""
    
    def __init__(
        self,
        raw_id: str,
        version: Optional[str] = None,
        skip_tests: bool = False,
        verbose: bool = False,
    ):
        """
        Args:
            raw_id: Raw ID
            version: Benchmarkversionå·ï¼ˆå¦‚v2, v3ç­‰ï¼‰
            skip_tests: skiptestrun
            verbose: è¯¦ç»†è¾“å‡º
        """
        self.raw_id = raw_id
        self.verbose = verbose
        self.skip_tests = skip_tests
        
        # ç¡®å®šè¾“å‡ºversion
        if version:
            self.version = version
        else:
            # è‡ªåŠ¨generateversionå·ï¼ˆbased onraw_idï¼‰
            if raw_id == 'latest':
                run_manager = RunManager()
                actual_raw_id = run_manager.get_latest_raw()
            else:
                actual_raw_id = raw_id
            self.version = f"v_{actual_raw_id}"
        
        self.run_manager = RunManager()
        
        # getrawdirectory
        try:
            self.raw_dir = self.run_manager.get_raw_dir(raw_id)
            if raw_id == 'latest':
                self.raw_id = self.run_manager.get_latest_raw()
        except FileNotFoundError as e:
            logger.error(f"âŒ {e}")
            raise
        
        logger.info(f"ğŸ“‹ Pipelineconfiguration:")
        logger.info(f"  Raw ID: {self.raw_id}")
        logger.info(f"  Rawdirectory: {self.raw_dir}")
        logger.info(f"  Benchmarkversion: {self.version}")
    
    def run(self) -> bool:
        """runå®Œæ•´æµç¨‹"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ startBenchmarkbuildæµç¨‹")
        logger.info("="*80)
        
        try:
            # Step 1: runtestï¼ˆcreaterunï¼‰
            if not self.skip_tests:
                if not self._step1_tests():
                    return False
            else:
                logger.info("\nâ­ï¸  skiptestrun")
                # ifskiptestï¼Œéœ€è¦ensurerunalreadyexist
                runs = self.run_manager.list_runs()
                if self.raw_id not in runs:
                    logger.error(f"âŒ Runä¸exist: {self.raw_id}")
                    logger.info("   æç¤ºï¼šcannotskiptestï¼Œbecauserunè¿˜notcreate")
                    return False
            
            # ç¡®å®šrun_id
            self.run_id = self.raw_id
            
            # Step 2: æ¸…æ´—å¹¶buildbenchmarkï¼ˆåˆå¹¶stepï¼‰
            if not self._step2_clean_and_build():
                return False
            
            logger.info("\n" + "="*80)
            logger.info("âœ… Pipelinecompleteï¼")
            logger.info("="*80)
            
            return True
            
        except Exception as e:
            logger.error(f"\nâŒ Pipelinefailed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _step1_tests(self) -> bool:
        """Step 1: runtestï¼ˆcreaterunï¼‰"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ§ª Step 1: runtestï¼ˆcreaterunï¼‰")
        logger.info("="*80)
        
        from bench.tools.test import TestRunner
        
        runner = TestRunner(raw_id=self.raw_id, verbose=self.verbose)
        runner.load_samples()
        runner.run_tests()
        runner.save_results()
        
        if self.verbose:
            runner.print_summary()
        
        # checktestresult
        if runner.stats['failed'] > 0:
            logger.warning(f"âš ï¸  æœ‰ {runner.stats['failed']} ä¸ªsampletestfailed")
            logger.warning(f"   è¿™äº›samplewillåœ¨æ¸…æ´—stepä¸­referenced byè¿‡æ»¤æ‰")
        
        self.run_id = runner.run_id
        logger.info(f"âœ… Step 1 complete: {runner.tests_dir}")
        return True
    
    def _step2_clean_and_build(self) -> bool:
        """Step 2: æ¸…æ´—å¹¶buildbenchmarkï¼ˆåˆå¹¶stepï¼‰"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ§¹ Step 2: æ¸…æ´—dataå¹¶buildBenchmark")
        logger.info("="*80)
        
        from bench.tools.clean import DataCleaner
        from bench.tools.build import BenchmarkBuilder
        
        # 2.1 æ¸…æ´—data
        logger.info("ğŸ“ 2.1 æ¸…æ´—data...")
        cleaner = DataCleaner(
            run_id=self.run_id,
            filter_unknown=True,
            filter_failed=(not self.skip_tests),
        )
        
        cleaner.load_test_results()
        cleaner.load_samples()
        filtered_samples = cleaner.filter_samples()
        
        if not filtered_samples:
            logger.error("âŒ æ²¡æœ‰sampleviaè¿‡æ»¤")
            return False
        
        cleaner.save_cleaned_data(filtered_samples)
        
        if self.verbose:
            cleaner.print_summary()
        
        logger.info(f"âœ… æ¸…æ´—complete: {self.run_manager.get_cleaned_dir(self.run_id)}")
        
        # 2.2 buildbenchmark
        logger.info("\nğŸ“ 2.2 buildBenchmark...")
        builder = BenchmarkBuilder(
            run_id=self.run_id,
            version=self.version,
            rebuild_ids=True,
        )
        
        builder.load_cleaned_data()
        builder.rebuild_sample_ids()
        builder.build()
        
        if self.verbose:
            builder.print_summary()
        
        logger.info(f"âœ… buildcomplete: {self.run_manager.get_benchmark_dir(self.version)}")
        
        return True
    
    def print_summary(self):
        """æ‰“å°å®Œæ•´æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“‹ Pipelineæ‘˜è¦")
        print("="*80)
        
        print(f"\nè¾“å…¥:")
        print(f"  Raw ID: {self.raw_id}")
        print(f"  Rawdirectory: {self.raw_dir}")
        
        print(f"\nè¾“å‡º:")
        print(f"  Run ID: {self.run_id}")
        print(f"  Rundirectory: {self.run_manager.get_run_dir(self.run_id)}")
        print(f"  Benchmarkversion: {self.version}")
        print(f"  Benchmarkdirectory: {self.run_manager.get_benchmark_dir(self.version)}")
        
        print(f"\nexecuteçš„step:")
        steps = []
        if not self.skip_tests:
            steps.append("âœ… runtestï¼ˆcreaterunï¼‰")
        steps.append("âœ… æ¸…æ´—dataå¹¶buildBenchmark")
        
        for step in steps:
            print(f"  {step}")
        
        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"  # verifybenchmark")
        print(f"  python -m bench run --split benchmark --verbose")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkå®Œæ•´æµç¨‹å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Œæ•´æµç¨‹åŒ…æ‹¬:
  1. runtest - fromrawcreaterunå¹¶testallsample
  2. æ¸…æ´—å¹¶build - è¿‡æ»¤failedsampleï¼Œåº”ç”¨è§„åˆ™ï¼Œé‡æ–°åˆ†é…IDï¼Œgenerateæœ€ç»ˆbenchmark

example:
  # å¤„ç†latestraw
  python -m bench.tools.pipeline --raw latest --version v2
  
  # å¤„ç†specifiedraw
  python -m bench.tools.pipeline --raw 20251015_131147 --version v2
  
  # skiptestï¼Œç›´æ¥æ¸…æ´—å¹¶buildï¼ˆrunmustalreadyexistï¼‰
  python -m bench.tools.pipeline --raw latest --version v2 --skip-tests
        """
    )
    
    parser.add_argument(
        '--raw',
        required=True,
        help='Raw ID (å¦‚ "20251015_131147" or "latest")'
    )
    parser.add_argument(
        '--version', '-v',
        help='Benchmarkversionå· (å¦‚ "v2", "v3"ï¼Œdefaultï¼šè‡ªåŠ¨generate)'
    )
    parser.add_argument(
        '--skip-tests',
        action='store_true',
        help='skiptestrunï¼ˆrunmustalreadyexistï¼‰'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º'
    )
    
    args = parser.parse_args()
    
    # createpipeline
    try:
        pipeline = BenchmarkPipeline(
            raw_id=args.raw,
            version=args.version,
            skip_tests=args.skip_tests,
            verbose=args.verbose,
        )
    except FileNotFoundError:
        return 1
    
    # run
    success = pipeline.run()
    
    if success:
        pipeline.print_summary()
        return 0
    else:
        return 1


if __name__ == '__main__':
    sys.exit(main())
