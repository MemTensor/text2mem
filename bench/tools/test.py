#!/usr/bin/env python3
"""
Benchmarkæµ‹è¯•å·¥å…· v3.0

åŠŸèƒ½ï¼š
1. ä»rawåˆ›å»ºrunå¹¶è¿è¡Œæµ‹è¯•
2. æ”¶é›†æˆåŠŸ/å¤±è´¥æ ·æœ¬
3. ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

ç”¨æ³•ï¼š
    # ä»æœ€æ–°rawåˆ›å»ºrunå¹¶æµ‹è¯•
    python -m bench.tools.test --raw latest
    
    # ä»æŒ‡å®šrawåˆ›å»ºrunå¹¶æµ‹è¯•
    python -m bench.tools.test --raw 20251015_131147
    
    # æµ‹è¯•å·²å­˜åœ¨çš„run
    python -m bench.tools.test --run 20251015_131147
    
    # åªæµ‹è¯•å‰Nä¸ªæ ·æœ¬
    python -m bench.tools.test --raw latest --limit 10
"""

import argparse
import json
import logging
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, raw_id: Optional[str] = None, run_id: Optional[str] = None, verbose: bool = False):
        """
        Args:
            raw_id: Raw IDï¼ˆç”¨äºåˆ›å»ºæ–°runï¼‰
            run_id: Run IDï¼ˆç”¨äºæµ‹è¯•å·²å­˜åœ¨çš„runï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        """
        self.verbose = verbose
        self.run_manager = RunManager()
        
        if raw_id:
            # ä»rawåˆ›å»ºrun
            logger.info(f"ğŸ“¦ ä»rawåˆ›å»ºrun: {raw_id}")
            self.raw_id = raw_id if raw_id != 'latest' else self.run_manager.get_latest_raw()
            
            if not self.raw_id:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°rawæ•°æ®")
            
            # åˆ›å»ºrunç›®å½•
            self.run_dir = self.run_manager.create_run_from_raw(self.raw_id)
            self.run_id = self.raw_id  # run_idé»˜è®¤ä¸raw_idç›¸åŒ
            
            # è·å–stage3æ–‡ä»¶è·¯å¾„ï¼ˆä»rawï¼‰
            self.stage3_file = self.run_manager.get_stage_file_from_raw(self.raw_id, 3)
            
        elif run_id:
            # ä½¿ç”¨å·²å­˜åœ¨çš„run
            self.run_id = run_id
            self.run_dir = self.run_manager.get_run_dir(run_id)
            
            # è·å–æ¥æºraw
            self.raw_id = self.run_manager.get_source_raw(run_id)
            if self.raw_id:
                self.stage3_file = self.run_manager.get_stage_file_from_raw(self.raw_id, 3)
            else:
                raise ValueError(f"æ— æ³•ç¡®å®šrun {run_id} çš„æ¥æºraw")
        else:
            raise ValueError("å¿…é¡»æŒ‡å®š raw_id æˆ– run_id")
        
        # è·å–æµ‹è¯•ç›®å½•
        self.tests_dir = self.run_manager.get_tests_dir(self.run_id)
        
        # åŠ è½½æ•°æ®
        self.samples: List[Dict[str, Any]] = []
        self.results: List[Dict[str, Any]] = []
        
        # æµ‹è¯•ç»Ÿè®¡
        self.stats = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'errors': 0,
            'by_operation': defaultdict(lambda: {'total': 0, 'passed': 0, 'failed': 0}),
            'by_language': defaultdict(lambda: {'total': 0, 'passed': 0, 'failed': 0}),
        }
        
        logger.info(f"ğŸ“‚ Rawç›®å½•: {self.run_manager.get_raw_dir(self.raw_id)}")
        logger.info(f"ğŸ“‚ Runç›®å½•: {self.run_dir}")
        logger.info(f"ğŸ“‚ æµ‹è¯•ç»“æœ: {self.tests_dir}")
    
    def load_samples(self) -> int:
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        if not self.stage3_file.exists():
            raise FileNotFoundError(f"Stage3æ–‡ä»¶ä¸å­˜åœ¨: {self.stage3_file}")
        
        logger.info(f"ğŸ“‚ åŠ è½½æ ·æœ¬: {self.stage3_file}")
        
        count = 0
        with self.stage3_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æå¤±è´¥: {e}")
        
        logger.info(f"âœ… åŠ è½½ {count} ä¸ªæ ·æœ¬")
        return count
    
    def run_tests(self, limit: Optional[int] = None, timeout: Optional[float] = None) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•
        
        Args:
            limit: é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
            timeout: æ¯ä¸ªæ ·æœ¬çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # å¯¼å…¥æµ‹è¯•runner
        try:
            from bench.core.runner import BenchRunner, BenchConfig
        except ImportError:
            logger.error("âŒ æ— æ³•å¯¼å…¥ bench.core.runnerï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œ")
            raise
        
        samples_to_test = self.samples[:limit] if limit else self.samples
        total = len(samples_to_test)
        
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯• {total} ä¸ªæ ·æœ¬...")
        
        # é…ç½®runner
        config = BenchConfig(
            db_root=Path('bench/data/db'),
            output_dir=Path('bench/output'),
            timeout=timeout,
        )
        runner = BenchRunner(config)
        
        start_time = time.time()
        
        for idx, sample in enumerate(samples_to_test, 1):
            sample_id = sample.get('id', f'sample-{idx}')
            class_info = sample.get('class', {})
            
            # æå–åˆ†ç±»ä¿¡æ¯
            lang = class_info.get('lang', 'unknown')
            
            # æå–æ“ä½œ
            schema_list = sample.get('schema_list', [])
            operation = schema_list[0].get('op', 'unknown') if schema_list else 'unknown'
            
            # æ˜¾ç¤ºè¿›åº¦ - æ¯ä¸ªæ ·æœ¬éƒ½æ˜¾ç¤ºï¼ˆä½¿ç”¨loggerè®©æ ¼å¼ç»Ÿä¸€ï¼‰
            progress_pct = (idx / total) * 100
            logger.info(f"[{idx}/{total} {progress_pct:.1f}%] æµ‹è¯•: {sample_id} ({operation})")
            
            # è¿è¡Œæµ‹è¯•
            sample_start = time.time()
            try:
                result = runner.run_sample(sample, sample_id=sample_id)
                sample_duration = time.time() - sample_start
                
                # è®°å½•ç»“æœ
                passed = result.passed
                error_msg = None
                
            except Exception as e:
                # æ•è·è¿è¡Œé”™è¯¯
                passed = False
                error_msg = str(e)
                sample_duration = time.time() - sample_start
                logger.error(f"  âŒ é”™è¯¯: {sample_id} - {error_msg}")
                self.stats['errors'] += 1
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total'] += 1
            if passed:
                self.stats['passed'] += 1
            else:
                self.stats['failed'] += 1
            
            # æŒ‰ç»´åº¦ç»Ÿè®¡
            for dim_name, dim_value in [
                ('by_operation', operation),
                ('by_language', lang),
            ]:
                self.stats[dim_name][dim_value]['total'] += 1
                if passed:
                    self.stats[dim_name][dim_value]['passed'] += 1
                else:
                    self.stats[dim_name][dim_value]['failed'] += 1
            
            # è®°å½•ç»“æœ
            test_result = {
                'sample_id': sample_id,
                'passed': passed,
                'duration': sample_duration,
                'class': class_info,
                'operation': operation,
                'error': error_msg,
            }
            
            if not passed and self.verbose:
                # è®°å½•å¤±è´¥è¯¦æƒ…
                if error_msg:
                    test_result['error_details'] = error_msg
                elif 'result' in locals():
                    test_result['failed_assertions'] = [
                        {'name': a.name, 'message': a.message}
                        for a in result.assertions if not a.passed
                    ]
            
            self.results.append(test_result)
            
            # æ˜¾ç¤ºç»“æœ - æ€»æ˜¯æ˜¾ç¤ºç»“æœ
            status = "âœ… PASS" if passed else "âŒ FAIL"
            print(f"  â†’ {status} ({sample_duration:.2f}s) | Pass: {self.stats['passed']}/{self.stats['total']}", flush=True)
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡æ±‡æ€»
            if idx % 10 == 0 or idx == total:
                pass_rate = (self.stats['passed'] / self.stats['total'] * 100) if self.stats['total'] > 0 else 0
                print(f"  ğŸ“Š å½“å‰ç»Ÿè®¡: Pass={self.stats['passed']}, Fail={self.stats['failed']}, Rate={pass_rate:.1f}%", flush=True)
        
        total_time = time.time() - start_time
        
        logger.info(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
        
        return self.stats
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
        
        # 1. ä¿å­˜æ‘˜è¦
        summary = {
            'metadata': {
                'run_id': self.run_id,
                'tested_at': datetime.now().isoformat(),
                'total_samples': self.stats['total'],
            },
            'summary': {
                'total': self.stats['total'],
                'passed': self.stats['passed'],
                'failed': self.stats['failed'],
                'errors': self.stats['errors'],
                'pass_rate': self.stats['passed'] / self.stats['total'] * 100 if self.stats['total'] > 0 else 0,
            },
            'by_operation': dict(self.stats['by_operation']),
            'by_language': dict(self.stats['by_language']),
        }
        
        summary_file = self.tests_dir / 'summary.json'
        with summary_file.open('w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… æ‘˜è¦: {summary_file}")
        
        # 2. åˆ†ç¦»é€šè¿‡å’Œå¤±è´¥çš„æ ·æœ¬
        passed_samples = []
        failed_samples = []
        
        for result in self.results:
            if result['passed']:
                passed_samples.append(result)
            else:
                failed_samples.append(result)
        
        # ä¿å­˜é€šè¿‡çš„æ ·æœ¬
        if passed_samples:
            passed_file = self.tests_dir / 'passed.jsonl'
            with passed_file.open('w', encoding='utf-8') as f:
                for result in passed_samples:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
            logger.info(f"  âœ… é€šè¿‡æ ·æœ¬: {passed_file} ({len(passed_samples)} ä¸ª)")
        
        # ä¿å­˜å¤±è´¥çš„æ ·æœ¬
        if failed_samples:
            failed_file = self.tests_dir / 'failed.jsonl'
            with failed_file.open('w', encoding='utf-8') as f:
                for result in failed_samples:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
            logger.info(f"  âŒ å¤±è´¥æ ·æœ¬: {failed_file} ({len(failed_samples)} ä¸ª)")
        
        # 3. ä¿å­˜å®Œæ•´ç»“æœ
        details_file = self.tests_dir / 'details.jsonl'
        with details_file.open('w', encoding='utf-8') as f:
            for result in self.results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        logger.info(f"  ğŸ“„ å®Œæ•´ç»“æœ: {details_file}")
        
        # 4. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.tests_dir / 'stats.json'
        with stats_file.open('w', encoding='utf-8') as f:
            json.dump({
                'total': self.stats['total'],
                'passed': self.stats['passed'],
                'failed': self.stats['failed'],
                'errors': self.stats['errors'],
                'pass_rate': self.stats['passed'] / self.stats['total'] * 100 if self.stats['total'] > 0 else 0,
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.tests_dir}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("="*80)
        
        print(f"\næ€»ä½“ç»“æœ:")
        print(f"  æ€»æ ·æœ¬æ•°: {self.stats['total']}")
        print(f"  é€šè¿‡: {self.stats['passed']} ({self.stats['passed']/self.stats['total']*100:.1f}%)")
        print(f"  å¤±è´¥: {self.stats['failed']} ({self.stats['failed']/self.stats['total']*100:.1f}%)")
        if self.stats['errors'] > 0:
            print(f"  é”™è¯¯: {self.stats['errors']}")
        
        # æŒ‰æ“ä½œç»Ÿè®¡
        print(f"\næŒ‰æ“ä½œç»Ÿè®¡:")
        for op, stat in sorted(self.stats['by_operation'].items(), key=lambda x: x[1]['total'], reverse=True):
            total = stat['total']
            passed = stat['passed']
            rate = passed / total * 100 if total > 0 else 0
            print(f"  {op}: {passed}/{total} ({rate:.1f}%)")
        
        # æŒ‰è¯­è¨€ç»Ÿè®¡
        print(f"\næŒ‰è¯­è¨€ç»Ÿè®¡:")
        for lang, stat in sorted(self.stats['by_language'].items(), key=lambda x: x[1]['total'], reverse=True):
            total = stat['total']
            passed = stat['passed']
            rate = passed / total * 100 if total > 0 else 0
            print(f"  {lang}: {passed}/{total} ({rate:.1f}%)")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkæµ‹è¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»æœ€æ–°rawåˆ›å»ºrunå¹¶æµ‹è¯•
  python -m bench.tools.test --raw latest
  
  # ä»æŒ‡å®šrawåˆ›å»ºrunå¹¶æµ‹è¯•
  python -m bench.tools.test --raw 20251015_131147
  
  # æµ‹è¯•å·²å­˜åœ¨çš„run
  python -m bench.tools.test --run 20251015_131147
  
  # åªæµ‹è¯•å‰10ä¸ªæ ·æœ¬
  python -m bench.tools.test --raw latest --limit 10 --verbose
        """
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--raw',
        help='ä»Raw IDåˆ›å»ºrunå¹¶æµ‹è¯• (å¦‚ "20251015_131147" æˆ– "latest")'
    )
    group.add_argument(
        '--run',
        help='æµ‹è¯•å·²å­˜åœ¨çš„Run ID (å¦‚ "20251015_131147" æˆ– "latest")'
    )
    parser.add_argument(
        '--limit', '-l',
        type=int,
        help='é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰'
    )
    parser.add_argument(
        '--timeout', '-t',
        type=float,
        help='æ¯ä¸ªæ ·æœ¬çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    try:
        runner = TestRunner(raw_id=args.raw, run_id=args.run, verbose=args.verbose)
    except (FileNotFoundError, ValueError) as e:
        logger.error(f"âŒ {e}")
        return 1
    
    try:
        # åŠ è½½æ ·æœ¬
        runner.load_samples()
        
        # è¿è¡Œæµ‹è¯•
        runner.run_tests(limit=args.limit, timeout=args.timeout)
        
        # ä¿å­˜ç»“æœ
        runner.save_results()
        
        # æ‰“å°æ‘˜è¦
        runner.print_summary()
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        
        # è¿”å›ç ï¼šå¦‚æœæœ‰å¤±è´¥åˆ™è¿”å›1
        return 1 if runner.stats['failed'] > 0 else 0
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
