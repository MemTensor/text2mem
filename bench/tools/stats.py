#!/usr/bin/env python3
"""
Benchmarkæ•°æ®ç»Ÿè®¡åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. ç»Ÿè®¡æ ·æœ¬åˆ†å¸ƒï¼ˆè¯­è¨€ã€åœºæ™¯ã€æ“ä½œã€æŒ‡ä»¤ç±»å‹ã€ç»“æ„ç­‰ï¼‰
2. åˆ†ææ•°æ®è´¨é‡æŒ‡æ ‡
3. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
4. æ£€æµ‹å¼‚å¸¸æ ·æœ¬

ç”¨æ³•ï¼š
    # ç»Ÿè®¡æœ€æ–°run
    python -m bench.tools.stats --run latest
    
    # ç»Ÿè®¡æŒ‡å®šrun
    python -m bench.tools.stats --run 20251015_131147
    
    # ç»Ÿè®¡æŒ‡å®šæ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
    python -m bench.tools.stats --input stage3.jsonl
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    python -m bench.tools.stats --run latest --verbose
"""

import argparse
import json
import logging
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BenchmarkStats:
    """Benchmarkæ•°æ®ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.samples: List[Dict[str, Any]] = []
        self.stats: Dict[str, Any] = {}
        
    def load_samples(self, input_file: Path) -> int:
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½æ ·æœ¬: {input_file}")
        
        if not input_file.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        
        count = 0
        with input_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æå¤±è´¥: {e}")
        
        logger.info(f"âœ… åŠ è½½ {count} ä¸ªæ ·æœ¬")
        return count
    
    def analyze(self) -> Dict[str, Any]:
        """åˆ†ææ ·æœ¬æ•°æ®"""
        logger.info("ğŸ“Š å¼€å§‹ç»Ÿè®¡åˆ†æ...")
        
        # åŸºæœ¬ç»Ÿè®¡
        total = len(self.samples)
        
        # åˆ†ç±»ç»Ÿè®¡
        langs = Counter()
        operations = Counter()
        instruction_types = Counter()
        structures = Counter()
        
        # ç»„åˆç»Ÿè®¡
        lang_op_combos = Counter()
        
        # è´¨é‡æŒ‡æ ‡
        has_nl = 0
        has_schema = 0
        has_expected = 0
        has_prerequisites = 0
        
        # å¼‚å¸¸æ£€æµ‹
        unknown_fields = []
        missing_fields = []
        
        # æ“ä½œåˆ†å¸ƒç»Ÿè®¡
        op_details = defaultdict(lambda: {
            'count': 0,
            'langs': Counter(),
            'instruction_types': Counter(),
            'structures': Counter(),
        })
        
        for idx, sample in enumerate(self.samples, 1):
            sample_id = sample.get('id', f'sample-{idx}')
            class_info = sample.get('class', {})
            
            # æå–åˆ†ç±»ä¿¡æ¯
            lang = class_info.get('lang', 'unknown')
            instruction_type = class_info.get('instruction_type', 'unknown')
            structure = class_info.get('structure', 'unknown')
            
            # ç»Ÿè®¡åˆ†ç±»
            langs[lang] += 1
            instruction_types[instruction_type] += 1
            structures[structure] += 1
            
            # æ£€æµ‹unknown
            if 'unknown' in [lang, instruction_type, structure]:
                unknown_fields.append({
                    'sample_id': sample_id,
                    'fields': {
                        'lang': lang,
                        'instruction_type': instruction_type,
                        'structure': structure,
                    }
                })
            
            # æå–æ“ä½œ
            schema_list = sample.get('schema_list', [])
            if schema_list:
                # ä¸»æ“ä½œï¼ˆç¬¬ä¸€ä¸ªï¼‰
                main_op = schema_list[0].get('op', 'unknown')
                operations[main_op] += 1
                
                # ç»„åˆç»Ÿè®¡
                lang_op_combos[f"{lang}-{main_op}"] += 1
                
                # æ“ä½œè¯¦ç»†ç»Ÿè®¡
                op_details[main_op]['count'] += 1
                op_details[main_op]['langs'][lang] += 1
                op_details[main_op]['instruction_types'][instruction_type] += 1
                op_details[main_op]['structures'][structure] += 1
                
                # å·¥ä½œæµä¸­çš„æ‰€æœ‰æ“ä½œ
                if len(schema_list) > 1:
                    workflow_ops = [s.get('op') for s in schema_list]
                    # è®°å½•å·¥ä½œæµæ¨¡å¼
                    # operations[f"workflow:{'+'.join(workflow_ops)}"] += 1
            
            # è´¨é‡æ£€æŸ¥
            if sample.get('nl'):
                has_nl += 1
            if schema_list:
                has_schema += 1
            if sample.get('expected'):
                has_expected += 1
            if sample.get('prerequisites'):
                has_prerequisites += 1
            
            # æ£€æŸ¥å¿…å¡«å­—æ®µ
            required_fields = ['id', 'class', 'nl', 'schema_list']
            for field in required_fields:
                if field not in sample or not sample[field]:
                    missing_fields.append({
                        'sample_id': sample_id,
                        'missing_field': field
                    })
        
        # æ„å»ºç»Ÿè®¡ç»“æœ
        self.stats = {
            'metadata': {
                'analyzed_at': datetime.now().isoformat(),
                'total_samples': total,
        # æ„å»ºç»Ÿè®¡ç»“æœ
        self.stats = {
            'metadata': {
                'analyzed_at': datetime.now().isoformat(),
                'total_samples': total,
            },
            'distribution': {
                'languages': dict(langs.most_common()),
                'operations': dict(operations.most_common()),
                'instruction_types': dict(instruction_types.most_common()),
                'structures': dict(structures.most_common()),
            },
            'combinations': {
                'lang_operation': dict(lang_op_combos.most_common(20)),  # Top 20
            },
            'operation_details': {
                op: {
                    'count': details['count'],
                    'percentage': details['count'] / total * 100,
                    'langs': dict(details['langs'].most_common()),
                    'instruction_types': dict(details['instruction_types'].most_common()),
                    'structures': dict(details['structures'].most_common()),
                }
                for op, details in sorted(op_details.items(), key=lambda x: x[1]['count'], reverse=True)
            },
            'quality': {
                'has_nl': has_nl,
                'has_nl_percentage': has_nl / total * 100 if total > 0 else 0,
                'has_schema': has_schema,
                'has_schema_percentage': has_schema / total * 100 if total > 0 else 0,
                'has_expected': has_expected,
                'has_expected_percentage': has_expected / total * 100 if total > 0 else 0,
                'has_prerequisites': has_prerequisites,
                'has_prerequisites_percentage': has_prerequisites / total * 100 if total > 0 else 0,
            },
            'issues': {
                'unknown_fields_count': len(unknown_fields),
                'unknown_fields': unknown_fields[:10] if not self.verbose else unknown_fields,  # åªæ˜¾ç¤ºå‰10ä¸ª
                'missing_fields_count': len(missing_fields),
                'missing_fields': missing_fields[:10] if not self.verbose else missing_fields,
            }
        }
        
        logger.info("âœ… ç»Ÿè®¡åˆ†æå®Œæˆ")
        return self.stats
    
    def print_report(self):
        """æ‰“å°ç»Ÿè®¡æŠ¥å‘Š"""
        if not self.stats:
            logger.error("âŒ è¯·å…ˆè¿è¡Œ analyze()")
            return
        
        stats = self.stats
        
        print("\n" + "="*80)
        print("ğŸ“Š Benchmark æ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“ åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {stats['metadata']['total_samples']}")
        print(f"  åˆ†ææ—¶é—´: {stats['metadata']['analyzed_at']}")
        
        # åˆ†å¸ƒç»Ÿè®¡
        print(f"\nğŸ“ˆ åˆ†å¸ƒç»Ÿè®¡:")
        
        print(f"\n  è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in stats['distribution']['languages'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {lang}: {count} ({pct:.1f}%)")
        
        print(f"\n  æ“ä½œåˆ†å¸ƒ:")
        for op, count in stats['distribution']['operations'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {op}: {count} ({pct:.1f}%)")
        
        print(f"\n  æŒ‡ä»¤ç±»å‹åˆ†å¸ƒ:")
        for itype, count in stats['distribution']['instruction_types'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {itype}: {count} ({pct:.1f}%)")
        
        print(f"\n  ç»“æ„åˆ†å¸ƒ:")
        for struct, count in stats['distribution']['structures'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {struct}: {count} ({pct:.1f}%)")
        
        # è´¨é‡æŒ‡æ ‡
        print(f"\nâœ… è´¨é‡æŒ‡æ ‡:")
        quality = stats['quality']
        print(f"  å®Œæ•´NL: {quality['has_nl']} ({quality['has_nl_percentage']:.1f}%)")
        print(f"  å®Œæ•´Schema: {quality['has_schema']} ({quality['has_schema_percentage']:.1f}%)")
        print(f"  å®Œæ•´Expected: {quality['has_expected']} ({quality['has_expected_percentage']:.1f}%)")
        print(f"  æœ‰Prerequisites: {quality['has_prerequisites']} ({quality['has_prerequisites_percentage']:.1f}%)")
        
        # é—®é¢˜æ£€æµ‹
        print(f"\nâš ï¸  é—®é¢˜æ£€æµ‹:")
        issues = stats['issues']
        print(f"  åŒ…å«unknownçš„æ ·æœ¬: {issues['unknown_fields_count']}")
        print(f"  ç¼ºå°‘å¿…å¡«å­—æ®µçš„æ ·æœ¬: {issues['missing_fields_count']}")
        
        if issues['unknown_fields_count'] > 0 and self.verbose:
            print(f"\n  åŒ…å«unknownçš„æ ·æœ¬è¯¦æƒ…:")
            for item in issues['unknown_fields'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    {item['sample_id']}: {item['fields']}")
        
        if issues['missing_fields_count'] > 0 and self.verbose:
            print(f"\n  ç¼ºå°‘å­—æ®µçš„æ ·æœ¬è¯¦æƒ…:")
            for item in issues['missing_fields'][:5]:
                print(f"    {item['sample_id']}: missing {item['missing_field']}")
        
        # Topç»„åˆ
        print(f"\nğŸ” Top 10 è¯­è¨€-æ“ä½œç»„åˆ:")
        for combo, count in list(stats['combinations']['lang_operation'].items())[:10]:
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {combo}: {count} ({pct:.1f}%)")
        
        print("\n" + "="*80)
    
    def save_report(self, output_file: Path):
        """ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
        if not self.stats:
            logger.error("âŒ è¯·å…ˆè¿è¡Œ analyze()")
            return
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with output_file.open('w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkæ•°æ®ç»Ÿè®¡åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç»Ÿè®¡æœ€æ–°run
  python -m bench.tools.stats --run latest
  
  # ç»Ÿè®¡æŒ‡å®šrun
  python -m bench.tools.stats --run 20251015_131147
  
  # ç»Ÿè®¡æŒ‡å®šæ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
  python -m bench.tools.stats --input stage3.jsonl
  
  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šå¹¶ä¿å­˜
  python -m bench.tools.stats --run latest --verbose
        """
    )
    
    parser.add_argument(
        '--run', '-r',
        help='Run ID (å¦‚ "20251015_131147" æˆ– "latest")'
    )
    parser.add_argument(
        '--input', '-i',
        type=Path,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼Œç›´æ¥æŒ‡å®šæ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        help='è¾“å‡ºç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œé»˜è®¤ä¿å­˜åˆ°runç›®å½•'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.run:
        # ä½¿ç”¨run ID
        run_manager = RunManager()
        try:
            input_file = run_manager.get_stage_file(args.run, 3)
            if not input_file.exists():
                logger.error(f"âŒ Run {args.run} æ²¡æœ‰stage3æ•°æ®")
                logger.info(f"   æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
                return 1
            logger.info(f"ğŸ“‚ ä½¿ç”¨run: {args.run}")
        except FileNotFoundError as e:
            logger.error(f"âŒ {e}")
            return 1
    elif args.input:
        # ç›´æ¥æŒ‡å®šæ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
        input_file = args.input
    else:
        # é»˜è®¤ä½¿ç”¨latest
        run_manager = RunManager()
        latest_run = run_manager.get_latest_run()
        if not latest_run:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•run")
            logger.info("ğŸ’¡ æç¤ºï¼šè¯·å…ˆè¿è¡Œç”Ÿæˆå·¥å…·")
            logger.info("   python bench/generate/generate.py")
            return 1
        
        try:
            input_file = run_manager.get_stage_file('latest', 3)
            logger.info(f"ğŸ” è‡ªåŠ¨ä½¿ç”¨æœ€æ–°run: {latest_run}")
        except FileNotFoundError as e:
            logger.error(f"âŒ {e}")
            return 1
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not input_file.exists():
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return 1
    
    # åˆ›å»ºç»Ÿè®¡å™¨
    analyzer = BenchmarkStats(verbose=args.verbose)
    
    try:
        # åŠ è½½æ ·æœ¬
        analyzer.load_samples(input_file)
        
        # åˆ†æ
        analyzer.analyze()
        
        # æ‰“å°æŠ¥å‘Š
        analyzer.print_report()
        
        # ä¿å­˜æŠ¥å‘Š
        if args.output:
            analyzer.save_report(args.output)
        else:
            # é»˜è®¤ä¿å­˜åˆ°è¾“å…¥æ–‡ä»¶åŒç›®å½•
            default_output = input_file.parent / 'stats.json'
            analyzer.save_report(default_output)
        
        print(f"\nâœ… ç»Ÿè®¡å®Œæˆï¼")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
