#!/usr/bin/env python3
"""
Benchmarkdataç»Ÿè®¡åˆ†æå·¥å…·

Features:
1. ç»Ÿè®¡sampleåˆ†å¸ƒï¼ˆè¯­è¨€ã€åœºæ™¯ã€æ“ä½œã€æŒ‡ä»¤typeã€ç»“æ„ç­‰ï¼‰
2. åˆ†ædataè´¨é‡æŒ‡æ ‡
3. generateç»Ÿè®¡report
4. æ£€æµ‹å¼‚å¸¸sample

Usage:
    # ç»Ÿè®¡latestrun
    python -m bench.tools.stats --run latest
    
    # ç»Ÿè®¡specifiedrun
    python -m bench.tools.stats --run 20251015_131147
    
    # ç»Ÿè®¡specifiedfileï¼ˆå‘åå…¼å®¹ï¼‰
    python -m bench.tools.stats --input stage3.jsonl
    
    # generateè¯¦ç»†report
    python -m bench.tools.stats --run latest --verbose
"""

import argparse
import json
import logging
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BenchmarkStats:
    """Benchmarkdataç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.samples: List[Dict[str, Any]] = []
        self.stats: Dict[str, Any] = {}
        
    def load_samples(self, input_file: Path) -> int:
        """loadsample countæ®"""
        logger.info(f"ğŸ“‚ loadsample: {input_file}")
        
        if not input_file.exists():
            raise FileNotFoundError(f"filedoes not exist: {input_file}")
        
        count = 0
        with input_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æfailed: {e}")
        
        logger.info(f"âœ… load {count} ä¸ªsample")
        return count
    
    def analyze(self) -> Dict[str, Any]:
        """åˆ†æsample countæ®"""
        logger.info("ğŸ“Š startç»Ÿè®¡åˆ†æ...")
        
        # åŸºæœ¬ç»Ÿè®¡
        total = len(self.samples)
        
        # åˆ†ç±»ç»Ÿè®¡
        langs = Counter()
        operations = Counter()
        instruction_types = Counter()
        structures = Counter()
        
        # ç»„åˆç»Ÿè®¡
        lang_op_combos = Counter()
        
        # è´¨é‡æŒ‡æ ‡
        has_nl = 0
        has_schema = 0
        has_expected = 0
        has_prerequisites = 0
        
        # å¼‚å¸¸æ£€æµ‹
        unknown_fields = []
        missing_fields = []
        
        # æ“ä½œåˆ†å¸ƒç»Ÿè®¡
        op_details = defaultdict(lambda: {
            'count': 0,
            'langs': Counter(),
            'instruction_types': Counter(),
            'structures': Counter(),
        })
        
        for idx, sample in enumerate(self.samples, 1):
            sample_id = sample.get('id', f'sample-{idx}')
            class_info = sample.get('class', {})
            
            # æå–åˆ†ç±»ä¿¡æ¯
            lang = class_info.get('lang', 'unknown')
            instruction_type = class_info.get('instruction_type', 'unknown')
            structure = class_info.get('structure', 'unknown')
            
            # ç»Ÿè®¡åˆ†ç±»
            langs[lang] += 1
            instruction_types[instruction_type] += 1
            structures[structure] += 1
            
            # æ£€æµ‹unknown
            if 'unknown' in [lang, instruction_type, structure]:
                unknown_fields.append({
                    'sample_id': sample_id,
                    'fields': {
                        'lang': lang,
                        'instruction_type': instruction_type,
                        'structure': structure,
                    }
                })
            
            # æå–æ“ä½œ
            schema_list = sample.get('schema_list', [])
            if schema_list:
                # ä¸»æ“ä½œï¼ˆç¬¬ä¸€ä¸ªï¼‰
                main_op = schema_list[0].get('op', 'unknown')
                operations[main_op] += 1
                
                # ç»„åˆç»Ÿè®¡
                lang_op_combos[f"{lang}-{main_op}"] += 1
                
                # æ“ä½œè¯¦ç»†ç»Ÿè®¡
                op_details[main_op]['count'] += 1
                op_details[main_op]['langs'][lang] += 1
                op_details[main_op]['instruction_types'][instruction_type] += 1
                op_details[main_op]['structures'][structure] += 1
                
                # å·¥ä½œæµä¸­çš„allæ“ä½œ
                if len(schema_list) > 1:
                    workflow_ops = [s.get('op') for s in schema_list]
                    # recordå·¥ä½œæµmode
                    # operations[f"workflow:{'+'.join(workflow_ops)}"] += 1
            
            # è´¨é‡check
            if sample.get('nl'):
                has_nl += 1
            if schema_list:
                has_schema += 1
            if sample.get('expected'):
                has_expected += 1
            if sample.get('prerequisites'):
                has_prerequisites += 1
            
            # checkå¿…å¡«å­—æ®µ
            required_fields = ['id', 'class', 'nl', 'schema_list']
            for field in required_fields:
                if field not in sample or not sample[field]:
                    missing_fields.append({
                        'sample_id': sample_id,
                        'missing_field': field
                    })
        
        # Build statistics result
        self.stats = {
            'metadata': {
                'analyzed_at': datetime.now().isoformat(),
                'total_samples': total,
            },
            'distribution': {
                'languages': dict(langs.most_common()),
                'operations': dict(operations.most_common()),
                'instruction_types': dict(instruction_types.most_common()),
                'structures': dict(structures.most_common()),
            },
            'combinations': {
                'lang_operation': dict(lang_op_combos.most_common(20)),  # Top 20
            },
            'operation_details': {
                op: {
                    'count': details['count'],
                    'percentage': details['count'] / total * 100,
                    'langs': dict(details['langs'].most_common()),
                    'instruction_types': dict(details['instruction_types'].most_common()),
                    'structures': dict(details['structures'].most_common()),
                }
                for op, details in sorted(op_details.items(), key=lambda x: x[1]['count'], reverse=True)
            },
            'quality': {
                'has_nl': has_nl,
                'has_nl_percentage': has_nl / total * 100 if total > 0 else 0,
                'has_schema': has_schema,
                'has_schema_percentage': has_schema / total * 100 if total > 0 else 0,
                'has_expected': has_expected,
                'has_expected_percentage': has_expected / total * 100 if total > 0 else 0,
                'has_prerequisites': has_prerequisites,
                'has_prerequisites_percentage': has_prerequisites / total * 100 if total > 0 else 0,
            },
            'issues': {
                'unknown_fields_count': len(unknown_fields),
                'unknown_fields': unknown_fields[:10] if not self.verbose else unknown_fields,  # åªæ˜¾ç¤ºå‰10ä¸ª
                'missing_fields_count': len(missing_fields),
                'missing_fields': missing_fields[:10] if not self.verbose else missing_fields,
            }
        }
        
        logger.info("âœ… ç»Ÿè®¡åˆ†æcomplete")
        return self.stats
    
    def print_report(self):
        """æ‰“å°ç»Ÿè®¡report"""
        if not self.stats:
            logger.error("âŒ è¯·å…ˆrun analyze()")
            return
        
        stats = self.stats
        
        print("\n" + "="*80)
        print("ğŸ“Š Benchmark dataç»Ÿè®¡report")
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“ åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»sample count: {stats['metadata']['total_samples']}")
        print(f"  åˆ†ætime: {stats['metadata']['analyzed_at']}")
        
        # åˆ†å¸ƒç»Ÿè®¡
        print(f"\nğŸ“ˆ åˆ†å¸ƒç»Ÿè®¡:")
        
        print(f"\n  è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in stats['distribution']['languages'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {lang}: {count} ({pct:.1f}%)")
        
        print(f"\n  æ“ä½œåˆ†å¸ƒ:")
        for op, count in stats['distribution']['operations'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {op}: {count} ({pct:.1f}%)")
        
        print(f"\n  æŒ‡ä»¤typeåˆ†å¸ƒ:")
        for itype, count in stats['distribution']['instruction_types'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {itype}: {count} ({pct:.1f}%)")
        
        print(f"\n  ç»“æ„åˆ†å¸ƒ:")
        for struct, count in stats['distribution']['structures'].items():
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {struct}: {count} ({pct:.1f}%)")
        
        # è´¨é‡æŒ‡æ ‡
        print(f"\nâœ… è´¨é‡æŒ‡æ ‡:")
        quality = stats['quality']
        print(f"  å®Œæ•´NL: {quality['has_nl']} ({quality['has_nl_percentage']:.1f}%)")
        print(f"  å®Œæ•´Schema: {quality['has_schema']} ({quality['has_schema_percentage']:.1f}%)")
        print(f"  å®Œæ•´Expected: {quality['has_expected']} ({quality['has_expected_percentage']:.1f}%)")
        print(f"  æœ‰Prerequisites: {quality['has_prerequisites']} ({quality['has_prerequisites_percentage']:.1f}%)")
        
        # é—®é¢˜æ£€æµ‹
        print(f"\nâš ï¸  é—®é¢˜æ£€æµ‹:")
        issues = stats['issues']
        print(f"  includeunknownçš„sample: {issues['unknown_fields_count']}")
        print(f"  ç¼ºå°‘å¿…å¡«å­—æ®µçš„sample: {issues['missing_fields_count']}")
        
        if issues['unknown_fields_count'] > 0 and self.verbose:
            print(f"\n  includeunknownçš„sampleè¯¦æƒ…:")
            for item in issues['unknown_fields'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    {item['sample_id']}: {item['fields']}")
        
        if issues['missing_fields_count'] > 0 and self.verbose:
            print(f"\n  ç¼ºå°‘å­—æ®µçš„sampleè¯¦æƒ…:")
            for item in issues['missing_fields'][:5]:
                print(f"    {item['sample_id']}: missing {item['missing_field']}")
        
        # Topç»„åˆ
        print(f"\nğŸ” Top 10 è¯­è¨€-æ“ä½œç»„åˆ:")
        for combo, count in list(stats['combinations']['lang_operation'].items())[:10]:
            pct = count / stats['metadata']['total_samples'] * 100
            print(f"    {combo}: {count} ({pct:.1f}%)")
        
        print("\n" + "="*80)
    
    def save_report(self, output_file: Path):
        """saveç»Ÿè®¡reporttoJSONfile"""
        if not self.stats:
            logger.error("âŒ è¯·å…ˆrun analyze()")
            return
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with output_file.open('w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç»Ÿè®¡reportalreadysave: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkdataç»Ÿè®¡åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
example:
  # ç»Ÿè®¡latestrun
  python -m bench.tools.stats --run latest
  
  # ç»Ÿè®¡specifiedrun
  python -m bench.tools.stats --run 20251015_131147
  
  # ç»Ÿè®¡specifiedfileï¼ˆå‘åå…¼å®¹ï¼‰
  python -m bench.tools.stats --input stage3.jsonl
  
  # generateè¯¦ç»†reportå¹¶save
  python -m bench.tools.stats --run latest --verbose
        """
    )
    
    parser.add_argument(
        '--run', '-r',
        help='Run ID (å¦‚ "20251015_131147" æˆ– "latest")'
    )
    parser.add_argument(
        '--input', '-i',
        type=Path,
        help='è¾“å…¥filepathï¼ˆå‘åå…¼å®¹ï¼Œç›´æ¥specifiedfileï¼‰'
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        help='è¾“å‡ºç»Ÿè®¡reportfilepathï¼ˆJSONformatï¼‰ï¼Œdefaultsavetorundirectory'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å…¥file
    if args.run:
        # userun ID
        run_manager = RunManager()
        try:
            input_file = run_manager.get_stage_file(args.run, 3)
            if not input_file.exists():
                logger.error(f"âŒ Run {args.run} æ²¡æœ‰stage3data")
                logger.info(f"   filedoes not exist: {input_file}")
                return 1
            logger.info(f"ğŸ“‚ userun: {args.run}")
        except FileNotFoundError as e:
            logger.error(f"âŒ {e}")
            return 1
    elif args.input:
        # ç›´æ¥specifiedfileï¼ˆå‘åå…¼å®¹ï¼‰
        input_file = args.input
    else:
        # defaultuselatest
        run_manager = RunManager()
        latest_run = run_manager.get_latest_run()
        if not latest_run:
            logger.error("âŒ æ²¡æœ‰foundä»»ä½•run")
            logger.info("ğŸ’¡ æç¤ºï¼šè¯·å…ˆrungenerateå·¥å…·")
            logger.info("   python bench/generate/generate.py")
            return 1
        
        try:
            input_file = run_manager.get_stage_file('latest', 3)
            logger.info(f"ğŸ” è‡ªåŠ¨uselatestrun: {latest_run}")
        except FileNotFoundError as e:
            logger.error(f"âŒ {e}")
            return 1
    
    # checkè¾“å…¥file
    if not input_file.exists():
        logger.error(f"âŒ è¾“å…¥filedoes not exist: {input_file}")
        return 1
    
    # createç»Ÿè®¡å™¨
    analyzer = BenchmarkStats(verbose=args.verbose)
    
    try:
        # loadsample
        analyzer.load_samples(input_file)
        
        # åˆ†æ
        analyzer.analyze()
        
        # æ‰“å°report
        analyzer.print_report()
        
        # savereport
        if args.output:
            analyzer.save_report(args.output)
        else:
            # defaultsavetoè¾“å…¥fileåŒdirectory
            default_output = input_file.parent / 'stats.json'
            analyzer.save_report(default_output)
        
        print(f"\nâœ… ç»Ÿè®¡completeï¼")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
