<div align="center">

# Bench Tools | å·¥å…·é›†åˆ

**Core toolkit for Text2Mem Benchmark data processing**  
**Text2Mem Benchmark çš„æ ¸å¿ƒå·¥å…·é›†**

</div>

---

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

# English

Core toolkit for Text2Mem Benchmark: generation, testing, cleaning, and building.

## ğŸ“ Tool Structure

```
bench/tools/
â”œâ”€â”€ Core Tools (Data Processing Pipeline)
â”‚   â”œâ”€â”€ run_manager.py      # Run directory management (core module)
â”‚   â”œâ”€â”€ test.py             # Test runner
â”‚   â”œâ”€â”€ clean.py            # Data cleaning
â”‚   â”œâ”€â”€ build.py            # Benchmark building
â”‚   â”œâ”€â”€ pipeline.py         # Complete workflow
â”‚   â””â”€â”€ stats.py            # Statistical analysis
â”‚
â”œâ”€â”€ Utility Tools
â”‚   â”œâ”€â”€ clock.py                # Virtual clock
â”‚   â”œâ”€â”€ sql_builder_sqlite.py   # SQL builder
â”‚   â””â”€â”€ create_empty_db.py      # Create empty database
â”‚
â””â”€â”€ _archive/               # Archived old tools
```

## ğŸš€ Quick Start

### One-Command Complete Pipeline (Recommended)

```bash
# Generate benchmark from latest raw data
python -m bench.tools.pipeline --raw latest --version v2

# Generate from specific raw
python -m bench.tools.pipeline --raw 20251022_184604 --version v2
```

### Step-by-Step Execution (Advanced)

```bash
# 1. Test - Create run from raw and run tests
python -m bench.tools.test --raw latest

# 2. Clean - Filter failed samples, apply rules
python -m bench.tools.clean --run latest

# 3. Build - Reassign IDs, generate final benchmark
python -m bench.tools.build --run latest --version v2

# 4. Stats - Analyze sample distribution and quality
python -m bench.tools.stats --run latest
```

## ğŸ“‹ Tool Details

### Core Tools

#### 1. run_manager.py - Run Directory Management

Core module for managing data directory structure.

```python
from bench.tools.run_manager import RunManager

manager = RunManager()
latest_raw = manager.get_latest_raw()
run_dir = manager.create_run_from_raw(latest_raw)
```

**Directory Structure**:
- `raw/` - Raw generation output
- `runs/` - Tested and cleaned data
- `benchmarks/` - Final benchmarks

#### 2. test.py - Test Runner

Create run from raw and execute tests, identifying passed/failed samples.

```bash
# Create run from latest raw and test
python -m bench.tools.test --raw latest

# Create run from specific raw
python -m bench.tools.test --raw 20251022_184604

# Test existing run
python -m bench.tools.test --run 20251022_184604

# Test only first N samples (debugging)
python -m bench.tools.test --raw latest --limit 10
```

**Output**:
- `runs/{RUN_ID}/tests/passed.jsonl` - Passed samples
- `runs/{RUN_ID}/tests/failed.jsonl` - Failed samples
- `runs/{RUN_ID}/tests/summary.json` - Test summary

#### 3. clean.py - Data Cleaning

Filter samples from test results, apply filtering rules.

```bash
# Clean latest run
python -m bench.tools.clean --run latest

# Clean specific run
python -m bench.tools.clean --run 20251022_184604

# Don't filter unknown fields
python -m bench.tools.clean --run latest --no-filter-unknown

# Don't filter failed samples
python -m bench.tools.clean --run latest --no-filter-failed
```

**Filtering Rules**:
1. Filter test-failed samples (if test results exist)
2. Filter samples containing 'unknown'
3. Keep only 'direct' and 'indirect' instruction types
4. Keep only 'single' and 'workflow' structures
5. Keep only 12 core operations

**Output**:
- `runs/{RUN_ID}/cleaned/cleaned.jsonl` - Cleaned samples
- `runs/{RUN_ID}/cleaned/metadata.json` - Metadata
- `runs/{RUN_ID}/cleaned/filter_report.json` - Filter report

#### 4. build.py - Benchmark Building

Build final benchmark from cleaned data.

```bash
# Build benchmark from latest run
python -m bench.tools.build --run latest --version v2

# Build from specific run
python -m bench.tools.build --run 20251022_184604 --version v2

# Don't reassign IDs
python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
```

**Features**:
- Reassign sample IDs (grouped by category)
- Generate metadata and statistics
- Support version management

**Output**:
- `benchmarks/{VERSION}/benchmark.jsonl` - Final benchmark
- `benchmarks/{VERSION}/metadata.json` - Metadata
- `benchmarks/{VERSION}/stats.json` - Statistics

#### 5. pipeline.py - Complete Workflow

Automate complete data processing pipeline.

```bash
# Process latest raw
python -m bench.tools.pipeline --raw latest --version v2

# Process specific raw
python -m bench.tools.pipeline --raw 20251022_184604 --version v2

# Skip test step (run must already exist)
python -m bench.tools.pipeline --raw latest --version v2 --skip-tests

# Show verbose output
python -m bench.tools.pipeline --raw latest --version v2 --verbose
```

**Pipeline**:
1. Run tests (create run)
2. Clean data
3. Build benchmark

#### 6. stats.py - Statistical Analysis

Analyze sample distribution and quality metrics.

```bash
# Stats for latest run
python -m bench.tools.stats --run latest

# Stats for specific run
python -m bench.tools.stats --run 20251022_184604

# Stats for specific file
python -m bench.tools.stats --input stage3.jsonl

# Generate detailed report
python -m bench.tools.stats --run latest --verbose

# Save report to file
python -m bench.tools.stats --run latest --output report.json
```

**Statistics**:
- Sample distribution (language, operation, instruction type, structure)
- Quality metrics (completeness, validity)
- Issue detection (unknown fields, missing fields)
- Top combination statistics

### Utility Tools

#### 7. clock.py - Virtual Clock

Used for time simulation in benchmarks.

```python
from bench.tools.clock import VirtualClock

clock = VirtualClock()
# Use for time-related operation simulation
```

#### 8. sql_builder_sqlite.py - SQL Builder

Compile test assertions into SQL queries.

```python
from bench.tools.sql_builder_sqlite import SQLiteAssertionCompiler

compiler = SQLiteAssertionCompiler()
compiled = compiler.compile(assertion)
```

#### 9. create_empty_db.py - Create Empty Database

Create Text2Mem standard empty database.

```bash
# Create in-memory database (testing)
python bench/tools/create_empty_db.py

# Create file database
python bench/tools/create_empty_db.py --output /path/to/database.db

# Verify schema
python bench/tools/create_empty_db.py --verify /path/to/database.db
```

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Benchmark Data Flow                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Generate
   â””â”€> bench/data/raw/{TIMESTAMP}/
       â”œâ”€â”€ stage1.jsonl  (NL instructions)
       â”œâ”€â”€ stage2.jsonl  (IR samples)
       â””â”€â”€ stage3.jsonl  (Complete samples)

2. Test
   â””â”€> bench/data/runs/{TIMESTAMP}/tests/
       â”œâ”€â”€ passed.jsonl   (Passed samples)
       â”œâ”€â”€ failed.jsonl   (Failed samples)
       â””â”€â”€ summary.json   (Test summary)

3. Clean
   â””â”€> bench/data/runs/{TIMESTAMP}/cleaned/
       â”œâ”€â”€ cleaned.jsonl       (Cleaned samples)
       â”œâ”€â”€ metadata.json       (Metadata)
       â””â”€â”€ filter_report.json  (Filter report)

4. Build
   â””â”€> bench/data/benchmarks/{VERSION}/
       â”œâ”€â”€ benchmark.jsonl  (Final benchmark)
       â”œâ”€â”€ metadata.json    (Metadata)
       â””â”€â”€ stats.json       (Statistics)
```

## ğŸ”§ Advanced Usage

### Custom Filtering Rules

Modify filtering rules in `clean.py`:

```python
class DataCleaner:
    ALLOWED_INSTRUCTION_TYPES = {'direct', 'indirect'}
    ALLOWED_STRUCTURES = {'single', 'workflow'}
    ALLOWED_OPERATIONS = {
        'Encode', 'Retrieve', 'Update', 'Delete', 
        'Summarize', 'Label', 'Promote', 'Demote',
        'Expire', 'Lock', 'Merge', 'Split',
    }
```

### Batch Process Multiple Raws

```bash
# Process all raws
for raw in bench/data/raw/*/; do
    raw_id=$(basename $raw)
    python -m bench.tools.pipeline --raw $raw_id --version "v_$raw_id"
done
```

### Compare Different Benchmark Versions

```bash
# Stats for v1
python -m bench.tools.stats --input bench/data/benchmarks/v1/benchmark.jsonl

# Stats for v2
python -m bench.tools.stats --input bench/data/benchmarks/v2/benchmark.jsonl
```

## ğŸ“ FAQ

### Q: How to generate new benchmark from scratch?

```bash
# 1. Generate raw data
python bench/generate/generate.py

# 2. Run complete workflow
python -m bench.tools.pipeline --raw latest --version v2
```

### Q: How to retest without generating new data?

```bash
# Retest from existing raw
python -m bench.tools.test --raw 20251022_184604
```

### Q: How to debug test failures?

```bash
# 1. Test only first few samples
python -m bench.tools.test --raw latest --limit 5 --verbose

# 2. View failed samples
cat bench/data/runs/latest/tests/failed.jsonl
```

### Q: How to customize benchmark version?

```bash
# Use custom version number
python -m bench.tools.pipeline --raw latest --version v2.1-custom
```

## ğŸ“š Related Documentation

- [Benchmark README](../README.md) - Overall description
- [Generation Tool Docs](../generate/QUICK_REFERENCE.md) - Data generation
- [Workflow Docs](../WORKFLOW.md) - Complete workflow

## ğŸ—‚ï¸ Archived Tools

Archived tools are saved in `_archive/` directory, including:
- `clean_benchmark.py` - Old cleaning tool
- `migrate_data.py` - Data structure migration script
- `migrate_to_v3.py` - v3 migration script
- `verify_setup.py` - Setup verification tool

See [_archive/README.md](_archive/README.md) for details.

---

# ä¸­æ–‡

Text2Mem Benchmark çš„æ ¸å¿ƒå·¥å…·é›†ï¼Œç”¨äºæ•°æ®ç”Ÿæˆã€æµ‹è¯•ã€æ¸…æ´—å’Œæ„å»ºã€‚

## ğŸ“ å·¥å…·ç»“æ„

```
bench/tools/
â”œâ”€â”€ æ ¸å¿ƒå·¥å…·ï¼ˆæ•°æ®å¤„ç†æµç¨‹ï¼‰
â”‚   â”œâ”€â”€ run_manager.py      # Runç›®å½•ç®¡ç†ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰
â”‚   â”œâ”€â”€ test.py             # æµ‹è¯•è¿è¡Œå™¨
â”‚   â”œâ”€â”€ clean.py            # æ•°æ®æ¸…æ´—
â”‚   â”œâ”€â”€ build.py            # Benchmarkæ„å»º
â”‚   â”œâ”€â”€ pipeline.py         # å®Œæ•´æµç¨‹
â”‚   â””â”€â”€ stats.py            # ç»Ÿè®¡åˆ†æ
â”‚
â”œâ”€â”€ å®ç”¨å·¥å…·
â”‚   â”œâ”€â”€ clock.py                # è™šæ‹Ÿæ—¶é’Ÿ
â”‚   â”œâ”€â”€ sql_builder_sqlite.py   # SQLæ„å»ºå™¨
â”‚   â””â”€â”€ create_empty_db.py      # åˆ›å»ºç©ºæ•°æ®åº“
â”‚
â””â”€â”€ _archive/               # å½’æ¡£çš„æ—§å·¥å…·
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰

```bash
# ä»æœ€æ–°rawæ•°æ®ç”Ÿæˆbenchmark
python -m bench.tools.pipeline --raw latest --version v2

# ä»æŒ‡å®šrawç”Ÿæˆ
python -m bench.tools.pipeline --raw 20251022_184604 --version v2
```

### åˆ†æ­¥æ‰§è¡Œï¼ˆé«˜çº§ç”¨æ³•ï¼‰

```bash
# 1. æµ‹è¯• - ä»rawåˆ›å»ºrunå¹¶è¿è¡Œæµ‹è¯•
python -m bench.tools.test --raw latest

# 2. æ¸…æ´— - è¿‡æ»¤å¤±è´¥æ ·æœ¬ï¼Œåº”ç”¨è§„åˆ™
python -m bench.tools.clean --run latest

# 3. æ„å»º - é‡æ–°åˆ†é…IDï¼Œç”Ÿæˆæœ€ç»ˆbenchmark
python -m bench.tools.build --run latest --version v2

# 4. ç»Ÿè®¡ - åˆ†ææ ·æœ¬åˆ†å¸ƒå’Œè´¨é‡
python -m bench.tools.stats --run latest
```

## ğŸ“‹ å·¥å…·è¯¦è§£

### æ ¸å¿ƒå·¥å…·

#### 1. run_manager.py - Runç›®å½•ç®¡ç†

æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ç®¡ç†æ•°æ®ç›®å½•ç»“æ„ã€‚

```python
from bench.tools.run_manager import RunManager

manager = RunManager()
latest_raw = manager.get_latest_raw()
run_dir = manager.create_run_from_raw(latest_raw)
```

**ç›®å½•ç»“æ„**:
- `raw/` - åŸå§‹ç”Ÿæˆè¾“å‡º
- `runs/` - æµ‹è¯•å’Œæ¸…æ´—åçš„æ•°æ®
- `benchmarks/` - æœ€ç»ˆbenchmark

#### 2. test.py - æµ‹è¯•è¿è¡Œå™¨

ä»rawåˆ›å»ºrunå¹¶è¿è¡Œæµ‹è¯•ï¼Œè¯†åˆ«é€šè¿‡/å¤±è´¥çš„æ ·æœ¬ã€‚

```bash
# ä»æœ€æ–°rawåˆ›å»ºrunå¹¶æµ‹è¯•
python -m bench.tools.test --raw latest

# ä»æŒ‡å®šrawåˆ›å»ºrun
python -m bench.tools.test --raw 20251022_184604

# æµ‹è¯•å·²å­˜åœ¨çš„run
python -m bench.tools.test --run 20251022_184604

# åªæµ‹è¯•å‰Nä¸ªæ ·æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰
python -m bench.tools.test --raw latest --limit 10
```

**è¾“å‡º**:
- `runs/{RUN_ID}/tests/passed.jsonl` - é€šè¿‡çš„æ ·æœ¬
- `runs/{RUN_ID}/tests/failed.jsonl` - å¤±è´¥çš„æ ·æœ¬
- `runs/{RUN_ID}/tests/summary.json` - æµ‹è¯•æ‘˜è¦

#### 3. clean.py - æ•°æ®æ¸…æ´—

ä»æµ‹è¯•ç»“æœä¸­ç­›é€‰æ ·æœ¬ï¼Œåº”ç”¨è¿‡æ»¤è§„åˆ™ã€‚

```bash
# æ¸…æ´—æœ€æ–°run
python -m bench.tools.clean --run latest

# æ¸…æ´—æŒ‡å®šrun
python -m bench.tools.clean --run 20251022_184604

# ä¸è¿‡æ»¤unknownå­—æ®µ
python -m bench.tools.clean --run latest --no-filter-unknown

# ä¸è¿‡æ»¤å¤±è´¥æ ·æœ¬
python -m bench.tools.clean --run latest --no-filter-failed
```

**è¿‡æ»¤è§„åˆ™**:
1. è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬ï¼ˆå¦‚æœæœ‰æµ‹è¯•ç»“æœï¼‰
2. è¿‡æ»¤åŒ…å«'unknown'çš„æ ·æœ¬
3. åªä¿ç•™'direct'å’Œ'indirect'æŒ‡ä»¤ç±»å‹
4. åªä¿ç•™'single'å’Œ'workflow'ç»“æ„
5. åªä¿ç•™12ç§æ ¸å¿ƒæ“ä½œ

**è¾“å‡º**:
- `runs/{RUN_ID}/cleaned/cleaned.jsonl` - æ¸…æ´—åçš„æ ·æœ¬
- `runs/{RUN_ID}/cleaned/metadata.json` - å…ƒæ•°æ®
- `runs/{RUN_ID}/cleaned/filter_report.json` - è¿‡æ»¤æŠ¥å‘Š

#### 4. build.py - Benchmarkæ„å»º

ä»æ¸…æ´—åçš„æ•°æ®æ„å»ºæœ€ç»ˆbenchmarkã€‚

```bash
# ä»æœ€æ–°runæ„å»ºbenchmark
python -m bench.tools.build --run latest --version v2

# ä»æŒ‡å®šrunæ„å»º
python -m bench.tools.build --run 20251022_184604 --version v2

# ä¸é‡æ–°åˆ†é…ID
python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
```

**åŠŸèƒ½**:
- é‡æ–°åˆ†é…æ ·æœ¬IDï¼ˆæŒ‰åˆ†ç±»åˆ†ç»„ï¼‰
- ç”Ÿæˆå…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
- æ”¯æŒç‰ˆæœ¬ç®¡ç†

**è¾“å‡º**:
- `benchmarks/{VERSION}/benchmark.jsonl` - æœ€ç»ˆbenchmark
- `benchmarks/{VERSION}/metadata.json` - å…ƒæ•°æ®
- `benchmarks/{VERSION}/stats.json` - ç»Ÿè®¡ä¿¡æ¯

#### 5. pipeline.py - å®Œæ•´æµç¨‹

è‡ªåŠ¨åŒ–æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ã€‚

```bash
# å¤„ç†æœ€æ–°raw
python -m bench.tools.pipeline --raw latest --version v2

# å¤„ç†æŒ‡å®šraw
python -m bench.tools.pipeline --raw 20251022_184604 --version v2

# è·³è¿‡æµ‹è¯•æ­¥éª¤ï¼ˆrunå¿…é¡»å·²å­˜åœ¨ï¼‰
python -m bench.tools.pipeline --raw latest --version v2 --skip-tests

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
python -m bench.tools.pipeline --raw latest --version v2 --verbose
```

**æµç¨‹**:
1. è¿è¡Œæµ‹è¯•ï¼ˆåˆ›å»ºrunï¼‰
2. æ¸…æ´—æ•°æ®
3. æ„å»ºbenchmark

#### 6. stats.py - ç»Ÿè®¡åˆ†æ

åˆ†ææ ·æœ¬åˆ†å¸ƒå’Œè´¨é‡æŒ‡æ ‡ã€‚

```bash
# ç»Ÿè®¡æœ€æ–°run
python -m bench.tools.stats --run latest

# ç»Ÿè®¡æŒ‡å®šrun
python -m bench.tools.stats --run 20251022_184604

# ç»Ÿè®¡æŒ‡å®šæ–‡ä»¶
python -m bench.tools.stats --input stage3.jsonl

# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
python -m bench.tools.stats --run latest --verbose

# ä¿å­˜æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶
python -m bench.tools.stats --run latest --output report.json
```

**ç»Ÿè®¡å†…å®¹**:
- æ ·æœ¬åˆ†å¸ƒï¼ˆè¯­è¨€ã€æ“ä½œã€æŒ‡ä»¤ç±»å‹ã€ç»“æ„ï¼‰
- è´¨é‡æŒ‡æ ‡ï¼ˆå®Œæ•´æ€§ã€æœ‰æ•ˆæ€§ï¼‰
- é—®é¢˜æ£€æµ‹ï¼ˆunknownå­—æ®µã€ç¼ºå¤±å­—æ®µï¼‰
- Topç»„åˆç»Ÿè®¡

### å®ç”¨å·¥å…·

#### 7. clock.py - è™šæ‹Ÿæ—¶é’Ÿ

ç”¨äºåŸºå‡†æµ‹è¯•ä¸­çš„æ—¶é—´æ¨¡æ‹Ÿã€‚

```python
from bench.tools.clock import VirtualClock

clock = VirtualClock()
# ç”¨äºæ¨¡æ‹Ÿæ—¶é—´ç›¸å…³çš„æ“ä½œ
```

#### 8. sql_builder_sqlite.py - SQLæ„å»ºå™¨

ç¼–è¯‘æµ‹è¯•æ–­è¨€ä¸ºSQLæŸ¥è¯¢ã€‚

```python
from bench.tools.sql_builder_sqlite import SQLiteAssertionCompiler

compiler = SQLiteAssertionCompiler()
compiled = compiler.compile(assertion)
```

#### 9. create_empty_db.py - åˆ›å»ºç©ºæ•°æ®åº“

åˆ›å»ºText2Memæ ‡å‡†ç©ºæ•°æ®åº“ã€‚

```bash
# åˆ›å»ºå†…å­˜æ•°æ®åº“ï¼ˆæµ‹è¯•ç”¨ï¼‰
python bench/tools/create_empty_db.py

# åˆ›å»ºæ–‡ä»¶æ•°æ®åº“
python bench/tools/create_empty_db.py --output /path/to/database.db

# éªŒè¯schema
python bench/tools/create_empty_db.py --verify /path/to/database.db
```

## ğŸ“Š æ•°æ®æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Benchmark æ•°æ®æµç¨‹                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Generate (ç”Ÿæˆ)
   â””â”€> bench/data/raw/{TIMESTAMP}/
       â”œâ”€â”€ stage1.jsonl  (NLæŒ‡ä»¤)
       â”œâ”€â”€ stage2.jsonl  (IRæ ·æœ¬)
       â””â”€â”€ stage3.jsonl  (å®Œæ•´æ ·æœ¬)

2. Test (æµ‹è¯•)
   â””â”€> bench/data/runs/{TIMESTAMP}/tests/
       â”œâ”€â”€ passed.jsonl   (é€šè¿‡çš„æ ·æœ¬)
       â”œâ”€â”€ failed.jsonl   (å¤±è´¥çš„æ ·æœ¬)
       â””â”€â”€ summary.json   (æµ‹è¯•æ‘˜è¦)

3. Clean (æ¸…æ´—)
   â””â”€> bench/data/runs/{TIMESTAMP}/cleaned/
       â”œâ”€â”€ cleaned.jsonl       (æ¸…æ´—åçš„æ ·æœ¬)
       â”œâ”€â”€ metadata.json       (å…ƒæ•°æ®)
       â””â”€â”€ filter_report.json  (è¿‡æ»¤æŠ¥å‘Š)

4. Build (æ„å»º)
   â””â”€> bench/data/benchmarks/{VERSION}/
       â”œâ”€â”€ benchmark.jsonl  (æœ€ç»ˆbenchmark)
       â”œâ”€â”€ metadata.json    (å…ƒæ•°æ®)
       â””â”€â”€ stats.json       (ç»Ÿè®¡ä¿¡æ¯)
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è¿‡æ»¤è§„åˆ™

ä¿®æ”¹ `clean.py` ä¸­çš„è¿‡æ»¤è§„åˆ™ï¼š

```python
class DataCleaner:
    ALLOWED_INSTRUCTION_TYPES = {'direct', 'indirect'}
    ALLOWED_STRUCTURES = {'single', 'workflow'}
    ALLOWED_OPERATIONS = {
        'Encode', 'Retrieve', 'Update', 'Delete', 
        'Summarize', 'Label', 'Promote', 'Demote',
        'Expire', 'Lock', 'Merge', 'Split',
    }
```

### æ‰¹é‡å¤„ç†å¤šä¸ªraw

```bash
# å¤„ç†æ‰€æœ‰raw
for raw in bench/data/raw/*/; do
    raw_id=$(basename $raw)
    python -m bench.tools.pipeline --raw $raw_id --version "v_$raw_id"
done
```

### æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„benchmark

```bash
# ç»Ÿè®¡v1
python -m bench.tools.stats --input bench/data/benchmarks/v1/benchmark.jsonl

# ç»Ÿè®¡v2
python -m bench.tools.stats --input bench/data/benchmarks/v2/benchmark.jsonl
```

## ğŸ“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä»å¤´å¼€å§‹ç”Ÿæˆæ–°çš„benchmarkï¼Ÿ

```bash
# 1. ç”ŸæˆåŸå§‹æ•°æ®
python bench/generate/generate.py

# 2. è¿è¡Œå®Œæ•´æµç¨‹
python -m bench.tools.pipeline --raw latest --version v2
```

### Q: å¦‚ä½•åªé‡æ–°æµ‹è¯•è€Œä¸ç”Ÿæˆæ–°æ•°æ®ï¼Ÿ

```bash
# ä»å·²æœ‰rawé‡æ–°æµ‹è¯•
python -m bench.tools.test --raw 20251022_184604
```

### Q: å¦‚ä½•è°ƒè¯•æµ‹è¯•å¤±è´¥ï¼Ÿ

```bash
# 1. åªæµ‹è¯•å‰å‡ ä¸ªæ ·æœ¬
python -m bench.tools.test --raw latest --limit 5 --verbose

# 2. æŸ¥çœ‹å¤±è´¥æ ·æœ¬
cat bench/data/runs/latest/tests/failed.jsonl
```

### Q: å¦‚ä½•è‡ªå®šä¹‰benchmarkç‰ˆæœ¬å·ï¼Ÿ

```bash
# ä½¿ç”¨è‡ªå®šä¹‰ç‰ˆæœ¬å·
python -m bench.tools.pipeline --raw latest --version v2.1-custom
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Benchmark README](../README.md) - æ€»ä½“è¯´æ˜
- [ç”Ÿæˆå·¥å…·æ–‡æ¡£](../generate/QUICK_REFERENCE.md) - æ•°æ®ç”Ÿæˆ
- [å·¥ä½œæµæ–‡æ¡£](../WORKFLOW.md) - å®Œæ•´å·¥ä½œæµç¨‹

## ğŸ—‚ï¸ å½’æ¡£å·¥å…·

å·²å½’æ¡£çš„å·¥å…·ä¿å­˜åœ¨ `_archive/` ç›®å½•ä¸­ï¼ŒåŒ…æ‹¬ï¼š
- `clean_benchmark.py` - æ—§ç‰ˆæ¸…æ´—å·¥å…·
- `migrate_data.py` - æ•°æ®ç»“æ„è¿ç§»è„šæœ¬
- `migrate_to_v3.py` - v3è¿ç§»è„šæœ¬
- `verify_setup.py` - è®¾ç½®éªŒè¯å·¥å…·

è¯¦è§ [_archive/README.md](_archive/README.md)

---

<div align="center">

**Last Updated | æœ€åæ›´æ–°**: 2026-01-07  
**Version | ç‰ˆæœ¬**: v3.0

[â¬† Back to top | è¿”å›é¡¶éƒ¨](#bench-tools--å·¥å…·é›†åˆ)

</div>
