#!/usr/bin/env python3
"""
Benchmarkbuildå·¥å…·

Features:
1. fromcleaneddatabuildbenchmark
2. é‡æ–°åˆ†é…ID
3. generatemetadata
4. æ”¯æŒversionmanage

Usage:
    # fromlatestrunbuildbenchmark
    python -m bench.tools.build --run latest --version v2
    
    # fromspecifiedrunbuild
    python -m bench.tools.build --run 20251015_131147 --version v2
    
    # ä¸é‡æ–°åˆ†é…ID
    python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
"""

import argparse
import json
import logging
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BenchmarkBuilder:
    """Benchmarkbuildå™¨"""
    
    def __init__(
        self,
        run_id: str,
        version: str,
        rebuild_ids: bool = True,
    ):
        """
        Args:
            run_id: Run ID
            version: Benchmarkversionå· (å¦‚ "v1", "v2")
            rebuild_ids: whetheré‡æ–°åˆ†é…ID
        """
        self.run_id = run_id
        self.version = version
        self.rebuild_ids = rebuild_ids
        
        self.run_manager = RunManager()
        self.run_dir = self.run_manager.get_run_dir(run_id)
        self.cleaned_dir = self.run_manager.get_cleaned_dir(run_id)
        self.benchmark_dir = self.run_manager.get_benchmark_dir(version)
        
        self.samples: List[Dict[str, Any]] = []
        
        logger.info(f"ğŸ“‚ Rundirectory: {self.run_dir}")
        logger.info(f"ğŸ“‚ Cleaneddata: {self.cleaned_dir}")
        logger.info(f"ğŸ“‚ Benchmarkè¾“å‡º: {self.benchmark_dir}")
    
    def load_cleaned_data(self):
        """loadæ¸…æ´—åçš„dataï¼ˆifä¸existåˆ™å…ˆexecuteæ¸…æ´—ï¼‰"""
        cleaned_file = self.cleaned_dir / 'cleaned.jsonl'
        
        # ifcleaneddataä¸existï¼Œå°è¯•è‡ªåŠ¨æ¸…æ´—
        if not cleaned_file.exists():
            logger.warning(f"âš ï¸  æ¸…æ´—dataä¸exist: {cleaned_file}")
            logger.info("ğŸ§¹ startè‡ªåŠ¨æ¸…æ´—data...")
            
            # å¯¼å…¥å¹¶executeæ¸…æ´—
            from bench.tools.clean import DataCleaner
            
            cleaner = DataCleaner(run_id=self.run_id)
            cleaner.load_test_results()
            cleaner.load_samples()
            filtered_samples = cleaner.filter_samples()
            cleaner.save_cleaned_data(filtered_samples)
            
            logger.info(f"âœ… è‡ªåŠ¨æ¸…æ´—complete")
            
            # å†æ¬¡check
            if not cleaned_file.exists():
                raise FileNotFoundError(
                    f"æ¸…æ´—ådataä»ä¸exist: {cleaned_file}"
                )
        
        logger.info(f"ğŸ“‚ loadæ¸…æ´—data: {cleaned_file}")
        
        count = 0
        with cleaned_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æfailed: {e}")
        
        logger.info(f"âœ… load {count} ä¸ªsample")
    
    def rebuild_sample_ids(self):
        """é‡æ–°åˆ†é…sampleID"""
        if not self.rebuild_ids:
            logger.info("â­ï¸  skipIDé‡æ–°åˆ†é…")
            return
        
        logger.info("ğŸ”§ é‡æ–°åˆ†é…sampleID...")
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        groups = {}
        for sample in self.samples:
            class_info = sample.get('class', {})
            
            # æå–åˆ†ç±»ä¿¡æ¯
            lang = class_info.get('lang', 'unknown')
            instruction_type = class_info.get('instruction_type', 'unknown')
            structure = class_info.get('structure', 'unknown')
            
            # æå–æ“ä½œtype
            schema_list = sample.get('schema_list', [])
            op = schema_list[0].get('op', 'unknown').lower() if schema_list else 'unknown'
            
            # buildåˆ†ç»„é”®
            group_key = f"{lang}-{instruction_type}-{structure}-{op}"
            
            if group_key not in groups:
                groups[group_key] = []
            
            groups[group_key].append(sample)
        
        logger.info(f"   å‘ç° {len(groups)} ä¸ªåˆ†ç»„")
        
        # ä¸ºeachåˆ†ç»„é‡æ–°number
        new_samples = []
        
        for group_key, samples_list in sorted(groups.items()):
            for idx, sample in enumerate(samples_list, 1):
                # generateæ–° ID
                new_id = f"t2m-{group_key}-{idx:03d}"
                
                # saveåŸå§‹ID
                if 'id' in sample:
                    sample['_original_id'] = sample['id']
                
                # update ID
                sample['id'] = new_id
                
                new_samples.append(sample)
            
            logger.info(f"   {group_key}: {len(samples_list)} ä¸ªsample")
        
        self.samples = new_samples
        logger.info(f"âœ… é‡æ–°åˆ†é…äº† {len(self.samples)} ä¸ªsampleçš„ ID")
    
    def build(self):
        """buildbenchmark"""
        logger.info("ğŸ—ï¸  buildBenchmark...")
        
        # 1. savebenchmarkdata
        benchmark_file = self.benchmark_dir / 'benchmark.jsonl'
        with benchmark_file.open('w', encoding='utf-8') as f:
            for sample in self.samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        logger.info(f"  âœ… Benchmarkdata: {benchmark_file}")
        
        # 2. generatemetadata
        metadata = {
            'version': self.version,
            'created_at': datetime.now().isoformat(),
            'source_run': self.run_id,
            'source_path': str(self.cleaned_dir / 'cleaned.jsonl'),
            'total_samples': len(self.samples),
            'rebuilt_ids': self.rebuild_ids,
        }
        
        metadata_file = self.benchmark_dir / 'metadata.json'
        with metadata_file.open('w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… metadata: {metadata_file}")
        
        # 3. generatestatistics
        stats = self._generate_stats()
        stats_file = self.benchmark_dir / 'stats.json'
        with stats_file.open('w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… statistics: {stats_file}")
        
        # 4. updatelatestlink
        self.run_manager.update_benchmark_latest(self.version)
        logger.info(f"  âœ… updatelatestlink â†’ {self.version}")
        
        logger.info(f"ğŸ—ï¸  Benchmarkbuildcomplete: {self.benchmark_dir}")
    
    def _generate_stats(self) -> Dict[str, Any]:
        """generatestatistics"""
        langs = Counter()
        operations = Counter()
        instruction_types = Counter()
        structures = Counter()
        
        for sample in self.samples:
            class_info = sample.get('class', {})
            
            langs[class_info.get('lang', 'unknown')] += 1
            instruction_types[class_info.get('instruction_type', 'unknown')] += 1
            structures[class_info.get('structure', 'unknown')] += 1
            
            schema_list = sample.get('schema_list', [])
            if schema_list:
                operations[schema_list[0].get('op', 'unknown')] += 1
        
        return {
            'total': len(self.samples),
            'distribution': {
                'languages': dict(langs.most_common()),
                'operations': dict(operations.most_common()),
                'instruction_types': dict(instruction_types.most_common()),
                'structures': dict(structures.most_common()),
            }
        }
    
    def print_summary(self):
        """æ‰“å°buildæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š Benchmarkbuildæ‘˜è¦")
        print("="*80)
        
        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"  Run ID: {self.run_id}")
        print(f"  Benchmarkversion: {self.version}")
        print(f"  sample counté‡: {len(self.samples)}")
        print(f"  é‡æ–°åˆ†é…ID: {'æ˜¯' if self.rebuild_ids else 'å¦'}")
        
        print(f"\nè¾“å‡ºfile:")
        print(f"  data: {self.benchmark_dir}/benchmark.jsonl")
        print(f"  metadata: {self.benchmark_dir}/metadata.json")
        print(f"  ç»Ÿè®¡: {self.benchmark_dir}/stats.json")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkbuildå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
example:
  # fromlatestrunbuildbenchmark
  python -m bench.tools.build --run latest --version v2
  
  # fromspecifiedrunbuild
  python -m bench.tools.build --run 20251015_131147 --version v2
  
  # ä¸é‡æ–°åˆ†é…ID
  python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
        """
    )
    
    parser.add_argument(
        '--run', '-r',
        default='latest',
        help='Run ID (å¦‚ "20251015_131147" or "latest"ï¼Œdefault: latest)'
    )
    parser.add_argument(
        '--version', '-v',
        required=True,
        help='Benchmarkversionå· (å¦‚ "v1", "v2")'
    )
    parser.add_argument(
        '--no-rebuild-ids',
        action='store_true',
        help='ä¸é‡æ–°åˆ†é…ID'
    )
    
    args = parser.parse_args()
    
    # createbuildå™¨
    try:
        builder = BenchmarkBuilder(
            run_id=args.run,
            version=args.version,
            rebuild_ids=not args.no_rebuild_ids,
        )
    except FileNotFoundError as e:
        logger.error(f"âŒ {e}")
        return 1
    
    try:
        # 1. loadæ¸…æ´—data
        builder.load_cleaned_data()
        
        # 2. é‡æ–°åˆ†é…ID
        builder.rebuild_sample_ids()
        
        # 3. buildbenchmark
        builder.build()
        
        # 4. æ‰“å°æ‘˜è¦
        builder.print_summary()
        
        print(f"\nâœ… Benchmarkbuildcompleteï¼")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   # verifybenchmark")
        print(f"   python -m bench run --split benchmark --verbose")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ buildfailed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
