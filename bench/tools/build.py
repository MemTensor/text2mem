#!/usr/bin/env python3
"""
Benchmarkæ„å»ºå·¥å…·

åŠŸèƒ½ï¼š
1. ä»cleanedæ•°æ®æ„å»ºbenchmark
2. é‡æ–°åˆ†é…ID
3. ç”Ÿæˆå…ƒæ•°æ®
4. æ”¯æŒç‰ˆæœ¬ç®¡ç†

ç”¨æ³•ï¼š
    # ä»æœ€æ–°runæ„å»ºbenchmark
    python -m bench.tools.build --run latest --version v2
    
    # ä»æŒ‡å®šrunæ„å»º
    python -m bench.tools.build --run 20251015_131147 --version v2
    
    # ä¸é‡æ–°åˆ†é…ID
    python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
"""

import argparse
import json
import logging
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BenchmarkBuilder:
    """Benchmarkæ„å»ºå™¨"""
    
    def __init__(
        self,
        run_id: str,
        version: str,
        rebuild_ids: bool = True,
    ):
        """
        Args:
            run_id: Run ID
            version: Benchmarkç‰ˆæœ¬å· (å¦‚ "v1", "v2")
            rebuild_ids: æ˜¯å¦é‡æ–°åˆ†é…ID
        """
        self.run_id = run_id
        self.version = version
        self.rebuild_ids = rebuild_ids
        
        self.run_manager = RunManager()
        self.run_dir = self.run_manager.get_run_dir(run_id)
        self.cleaned_dir = self.run_manager.get_cleaned_dir(run_id)
        self.benchmark_dir = self.run_manager.get_benchmark_dir(version)
        
        self.samples: List[Dict[str, Any]] = []
        
        logger.info(f"ğŸ“‚ Runç›®å½•: {self.run_dir}")
        logger.info(f"ğŸ“‚ Cleanedæ•°æ®: {self.cleaned_dir}")
        logger.info(f"ğŸ“‚ Benchmarkè¾“å‡º: {self.benchmark_dir}")
    
    def load_cleaned_data(self):
        """åŠ è½½æ¸…æ´—åçš„æ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™å…ˆæ‰§è¡Œæ¸…æ´—ï¼‰"""
        cleaned_file = self.cleaned_dir / 'cleaned.jsonl'
        
        # å¦‚æœcleanedæ•°æ®ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨æ¸…æ´—
        if not cleaned_file.exists():
            logger.warning(f"âš ï¸  æ¸…æ´—æ•°æ®ä¸å­˜åœ¨: {cleaned_file}")
            logger.info("ğŸ§¹ å¼€å§‹è‡ªåŠ¨æ¸…æ´—æ•°æ®...")
            
            # å¯¼å…¥å¹¶æ‰§è¡Œæ¸…æ´—
            from bench.tools.clean import DataCleaner
            
            cleaner = DataCleaner(run_id=self.run_id)
            cleaner.load_test_results()
            cleaner.load_samples()
            filtered_samples = cleaner.filter_samples()
            cleaner.save_cleaned_data(filtered_samples)
            
            logger.info(f"âœ… è‡ªåŠ¨æ¸…æ´—å®Œæˆ")
            
            # å†æ¬¡æ£€æŸ¥
            if not cleaned_file.exists():
                raise FileNotFoundError(
                    f"æ¸…æ´—åæ•°æ®ä»ä¸å­˜åœ¨: {cleaned_file}"
                )
        
        logger.info(f"ğŸ“‚ åŠ è½½æ¸…æ´—æ•°æ®: {cleaned_file}")
        
        count = 0
        with cleaned_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æå¤±è´¥: {e}")
        
        logger.info(f"âœ… åŠ è½½ {count} ä¸ªæ ·æœ¬")
    
    def rebuild_sample_ids(self):
        """é‡æ–°åˆ†é…æ ·æœ¬ID"""
        if not self.rebuild_ids:
            logger.info("â­ï¸  è·³è¿‡IDé‡æ–°åˆ†é…")
            return
        
        logger.info("ğŸ”§ é‡æ–°åˆ†é…æ ·æœ¬ID...")
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        groups = {}
        for sample in self.samples:
            class_info = sample.get('class', {})
            
            # æå–åˆ†ç±»ä¿¡æ¯
            lang = class_info.get('lang', 'unknown')
            instruction_type = class_info.get('instruction_type', 'unknown')
            structure = class_info.get('structure', 'unknown')
            
            # æå–æ“ä½œç±»å‹
            schema_list = sample.get('schema_list', [])
            op = schema_list[0].get('op', 'unknown').lower() if schema_list else 'unknown'
            
            # æ„å»ºåˆ†ç»„é”®
            group_key = f"{lang}-{instruction_type}-{structure}-{op}"
            
            if group_key not in groups:
                groups[group_key] = []
            
            groups[group_key].append(sample)
        
        logger.info(f"   å‘ç° {len(groups)} ä¸ªåˆ†ç»„")
        
        # ä¸ºæ¯ä¸ªåˆ†ç»„é‡æ–°ç¼–å·
        new_samples = []
        
        for group_key, samples_list in sorted(groups.items()):
            for idx, sample in enumerate(samples_list, 1):
                # ç”Ÿæˆæ–° ID
                new_id = f"t2m-{group_key}-{idx:03d}"
                
                # ä¿å­˜åŸå§‹ID
                if 'id' in sample:
                    sample['_original_id'] = sample['id']
                
                # æ›´æ–° ID
                sample['id'] = new_id
                
                new_samples.append(sample)
            
            logger.info(f"   {group_key}: {len(samples_list)} ä¸ªæ ·æœ¬")
        
        self.samples = new_samples
        logger.info(f"âœ… é‡æ–°åˆ†é…äº† {len(self.samples)} ä¸ªæ ·æœ¬çš„ ID")
    
    def build(self):
        """æ„å»ºbenchmark"""
        logger.info("ğŸ—ï¸  æ„å»ºBenchmark...")
        
        # 1. ä¿å­˜benchmarkæ•°æ®
        benchmark_file = self.benchmark_dir / 'benchmark.jsonl'
        with benchmark_file.open('w', encoding='utf-8') as f:
            for sample in self.samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        logger.info(f"  âœ… Benchmarkæ•°æ®: {benchmark_file}")
        
        # 2. ç”Ÿæˆå…ƒæ•°æ®
        metadata = {
            'version': self.version,
            'created_at': datetime.now().isoformat(),
            'source_run': self.run_id,
            'source_path': str(self.cleaned_dir / 'cleaned.jsonl'),
            'total_samples': len(self.samples),
            'rebuilt_ids': self.rebuild_ids,
        }
        
        metadata_file = self.benchmark_dir / 'metadata.json'
        with metadata_file.open('w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… å…ƒæ•°æ®: {metadata_file}")
        
        # 3. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self._generate_stats()
        stats_file = self.benchmark_dir / 'stats.json'
        with stats_file.open('w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        
        # 4. æ›´æ–°latesté“¾æ¥
        self.run_manager.update_benchmark_latest(self.version)
        logger.info(f"  âœ… æ›´æ–°latesté“¾æ¥ â†’ {self.version}")
        
        logger.info(f"ğŸ—ï¸  Benchmarkæ„å»ºå®Œæˆ: {self.benchmark_dir}")
    
    def _generate_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        langs = Counter()
        operations = Counter()
        instruction_types = Counter()
        structures = Counter()
        
        for sample in self.samples:
            class_info = sample.get('class', {})
            
            langs[class_info.get('lang', 'unknown')] += 1
            instruction_types[class_info.get('instruction_type', 'unknown')] += 1
            structures[class_info.get('structure', 'unknown')] += 1
            
            schema_list = sample.get('schema_list', [])
            if schema_list:
                operations[schema_list[0].get('op', 'unknown')] += 1
        
        return {
            'total': len(self.samples),
            'distribution': {
                'languages': dict(langs.most_common()),
                'operations': dict(operations.most_common()),
                'instruction_types': dict(instruction_types.most_common()),
                'structures': dict(structures.most_common()),
            }
        }
    
    def print_summary(self):
        """æ‰“å°æ„å»ºæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š Benchmarkæ„å»ºæ‘˜è¦")
        print("="*80)
        
        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"  Run ID: {self.run_id}")
        print(f"  Benchmarkç‰ˆæœ¬: {self.version}")
        print(f"  æ ·æœ¬æ•°é‡: {len(self.samples)}")
        print(f"  é‡æ–°åˆ†é…ID: {'æ˜¯' if self.rebuild_ids else 'å¦'}")
        
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        print(f"  æ•°æ®: {self.benchmark_dir}/benchmark.jsonl")
        print(f"  å…ƒæ•°æ®: {self.benchmark_dir}/metadata.json")
        print(f"  ç»Ÿè®¡: {self.benchmark_dir}/stats.json")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Benchmarkæ„å»ºå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»æœ€æ–°runæ„å»ºbenchmark
  python -m bench.tools.build --run latest --version v2
  
  # ä»æŒ‡å®šrunæ„å»º
  python -m bench.tools.build --run 20251015_131147 --version v2
  
  # ä¸é‡æ–°åˆ†é…ID
  python -m bench.tools.build --run latest --version v2 --no-rebuild-ids
        """
    )
    
    parser.add_argument(
        '--run', '-r',
        default='latest',
        help='Run ID (å¦‚ "20251015_131147" æˆ– "latest"ï¼Œé»˜è®¤: latest)'
    )
    parser.add_argument(
        '--version', '-v',
        required=True,
        help='Benchmarkç‰ˆæœ¬å· (å¦‚ "v1", "v2")'
    )
    parser.add_argument(
        '--no-rebuild-ids',
        action='store_true',
        help='ä¸é‡æ–°åˆ†é…ID'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ„å»ºå™¨
    try:
        builder = BenchmarkBuilder(
            run_id=args.run,
            version=args.version,
            rebuild_ids=not args.no_rebuild_ids,
        )
    except FileNotFoundError as e:
        logger.error(f"âŒ {e}")
        return 1
    
    try:
        # 1. åŠ è½½æ¸…æ´—æ•°æ®
        builder.load_cleaned_data()
        
        # 2. é‡æ–°åˆ†é…ID
        builder.rebuild_sample_ids()
        
        # 3. æ„å»ºbenchmark
        builder.build()
        
        # 4. æ‰“å°æ‘˜è¦
        builder.print_summary()
        
        print(f"\nâœ… Benchmarkæ„å»ºå®Œæˆï¼")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   # éªŒè¯benchmark")
        print(f"   python -m bench run --split benchmark --verbose")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
