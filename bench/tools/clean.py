#!/usr/bin/env python3
"""
æ•°æ®æ¸…æ´—å·¥å…·

åŠŸèƒ½ï¼š
1. ä»stage3å’Œæµ‹è¯•ç»“æœä¸­ç­›é€‰æ ·æœ¬
2. åº”ç”¨è¿‡æ»¤è§„åˆ™
3. ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®

ç”¨æ³•ï¼š
    # æ¸…æ´—æœ€æ–°run
    python -m bench.tools.clean --run latest
    
    # æ¸…æ´—æŒ‡å®šrun
    python -m bench.tools.clean --run 20251015_131147
    
    # ä¸è¿‡æ»¤unknown
    python -m bench.tools.clean --run latest --no-filter-unknown
"""

import argparse
import json
import logging
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from bench.tools.run_manager import RunManager

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    # é»˜è®¤è¿‡æ»¤è§„åˆ™
    ALLOWED_INSTRUCTION_TYPES = {'direct', 'indirect'}
    ALLOWED_STRUCTURES = {'single', 'workflow'}
    ALLOWED_OPERATIONS = {
        'Encode', 'Retrieve', 'Update', 'Delete', 'Summarize', 'Label',
        'Promote', 'Demote', 'Expire', 'Lock', 'Merge', 'Split',
    }
    
    def __init__(
        self,
        run_id: str,
        filter_unknown: bool = True,
        filter_failed: bool = True,
    ):
        """
        Args:
            run_id: Run ID
            filter_unknown: æ˜¯å¦è¿‡æ»¤åŒ…å«unknownçš„æ ·æœ¬
            filter_failed: æ˜¯å¦è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬
        """
        self.run_id = run_id
        self.filter_unknown = filter_unknown
        self.filter_failed = filter_failed
        
        self.run_manager = RunManager()
        self.run_dir = self.run_manager.get_run_dir(run_id)
        self.cleaned_dir = self.run_manager.get_cleaned_dir(run_id)
        
        self.samples: List[Dict[str, Any]] = []
        self.passed_sample_ids: Set[str] = set()
        
        self.stats = {
            'total_loaded': 0,
            'total_passed_tests': 0,
            'total_filtered': 0,
            'total_final': 0,
            'filter_reasons': {
                'failed_test': 0,
                'unknown_fields': 0,
                'invalid_instruction_type': 0,
                'invalid_structure': 0,
                'invalid_operation': 0,
            }
        }
        
        logger.info(f"ğŸ“‚ Runç›®å½•: {self.run_dir}")
        logger.info(f"ğŸ“‚ æ¸…æ´—è¾“å‡º: {self.cleaned_dir}")
    
    def load_test_results(self):
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        if not self.filter_failed:
            logger.info("âš ï¸  ä¸è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬")
            return
        
        has_tests = self.run_manager.has_tests(self.run_id)
        if not has_tests:
            logger.warning("âš ï¸  æ²¡æœ‰æµ‹è¯•ç»“æœï¼Œå°†ä¸è¿‡æ»¤å¤±è´¥æ ·æœ¬")
            self.filter_failed = False
            return
        
        tests_dir = self.run_manager.get_tests_dir(self.run_id)
        
        # ä¼˜å…ˆä½¿ç”¨passed.jsonlï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨details.jsonl
        passed_file = tests_dir / 'passed.jsonl'
        details_file = tests_dir / 'details.jsonl'
        
        passed_count = 0
        if passed_file.exists():
            logger.info(f"ğŸ“‚ åŠ è½½æµ‹è¯•ç»“æœ: {passed_file}")
            with passed_file.open('r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    result = json.loads(line)
                    self.passed_sample_ids.add(result['sample_id'])
                    passed_count += 1
        elif details_file.exists():
            logger.info(f"ğŸ“‚ ä»è¯¦ç»†ç»“æœä¸­åŠ è½½é€šè¿‡çš„æ ·æœ¬: {details_file}")
            with details_file.open('r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    result = json.loads(line)
                    # åªæ·»åŠ passed=Trueçš„æ ·æœ¬
                    if result.get('passed', False):
                        self.passed_sample_ids.add(result['sample_id'])
                        passed_count += 1
        else:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
            self.filter_failed = False
            return
        
        # å¦‚æœæœ‰é‡å¤IDï¼Œè¾“å‡ºè­¦å‘Š
        unique_ids = len(self.passed_sample_ids)
        if passed_count > unique_ids:
            logger.warning(f"âš ï¸  å‘ç° {passed_count - unique_ids} ä¸ªé‡å¤çš„sample_idåœ¨æµ‹è¯•ç»“æœä¸­")
            logger.warning(f"   è¿™å¯èƒ½æ˜¯å› ä¸ºç”Ÿæˆçš„æ•°æ®æœ‰é‡å¤IDï¼Œå»ºè®®æ£€æŸ¥ç”Ÿæˆé€»è¾‘")
        
        logger.info(f"âœ… åŠ è½½ {unique_ids} ä¸ªå”¯ä¸€çš„é€šè¿‡æµ‹è¯•çš„æ ·æœ¬ID (æ€»è®¡ {passed_count} æ¡é€šè¿‡è®°å½•)")
        self.stats['total_passed_tests'] = unique_ids
        self.stats['total_passed_records'] = passed_count
    
    def load_samples(self):
        """åŠ è½½åŸå§‹æ ·æœ¬æ•°æ®"""
        # è·å–æ¥æºraw
        raw_id = self.run_manager.get_source_raw(self.run_id)
        if not raw_id:
            raise FileNotFoundError(f"æ— æ³•ç¡®å®šrun {self.run_id} çš„æ¥æºraw")
        
        stage3_file = self.run_manager.get_stage_file_from_raw(raw_id, 3)
        
        if not stage3_file.exists():
            raise FileNotFoundError(f"Stage3æ–‡ä»¶ä¸å­˜åœ¨: {stage3_file}")
        
        logger.info(f"ğŸ“‚ åŠ è½½æ ·æœ¬æ•°æ®: {stage3_file}")
        
        count = 0
        with stage3_file.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    self.samples.append(sample)
                    count += 1
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸  è¡Œ {line_num} è§£æå¤±è´¥: {e}")
        
        logger.info(f"âœ… åŠ è½½ {count} ä¸ªæ ·æœ¬")
        self.stats['total_loaded'] = count
    
    def filter_samples(self) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æ ·æœ¬"""
        logger.info("ğŸ§¹ å¼€å§‹è¿‡æ»¤æ ·æœ¬...")
        logger.info(f"   filter_failed={self.filter_failed}, passed_ids count={len(self.passed_sample_ids)}")
        
        filtered_samples = []
        
        for sample in self.samples:
            sample_id = sample.get('id', '')
            class_info = sample.get('class', {})
            
            # è§„åˆ™1: è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬
            if self.filter_failed and sample_id not in self.passed_sample_ids:
                self.stats['filter_reasons']['failed_test'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            # æå–å­—æ®µ
            lang = class_info.get('lang', 'unknown')
            instruction_type = class_info.get('instruction_type', 'unknown')
            structure = class_info.get('structure', 'unknown')
            
            # æå–æ“ä½œ
            schema_list = sample.get('schema_list', [])
            if not schema_list:
                self.stats['filter_reasons']['invalid_operation'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            operation = schema_list[0].get('op', 'unknown')
            
            # è§„åˆ™2: è¿‡æ»¤åŒ…å«unknownçš„æ ·æœ¬
            if self.filter_unknown and 'unknown' in [lang, instruction_type, structure, operation]:
                self.stats['filter_reasons']['unknown_fields'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            # è§„åˆ™3: æ£€æŸ¥æŒ‡ä»¤ç±»å‹
            if instruction_type not in self.ALLOWED_INSTRUCTION_TYPES:
                self.stats['filter_reasons']['invalid_instruction_type'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            # è§„åˆ™4: æ£€æŸ¥ç»“æ„
            if structure not in self.ALLOWED_STRUCTURES:
                self.stats['filter_reasons']['invalid_structure'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            # è§„åˆ™5: æ£€æŸ¥æ“ä½œ
            if operation not in self.ALLOWED_OPERATIONS:
                self.stats['filter_reasons']['invalid_operation'] += 1
                self.stats['total_filtered'] += 1
                continue
            
            # é€šè¿‡æ‰€æœ‰è¿‡æ»¤
            filtered_samples.append(sample)
        
        self.stats['total_final'] = len(filtered_samples)
        logger.info(f"âœ… è¿‡æ»¤å®Œæˆ: {len(filtered_samples)} ä¸ªæ ·æœ¬ä¿ç•™")
        logger.info(f"   è¿‡æ»¤æ‰: {self.stats['total_filtered']} ä¸ªæ ·æœ¬")
        
        return filtered_samples
    
    def save_cleaned_data(self, samples: List[Dict[str, Any]]):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        logger.info("ğŸ’¾ ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")
        
        # 1. ä¿å­˜æ¸…æ´—åçš„æ ·æœ¬
        cleaned_file = self.cleaned_dir / 'cleaned.jsonl'
        with cleaned_file.open('w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        logger.info(f"  âœ… æ¸…æ´—æ•°æ®: {cleaned_file}")
        
        # 2. ç”Ÿæˆå…ƒæ•°æ®
        # è·å–æ¥æºraw
        raw_id = self.run_manager.get_source_raw(self.run_id)
        
        metadata = {
            'run_id': self.run_id,
            'created_at': datetime.now().isoformat(),
            'source_raw': raw_id,
            'source_stage3': str(self.run_manager.get_stage_file_from_raw(raw_id, 3)) if raw_id else None,
            'total_samples': len(samples),
            'filtering': {
                'filter_unknown': self.filter_unknown,
                'filter_failed': self.filter_failed,
            },
            'stats': self.stats,
        }
        
        metadata_file = self.cleaned_dir / 'metadata.json'
        with metadata_file.open('w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… å…ƒæ•°æ®: {metadata_file}")
        
        # 3. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self._generate_stats(samples)
        stats_file = self.cleaned_dir / 'stats.json'
        with stats_file.open('w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        
        # 4. ç”Ÿæˆè¿‡æ»¤æŠ¥å‘Š
        filter_report = {
            'created_at': datetime.now().isoformat(),
            'total_loaded': self.stats['total_loaded'],
            'total_passed_tests': self.stats['total_passed_tests'],
            'total_passed_records': self.stats.get('total_passed_records', self.stats['total_passed_tests']),
            'total_filtered': self.stats['total_filtered'],
            'total_final': self.stats['total_final'],
            'retention_rate': self.stats['total_final'] / self.stats['total_loaded'] * 100 if self.stats['total_loaded'] > 0 else 0,
            'filter_reasons': self.stats['filter_reasons'],
        }
        
        report_file = self.cleaned_dir / 'filter_report.json'
        with report_file.open('w', encoding='utf-8') as f:
            json.dump(filter_report, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… è¿‡æ»¤æŠ¥å‘Š: {report_file}")
        
        logger.info(f"ğŸ’¾ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {self.cleaned_dir}")
    
    def _generate_stats(self, samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        langs = Counter()
        operations = Counter()
        instruction_types = Counter()
        structures = Counter()
        
        for sample in samples:
            class_info = sample.get('class', {})
            
            langs[class_info.get('lang', 'unknown')] += 1
            instruction_types[class_info.get('instruction_type', 'unknown')] += 1
            structures[class_info.get('structure', 'unknown')] += 1
            
            schema_list = sample.get('schema_list', [])
            if schema_list:
                operations[schema_list[0].get('op', 'unknown')] += 1
        
        return {
            'total': len(samples),
            'distribution': {
                'languages': dict(langs.most_common()),
                'operations': dict(operations.most_common()),
                'instruction_types': dict(instruction_types.most_common()),
                'structures': dict(structures.most_common()),
            }
        }
    
    def print_summary(self):
        """æ‰“å°æ¸…æ´—æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š æ¸…æ´—æ‘˜è¦")
        print("="*80)
        
        print(f"\nå¤„ç†ç»Ÿè®¡:")
        print(f"  åŠ è½½æ ·æœ¬æ•°: {self.stats['total_loaded']}")
        if self.stats['total_passed_tests'] > 0:
            print(f"  é€šè¿‡æµ‹è¯•: {self.stats['total_passed_tests']}")
        print(f"  è¿‡æ»¤æ ·æœ¬æ•°: {self.stats['total_filtered']}")
        print(f"  æœ€ç»ˆæ ·æœ¬æ•°: {self.stats['total_final']}")
        print(f"  ä¿ç•™æ¯”ä¾‹: {self.stats['total_final']/self.stats['total_loaded']*100:.1f}%")
        
        if self.stats['total_filtered'] > 0:
            print(f"\nè¿‡æ»¤åŸå› :")
            for reason, count in self.stats['filter_reasons'].items():
                if count > 0:
                    print(f"  {reason}: {count} ä¸ª")
        
        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ•°æ®æ¸…æ´—å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¿‡æ»¤è§„åˆ™:
  1. è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬ï¼ˆå¦‚æœæœ‰æµ‹è¯•ç»“æœï¼‰
  2. è¿‡æ»¤åŒ…å«'unknown'çš„æ ·æœ¬
  3. åªä¿ç•™'direct'å’Œ'indirect'æŒ‡ä»¤ç±»å‹
  4. åªä¿ç•™'single'å’Œ'workflow'ç»“æ„
  5. åªä¿ç•™12ç§æ ¸å¿ƒæ“ä½œ

ç¤ºä¾‹:
  # æ¸…æ´—æœ€æ–°run
  python -m bench.tools.clean --run latest
  
  # æ¸…æ´—æŒ‡å®šrun
  python -m bench.tools.clean --run 20251015_131147
  
  # ä¸è¿‡æ»¤unknownå­—æ®µ
  python -m bench.tools.clean --run latest --no-filter-unknown
  
  # ä¸è¿‡æ»¤å¤±è´¥æ ·æœ¬
  python -m bench.tools.clean --run latest --no-filter-failed
        """
    )
    
    parser.add_argument(
        '--run', '-r',
        default='latest',
        help='Run ID (å¦‚ "20251015_131147" æˆ– "latest"ï¼Œé»˜è®¤: latest)'
    )
    parser.add_argument(
        '--no-filter-unknown',
        action='store_true',
        help='ä¸è¿‡æ»¤åŒ…å«unknownçš„æ ·æœ¬'
    )
    parser.add_argument(
        '--no-filter-failed',
        action='store_true',
        help='ä¸è¿‡æ»¤æµ‹è¯•å¤±è´¥çš„æ ·æœ¬'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¸…æ´—å™¨
    try:
        cleaner = DataCleaner(
            run_id=args.run,
            filter_unknown=not args.no_filter_unknown,
            filter_failed=not args.no_filter_failed,
        )
    except FileNotFoundError as e:
        logger.error(f"âŒ {e}")
        return 1
    
    try:
        # 1. åŠ è½½æµ‹è¯•ç»“æœ
        cleaner.load_test_results()
        
        # 2. åŠ è½½æ ·æœ¬
        cleaner.load_samples()
        
        # 3. è¿‡æ»¤æ ·æœ¬
        filtered_samples = cleaner.filter_samples()
        
        if not filtered_samples:
            logger.error("âŒ æ²¡æœ‰æ ·æœ¬é€šè¿‡è¿‡æ»¤")
            return 1
        
        # 4. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        cleaner.save_cleaned_data(filtered_samples)
        
        # 5. æ‰“å°æ‘˜è¦
        cleaner.print_summary()
        
        print(f"\nâœ… æ¸…æ´—å®Œæˆï¼")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ¸…æ´—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
