# =============================================================================
# Text2Mem Environment Configuration Template
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# Then edit .env with your preferred settings.
# =============================================================================

# =============================================================================
# 1. Database Configuration / 数据库配置
# =============================================================================
TEXT2MEM_DB_PATH=./text2mem.db
TEXT2MEM_DB_WAL=true
TEXT2MEM_DB_TIMEOUT=30

# =============================================================================
# 2. Model Provider / 模型提供者
# =============================================================================
# Options: mock | ollama | openai
# - mock: For testing, no real LLM required / 测试用，无需真实 LLM
# - ollama: Local models via Ollama / 本地 Ollama 模型
# - openai: OpenAI API / OpenAI API
TEXT2MEM_PROVIDER=mock

# =============================================================================
# 3. Embedding Model / 嵌入模型
# =============================================================================
# For Ollama: nomic-embed-text, mxbai-embed-large
# For OpenAI: text-embedding-3-small, text-embedding-3-large
TEXT2MEM_EMBEDDING_MODEL=nomic-embed-text

# =============================================================================
# 4. Generation Model / 生成模型
# =============================================================================
# For Ollama: qwen2:0.5b, llama3, mistral
# For OpenAI: gpt-4o-mini, gpt-4, gpt-3.5-turbo
TEXT2MEM_GENERATION_MODEL=qwen2:0.5b
TEXT2MEM_TEMPERATURE=0.7
TEXT2MEM_MAX_TOKENS=512

# =============================================================================
# 5. OpenAI Configuration (if using openai provider)
# =============================================================================
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1

# =============================================================================
# 6. Ollama Configuration (if using ollama provider)
# =============================================================================
OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# Request Configuration
# =============================================================================
# Request timeout in seconds
TEXT2MEM_REQUEST_TIMEOUT=60

# Maximum retry attempts for failed requests
TEXT2MEM_MAX_RETRIES=3

# Batch size for batch operations
TEXT2MEM_BATCH_SIZE=10

# =============================================================================
# Search / Retrieval Configuration
# =============================================================================
# Hybrid search weights (semantic + keyword matching)
# Alpha: semantic similarity weight (0.0-1.0)
TEXT2MEM_SEARCH_ALPHA=0.7

# Beta: keyword matching weight (0.0-1.0)
# Note: alpha + beta should typically sum to 1.0 or less
TEXT2MEM_SEARCH_BETA=0.3

# Phrase bonus: additional score for exact phrase matches (0.0-1.0)
TEXT2MEM_SEARCH_PHRASE_BONUS=0.2

# Default search result limit
TEXT2MEM_SEARCH_DEFAULT_LIMIT=10

# Maximum search result limit (safety boundary)
TEXT2MEM_SEARCH_MAX_LIMIT=100

# Default top-k for semantic search
TEXT2MEM_SEARCH_DEFAULT_K=5

# =============================================================================
# Bench Testing Configuration
# =============================================================================
# Timeout for each test sample in seconds (0 or empty = no timeout)
TEXT2MEM_BENCH_TIMEOUT=

# Default test split to use
TEXT2MEM_BENCH_SPLIT=basic

# Default bench mode (auto | mock | ollama | openai)
TEXT2MEM_BENCH_MODE=auto

# Verbose output for bench tests
TEXT2MEM_BENCH_VERBOSE=false

# =============================================================================
# Bench Generation Configuration
# =============================================================================
# LLM provider for test generation (openai | ollama | anthropic)
TEXT2MEM_BENCH_GEN_PROVIDER=openai

# LLM model for test generation
TEXT2MEM_BENCH_GEN_MODEL=gpt-4o-mini

# Temperature for test generation
TEXT2MEM_BENCH_GEN_TEMPERATURE=0.7

# Max tokens for test generation
TEXT2MEM_BENCH_GEN_MAX_TOKENS=4000

# Timeout for LLM generation requests in seconds
TEXT2MEM_BENCH_GEN_TIMEOUT=120

# Async Parallel Generation Configuration
# =============================================================================
# Enable async parallel generation (true | false)
# Async mode provides 5-10x speed improvement
TEXT2MEM_BENCH_GEN_USE_ASYNC=true

# Maximum concurrent requests (recommended: 5-10)
# Higher values = faster but may hit API rate limits
TEXT2MEM_BENCH_GEN_MAX_CONCURRENT=5

# Maximum retry attempts for failed requests
TEXT2MEM_BENCH_GEN_RETRY_MAX=3

# Retry delay in seconds (exponential backoff)
TEXT2MEM_BENCH_GEN_RETRY_DELAY=2

# Checkpoint batch size (update checkpoint every N samples)
# Smaller value = more frequent updates but slower
# Larger value = less frequent updates but faster
# Recommended: 10
TEXT2MEM_BENCH_GEN_CHECKPOINT_BATCH=10

# =============================================================================
# Logging Configuration
# =============================================================================
# Log level (DEBUG | INFO | WARNING | ERROR | CRITICAL)
TEXT2MEM_LOG_LEVEL=INFO

# Default language for operations (en | zh)
TEXT2MEM_LANG=en

# =============================================================================
# Advanced Configuration (JSON format)
# =============================================================================
# Complex model configuration (optional, overrides individual settings)
# Format: {"provider":{"embedding":"model","generation":"model","base_url":"url"}}
TEXT2MEM_MODELS=
