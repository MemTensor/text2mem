#!/usr/bin/env python3
"""
Complete Bench CLI - Full Benchmark System

Features:
1. run          - Run benchmark tests
2. generate     - Generate new benchmark candidate data
3. validate     - Validate the quality of generated data
4. promote      - Promote candidate to official benchmark
5. list-results - List test results
6. show-result  - Show result details
7. compare      - Compare two results
8. info         - Show benchmark info
"""

import argparse
import json
import logging
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from bench.core.simple_manager import Benchmark, ResultsManager
from bench.core.simple_runner import SimpleTestRunner

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def cmd_run(args):
    """Run benchmark tests"""
    schema_filter = None
    if args.schema_filter:
        schema_filter = [s.strip() for s in args.schema_filter.split(',')]
    
    schema_indices = None
    if args.schema_indices:
        schema_indices = [int(i.strip()) for i in args.schema_indices.split(',')]
    
    runner = SimpleTestRunner(
        mode=args.mode,
        filter_expr=args.filter,
        schema_filter=schema_filter,
        schema_indices=schema_indices,
        timeout=args.timeout,
    )
    
    try:
        result = runner.run(
            result_id=args.output_id,
            verbose=args.verbose
        )
        return 0
    except Exception as e:
        logger.error(f"âŒ Test failed / æµ‹è¯•å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


def cmd_generate(args):
    """Generate new benchmark candidate data"""
    print("=" * 80)
    print("ğŸ”„ Generate Benchmark Candidate / ç”Ÿæˆ Benchmark å€™é€‰æ•°æ®")
    print("=" * 80)
    print()
    
    import subprocess
    from datetime import datetime
    
    gen_id = args.output_id or datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Prepare output directory
    if args.use_generation_dir:
        output_dir = Path(f'bench/data/generation/{gen_id}')
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“‚ Output Directory / è¾“å‡ºç›®å½•: {output_dir}")
    else:
        output_dir = None
        print(f"ğŸ“‚ Output to default location (bench/data/raw/) / è¾“å‡ºåˆ°é»˜è®¤ä½ç½® (bench/data/raw/)")
    
    print()
    
    # Run the generation script
    cmd = [sys.executable, 'bench/generate/generate.py']
    
    if args.config:
        cmd.extend(['--config', args.config])
    
    print("ğŸƒ Running generation script / è¿è¡Œç”Ÿæˆè„šæœ¬...")
    print(f"   Command / å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    result = subprocess.run(cmd, cwd=Path.cwd())
    
    if result.returncode == 0:
        print()
        print("âœ… Generation complete! / ç”Ÿæˆå®Œæˆï¼")
        print()
        
        # Find generated data
        raw_dir = Path('bench/data/raw')
        if raw_dir.exists():
            gen_dirs = sorted([d for d in raw_dir.iterdir() if d.is_dir()], 
                              key=lambda x: x.name, reverse=True)
            if gen_dirs:
                latest_gen = gen_dirs[0].name
                print("Next steps / ä¸‹ä¸€æ­¥:")
                print(f"  1. Validate quality / éªŒè¯è´¨é‡: ./bench-cli validate {latest_gen}")
                print(f"  2. Promote to benchmark / æå‡ä¸º benchmark: ./bench-cli promote {latest_gen}")
    else:
        print()
        print("âŒ Generation failed / ç”Ÿæˆå¤±è´¥")
    
    return result.returncode


def cmd_validate(args):
    """Validate generated data quality"""
    gen_id = args.generation_id
    
    # Locate generated data
    possible_paths = [
        Path(f'bench/data/generation/{gen_id}'),
        Path(f'bench/data/raw/{gen_id}'),
    ]
    
    gen_dir = None
    for p in possible_paths:
        if p.exists():
            gen_dir = p
            break
    
    if not gen_dir:
        logger.error(f"Generation not found / æœªæ‰¾åˆ°ç”Ÿæˆæ•°æ®: {gen_id}")
        logger.error(f"Searched in / æœç´¢è·¯å¾„: {', '.join(str(p) for p in possible_paths)}")
        return 1
    
    print("=" * 80)
    print(f"ğŸ” Validate Data / éªŒè¯æ•°æ®: {gen_id}")
    print("=" * 80)
    print(f"ğŸ“‚ Location / ä½ç½®: {gen_dir}")
    print()
    
    # Find stage3.jsonl
    stage3_file = gen_dir / 'stage3.jsonl'
    if not stage3_file.exists():
        logger.error(f"stage3.jsonl not found in {gen_dir}")
        return 1
    
    # Count samples
    samples = []
    with open(stage3_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except:
                    pass
    
    print(f"ğŸ“Š Total samples / æ€»æ ·æœ¬æ•°: {len(samples)}")
    print()
    
    # Quick statistics
    from collections import Counter
    lang_counter = Counter()
    op_counter = Counter()
    
    for sample in samples:
        class_info = sample.get('class', {})
        lang_counter[class_info.get('lang', 'unknown')] += 1
        
        for schema in sample.get('schema_list', []):
            op_counter[schema.get('op', 'unknown')] += 1
    
    print("ğŸ“ˆ Data distribution / æ•°æ®åˆ†å¸ƒ:")
    print(f"   Languages / è¯­è¨€: {dict(lang_counter)}")
    print(f"   Operations (Top 5) / æ“ä½œå‰5: {dict(op_counter.most_common(5))}")
    print()
    
    # Run test validation
    if args.run_tests:
        print("ğŸ§ª Running test validation / è¿è¡Œæµ‹è¯•éªŒè¯...")
        print()
        
        import tempfile
        import shutil
        
        with tempfile.TemporaryDirectory() as tmpdir:
            # Create temporary benchmark
            tmp_benchmark_dir = Path(tmpdir) / 'benchmark'
            tmp_benchmark_dir.mkdir()
            
            shutil.copy(stage3_file, tmp_benchmark_dir / 'benchmark.jsonl')
            
            metadata = {
                'total_samples': len(samples),
                'created_at': 'validation',
            }
            with open(tmp_benchmark_dir / 'metadata.json', 'w') as f:
                json.dump(metadata, f)
            
            # Temporary run
            from bench.core.simple_manager import Benchmark
            runner = SimpleTestRunner(mode='mock')
            
            original_dir = runner.benchmark.benchmark_dir
            runner.benchmark.benchmark_dir = tmp_benchmark_dir
            runner.benchmark.benchmark_file = tmp_benchmark_dir / 'benchmark.jsonl'
            runner.benchmark.metadata_file = tmp_benchmark_dir / 'metadata.json'
            
            try:
                result = runner.run(verbose=args.verbose)
                
                pass_rate = result.report['summary']['pass_rate']
                print()
                print(f"ğŸ“Š Pass rate / æµ‹è¯•é€šè¿‡ç‡: {pass_rate*100:.1f}%")
                print()
                
                if pass_rate >= 0.5:
                    print("ğŸ’¡ Quality evaluation / è´¨é‡è¯„ä¼°: Good â€” ready for promotion / è‰¯å¥½ï¼Œå¯ä»¥æå‡ä¸ºæ­£å¼ benchmark")
                    print()
                    print("Next step / ä¸‹ä¸€æ­¥:")
                    print(f"  ./bench-cli promote {gen_id}")
                else:
                    print("âš ï¸  Quality evaluation / è´¨é‡è¯„ä¼°: Low pass rate â€” consider regenerating / é€šè¿‡ç‡åä½ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")
                
            finally:
                runner.benchmark.benchmark_dir = original_dir
    else:
        print("ğŸ’¡ Tip / æç¤º: Add --run-tests to perform validation / æ·»åŠ  --run-tests å‚æ•°å¯è¿è¡ŒéªŒè¯")
        print()
        print("Next steps / ä¸‹ä¸€æ­¥:")
        print(f"  ./bench-cli validate {gen_id} --run-tests  # Run tests / è¿è¡Œæµ‹è¯•")
        print(f"  ./bench-cli promote {gen_id}               # Promote / æå‡ä¸º benchmark")
    
    return 0
def cmd_promote(args):
    """Promote generated data to official benchmark"""
    gen_id = args.generation_id
    
    # Locate generated data
    possible_paths = [
        Path(f'bench/data/generation/{gen_id}'),
        Path(f'bench/data/raw/{gen_id}'),
    ]
    
    gen_dir = None
    for p in possible_paths:
        if p.exists():
            gen_dir = p
            break
    
    if not gen_dir:
        logger.error(f"Generation not found / æœªæ‰¾åˆ°ç”Ÿæˆæ•°æ®: {gen_id}")
        return 1
    
    stage3_file = gen_dir / 'stage3.jsonl'
    if not stage3_file.exists():
        logger.error(f"stage3.jsonl not found in {gen_dir}")
        return 1
    
    print("=" * 80)
    print(f"â¬†ï¸  Promote to Official Benchmark / æå‡ä¸ºæ­£å¼ Benchmark: {gen_id}")
    print("=" * 80)
    print()
    
    # Confirmation
    if not args.yes:
        print("âš ï¸  This will replace the current benchmark! / è¿™å°†æ›¿æ¢å½“å‰çš„ benchmarkï¼")
        
        try:
            benchmark = Benchmark()
            print(f"   Current benchmark / å½“å‰ benchmark: {benchmark.sample_count} samples")
        except:
            print(f"   Current benchmark / å½“å‰ benchmark: Not found / ä¸å­˜åœ¨")
        
        with open(stage3_file, 'r') as f:
            new_count = sum(1 for _ in f if _.strip())
        print(f"   New benchmark / æ–° benchmark: {new_count} samples")
        print()
        
        confirm = input("Confirm replacement? (yes/no) / ç¡®è®¤æ›¿æ¢? (yes/no): ")
        if confirm.lower() != 'yes':
            print("â Cancelled / å·²å–æ¶ˆ")
            return 0
    
    # Backup current benchmark
    import shutil
    from datetime import datetime
    
    benchmark_dir = Path('bench/data/benchmark')
    backup_dir = Path(f'bench/data/archive/benchmark_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    backup_dir.parent.mkdir(parents=True, exist_ok=True)
    
    if benchmark_dir.exists():
        shutil.copytree(benchmark_dir, backup_dir)
        print(f"âœ“ Backed up current benchmark / å·²å¤‡ä»½å½“å‰ benchmark -> {backup_dir}")
        print()
    
    # Read and filter samples
    print("ğŸ”„ Processing data / å¤„ç†æ•°æ®...")
    
    samples = []
    with open(stage3_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except:
                    pass
    
    print(f"   Original samples / åŸå§‹æ ·æœ¬: {len(samples)}")
    
    # Filter invalid data
    ALLOWED_OPERATIONS = {
        'Encode', 'Retrieve', 'Update', 'Delete', 'Summarize', 'Label',
        'Promote', 'Demote', 'Expire', 'Lock', 'Merge', 'Split',
    }
    
    filtered_samples = []
    for sample in samples:
        schema_list = sample.get('schema_list', [])
        if not schema_list:
            continue
        
        sample_str = json.dumps(sample)
        if 'unknown' in sample_str.lower():
            continue
        
        valid = True
        for schema in schema_list:
            if schema.get('op') not in ALLOWED_OPERATIONS:
                valid = False
                break
        
        if valid:
            filtered_samples.append(sample)
    
    print(f"   Filtered / è¿‡æ»¤å: {len(filtered_samples)}")
    print()
    
    # Save to benchmark directory
    benchmark_dir.mkdir(parents=True, exist_ok=True)
    
    with open(benchmark_dir / 'benchmark.jsonl', 'w', encoding='utf-8') as f:
        for sample in filtered_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    # Generate statistics
    from collections import Counter
    
    lang_counter = Counter()
    op_counter = Counter()
    structure_counter = Counter()
    
    for sample in filtered_samples:
        class_info = sample.get('class', {})
        lang_counter[class_info.get('lang', 'unknown')] += 1
        structure_counter[class_info.get('structure', 'unknown')] += 1
        
        for schema in sample.get('schema_list', []):
            op_counter[schema.get('op', 'unknown')] += 1
    
    stats = {
        'total': len(filtered_samples),
        'distribution': {
            'languages': dict(lang_counter),
            'operations': dict(op_counter),
            'structures': dict(structure_counter),
        }
    }
    
    with open(benchmark_dir / 'stats.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # Save metadata
    metadata = {
        'total_samples': len(filtered_samples),
        'created_at': datetime.now().isoformat(),
        'last_updated': datetime.now().isoformat(),
        'source': f'{gen_dir.parent.name}/{gen_id}',
        'notes': args.notes or f'Promoted from {gen_id}'
    }
    
    with open(benchmark_dir / 'metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print("=" * 80)
    print("âœ… Benchmark updated! / Benchmark å·²æ›´æ–°ï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“Š New Benchmark Info / æ–° Benchmark ä¿¡æ¯:")
    print(f"   Total samples / æ€»æ ·æœ¬æ•°: {len(filtered_samples)}")
    print(f"   Language distribution / è¯­è¨€åˆ†å¸ƒ: {dict(lang_counter)}")
    print(f"   Operation distribution (Top 5) / æ“ä½œåˆ†å¸ƒå‰5: {dict(op_counter.most_common(5))}")
    print()
    print("Next step / ä¸‹ä¸€æ­¥:")
    print("  ./bench-cli run --mode ollama -v  # Run full test / è¿è¡Œå®Œæ•´æµ‹è¯•")
    print()
    
    return 0


def cmd_list_results(args):
    """List benchmark test results"""
    manager = ResultsManager()
    results = manager.list_results(limit=args.limit)
    
    if not results:
        print("â„¹ï¸  No test results found / æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç»“æœ")
        return 0
    
    print()
    print("=" * 100)
    print(f"{'ID':<20} {'Mode':<10} {'Pass Rate':<12} {'Duration':<12} {'Timestamp':<20}")
    print("=" * 100)
    
    for result in results:
        config = result.config
        report = result.report
        
        mode = config.get('test_config', {}).get('mode', 'unknown')
        summary = report.get('summary', {})
        pass_rate = summary.get('pass_rate', 0.0)
        duration = summary.get('duration_seconds', 0.0)
        timestamp = config.get('timestamp', '')
        
        if timestamp and 'T' in timestamp:
            from datetime import datetime
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                timestamp = dt.strftime('%Y-%m-%d %H:%M')
            except:
                timestamp = timestamp[:16]
        
        print(f"{result.result_id:<20} {mode:<10} {pass_rate*100:>6.1f}%      {duration:>8.1f}s    {timestamp:<20}")
    
    print()
    return 0


def cmd_show_result(args):
    """Show benchmark result details"""
    manager = ResultsManager()
    
    try:
        result = manager.get_result(args.result_id)
    except FileNotFoundError as e:
        logger.error(str(e))
        return 1
    
    config = result.config
    report = result.report
    
    print()
    print("=" * 80)
    print(f"ğŸ“Š Test Result / æµ‹è¯•ç»“æœ: {result.result_id}")
    print("=" * 80)
    print()
    
    test_config = config.get('test_config', {})
    print("âš™ï¸  Configuration / é…ç½®:")
    print(f"  Mode / æ¨¡å¼: {test_config.get('mode', 'unknown')}")
    print(f"  Samples / æ ·æœ¬æ•°: {config.get('benchmark_samples', 0)}")
    
    filters = test_config.get('filters', {})
    if filters.get('filter_expr'):
        print(f"  Filter / è¿‡æ»¤: {filters['filter_expr']}")
    if filters.get('schema_filter'):
        print(f"  Schema filter / Schema è¿‡æ»¤: {', '.join(filters['schema_filter'])}")
    
    print()
    
    summary = report.get('summary', {})
    print("ğŸ“ˆ Summary / æ‘˜è¦:")
    print(f"  Total / æ€»æ•°: {summary.get('total', 0)}")
    print(f"  Passed / é€šè¿‡: {summary.get('passed', 0)}")
    print(f"  Failed / å¤±è´¥: {summary.get('failed', 0)}")
    print(f"  Pass rate / é€šè¿‡ç‡: {summary.get('pass_rate', 0)*100:.1f}%")
    print(f"  Duration / è€—æ—¶: {summary.get('duration_seconds', 0):.1f}s")
    print()
    
    by_op = report.get('by_operation', {})
    if by_op:
        print("ğŸ“‹ By Operation / æŒ‰æ“ä½œ:")
        sorted_ops = sorted(by_op.items(), key=lambda x: x[1]['total'], reverse=True)
        for op, stats in sorted_ops[:10]:
            print(f"  {op:<12} {stats['passed']:>4}/{stats['total']:<4} ({stats['pass_rate']*100:>5.1f}%)")
        print()
    
    by_lang = report.get('by_language', {})
    if by_lang:
        print("ğŸŒ By Language / æŒ‰è¯­è¨€:")
        for lang, stats in sorted(by_lang.items()):
            print(f"  {lang:<6} {stats['passed']:>4}/{stats['total']:<4} ({stats['pass_rate']*100:>5.1f}%)")
        print()
    
    if args.show_failed and result.failed_file.exists():
        print("âŒ Failed Samples / å¤±è´¥æ ·æœ¬:")
        with open(result.failed_file, 'r', encoding='utf-8') as f:
            failed = [json.loads(line) for line in f if line.strip()]
        
        for item in failed[:10]:
            sample_id = item.get('sample_id', 'unknown')
            errors = item.get('errors', [])
            print(f"  {sample_id}")
            if errors:
                print(f"    {errors[0][:80]}...")
        
        if len(failed) > 10:
            print(f"  ... and {len(failed) - 10} more / è¿˜æœ‰ {len(failed) - 10} ä¸ª")
        print()
    
    print(f"ğŸ“‚ Result Directory / ç»“æœç›®å½•: bench/data/results/{result.result_id}/")
    print()
    
    return 0
def cmd_compare(args):
    """Compare two benchmark test results"""
    manager = ResultsManager()
    
    try:
        result1 = manager.get_result(args.result_id1)
        result2 = manager.get_result(args.result_id2)
    except FileNotFoundError as e:
        logger.error(str(e))
        return 1
    
    report1 = result1.report
    report2 = result2.report
    
    summary1 = report1.get('summary', {})
    summary2 = report2.get('summary', {})
    
    print()
    print("=" * 80)
    print("ğŸ“Š Result Comparison / ç»“æœå¯¹æ¯”")
    print("=" * 80)
    print()
    print(f"Left / å·¦:  {result1.result_id}")
    print(f"Right / å³: {result2.result_id}")
    print()
    
    print("ğŸ“ˆ Summary / æ‘˜è¦:")
    print(f"{'Metric / æŒ‡æ ‡':<20} {'Left / å·¦':<15} {'Right / å³':<15} {'Change / å˜åŒ–':<15}")
    print("-" * 65)
    
    total1 = summary1.get('total', 0)
    total2 = summary2.get('total', 0)
    print(f"{'Total / æ€»æ•°':<20} {total1:<15} {total2:<15} {total2 - total1:+15}")
    
    passed1 = summary1.get('passed', 0)
    passed2 = summary2.get('passed', 0)
    print(f"{'Passed / é€šè¿‡':<20} {passed1:<15} {passed2:<15} {passed2 - passed1:+15}")
    
    rate1 = summary1.get('pass_rate', 0)
    rate2 = summary2.get('pass_rate', 0)
    print(f"{'Pass Rate / é€šè¿‡ç‡':<20} {rate1*100:.1f}%{'':<11} {rate2*100:.1f}%{'':<11} {(rate2-rate1)*100:+.1f}%")
    
    dur1 = summary1.get('duration_seconds', 0)
    dur2 = summary2.get('duration_seconds', 0)
    print(f"{'Duration (s) / è€—æ—¶ (ç§’)':<20} {dur1:.1f}{'':<11} {dur2:.1f}{'':<11} {dur2-dur1:+.1f}")
    
    print()
    return 0


def cmd_info(args):
    """Show information about the current benchmark"""
    try:
        benchmark = Benchmark()
        print(benchmark.info())
        return 0
    except FileNotFoundError as e:
        logger.error(str(e))
        logger.error("Benchmark not found / Benchmark ä¸å­˜åœ¨ï¼Œè¯·å…ˆç”Ÿæˆæˆ–å¯¼å…¥ benchmark")
        return 1


def main():
    parser = argparse.ArgumentParser(
        description='Text2Mem Benchmark CLI - Full System / å®Œæ•´åŸºå‡†æµ‹è¯•ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available Commands / å¯ç”¨å‘½ä»¤')
    
    # --- run ---
    run_parser = subparsers.add_parser('run', help='Run benchmark tests / è¿è¡Œ benchmark æµ‹è¯•')
    run_parser.add_argument('--mode', default='auto',
                            choices=['auto', 'mock', 'ollama', 'openai'],
                            help='Test mode / æµ‹è¯•æ¨¡å¼')
    run_parser.add_argument('--filter', help='Sample filter, e.g. "lang:zh" / æ ·æœ¬è¿‡æ»¤ (å¦‚ "lang:zh")')
    run_parser.add_argument('--schema-filter', help='Schema filter, e.g. "Encode,Retrieve" / Schema è¿‡æ»¤')
    run_parser.add_argument('--schema-indices', help='Schema indices, e.g. "0,2" / Schema ç´¢å¼•')
    run_parser.add_argument('--timeout', type=float, help='Timeout in seconds / è¶…æ—¶ (ç§’)')
    run_parser.add_argument('--output-id', help='Result ID (default: timestamp) / ç»“æœ ID (é»˜è®¤: æ—¶é—´æˆ³)')
    run_parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output / è¯¦ç»†è¾“å‡º')
    
    # --- generate ---
    gen_parser = subparsers.add_parser('generate', help='Generate new benchmark candidate / ç”Ÿæˆæ–°çš„ benchmark å€™é€‰æ•°æ®')
    gen_parser.add_argument('--config', help='Path to config file / é…ç½®æ–‡ä»¶è·¯å¾„')
    gen_parser.add_argument('--output-id', help='Output ID (default: timestamp) / è¾“å‡º ID (é»˜è®¤: æ—¶é—´æˆ³)')
    gen_parser.add_argument('--use-generation-dir', action='store_true',
                            help='Use generation/ directory instead of raw/ / ä½¿ç”¨ generation/ ç›®å½•è€Œä¸æ˜¯ raw/')
    
    # --- validate ---
    val_parser = subparsers.add_parser('validate', help='Validate generated data quality / éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡')
    val_parser.add_argument('generation_id', help='Generation ID / ç”ŸæˆID')
    val_parser.add_argument('--run-tests', action='store_true', help='Run validation tests / è¿è¡Œæµ‹è¯•éªŒè¯')
    val_parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output / è¯¦ç»†è¾“å‡º')
    
    # --- promote ---
    pro_parser = subparsers.add_parser('promote', help='Promote candidate to benchmark / æå‡ä¸ºæ­£å¼ benchmark')
    pro_parser.add_argument('generation_id', help='Generation ID / ç”ŸæˆID')
    pro_parser.add_argument('--yes', '-y', action='store_true', help='Skip confirmation / è·³è¿‡ç¡®è®¤')
    pro_parser.add_argument('--notes', help='Notes / å¤‡æ³¨')
    
    # --- list-results ---
    list_parser = subparsers.add_parser('list-results', help='List benchmark test results / åˆ—å‡ºæµ‹è¯•ç»“æœ')
    list_parser.add_argument('--limit', type=int, default=20, help='Limit number / é™åˆ¶æ•°é‡')
    
    # --- show-result ---
    show_parser = subparsers.add_parser('show-result', help='Show result details / æ˜¾ç¤ºç»“æœè¯¦æƒ…')
    show_parser.add_argument('result_id', help='Result ID (or "latest") / ç»“æœ ID (æˆ– "latest")')
    show_parser.add_argument('--show-failed', action='store_true', help='Show failed samples / æ˜¾ç¤ºå¤±è´¥æ ·æœ¬')
    
    # --- compare ---
    cmp_parser = subparsers.add_parser('compare', help='Compare two results / å¯¹æ¯”ä¸¤ä¸ªç»“æœ')
    cmp_parser.add_argument('result_id1', help='Result ID 1 / ç»“æœ ID 1')
    cmp_parser.add_argument('result_id2', help='Result ID 2 / ç»“æœ ID 2')
    
    # --- info ---
    info_parser = subparsers.add_parser('info', help='Show benchmark information / æ˜¾ç¤º benchmark ä¿¡æ¯')
    
    # Parse CLI arguments
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    commands = {
        'run': cmd_run,
        'generate': cmd_generate,
        'validate': cmd_validate,
        'promote': cmd_promote,
        'list-results': cmd_list_results,
        'show-result': cmd_show_result,
        'compare': cmd_compare,
        'info': cmd_info,
    }
    
    # Execute selected command
    return commands[args.command](args)


if __name__ == '__main__':
    sys.exit(main())
