#!/usr/bin/env python3
"""
Complete Bench CLI - å®Œæ•´çš„ Benchmark ç³»ç»Ÿ

åŠŸèƒ½:
1. run          - è¿è¡Œæµ‹è¯• (ä½¿ç”¨ç°æœ‰ benchmark)
2. generate     - ç”Ÿæˆæ–° benchmark å€™é€‰æ•°æ®
3. validate     - éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡
4. promote      - æå‡ä¸ºæ­£å¼ benchmark
5. list-results - åˆ—å‡ºæµ‹è¯•ç»“æœ
6. show-result  - æ˜¾ç¤ºç»“æœè¯¦æƒ…
7. compare      - å¯¹æ¯”ä¸¤ä¸ªç»“æœ
8. info         - æ˜¾ç¤º benchmark ä¿¡æ¯
"""
import argparse
import json
import logging
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from bench.core.simple_manager import Benchmark, ResultsManager
from bench.core.simple_runner import SimpleTestRunner

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def cmd_run(args):
    """è¿è¡Œ benchmark æµ‹è¯•"""
    schema_filter = None
    if args.schema_filter:
        schema_filter = [s.strip() for s in args.schema_filter.split(',')]
    
    schema_indices = None
    if args.schema_indices:
        schema_indices = [int(i.strip()) for i in args.schema_indices.split(',')]
    
    runner = SimpleTestRunner(
        mode=args.mode,
        filter_expr=args.filter,
        schema_filter=schema_filter,
        schema_indices=schema_indices,
        timeout=args.timeout,
    )
    
    try:
        result = runner.run(
            result_id=args.output_id,
            verbose=args.verbose
        )
        return 0
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


def cmd_generate(args):
    """ç”Ÿæˆæ–°çš„ benchmark å€™é€‰æ•°æ®"""
    print("=" * 80)
    print("ğŸ”„ ç”Ÿæˆ Benchmark å€™é€‰æ•°æ®")
    print("=" * 80)
    print()
    
    import subprocess
    from datetime import datetime
    
    gen_id = args.output_id or datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    if args.use_generation_dir:
        output_dir = Path(f'bench/data/generation/{gen_id}')
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    else:
        output_dir = None
        print(f"ğŸ“‚ è¾“å‡ºåˆ°é»˜è®¤ä½ç½® (bench/data/raw/)")
    
    print()
    
    # è¿è¡Œç”Ÿæˆè„šæœ¬
    cmd = [sys.executable, 'bench/generate/generate.py']
    
    if args.config:
        cmd.extend(['--config', args.config])
    
    print(f"ğŸƒ è¿è¡Œç”Ÿæˆ...")
    print(f"   å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    result = subprocess.run(cmd, cwd=Path.cwd())
    
    if result.returncode == 0:
        print()
        print("âœ… ç”Ÿæˆå®Œæˆï¼")
        print()
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„æ•°æ®
        raw_dir = Path('bench/data/raw')
        if raw_dir.exists():
            gen_dirs = sorted([d for d in raw_dir.iterdir() if d.is_dir()], 
                            key=lambda x: x.name, reverse=True)
            if gen_dirs:
                latest_gen = gen_dirs[0].name
                print("ä¸‹ä¸€æ­¥:")
                print(f"  1. éªŒè¯è´¨é‡: ./bench-cli validate {latest_gen}")
                print(f"  2. æå‡ä¸º benchmark: ./bench-cli promote {latest_gen}")
    else:
        print()
        print("âŒ ç”Ÿæˆå¤±è´¥")
    
    return result.returncode


def cmd_validate(args):
    """éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡"""
    gen_id = args.generation_id
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„æ•°æ®
    possible_paths = [
        Path(f'bench/data/generation/{gen_id}'),
        Path(f'bench/data/raw/{gen_id}'),
    ]
    
    gen_dir = None
    for p in possible_paths:
        if p.exists():
            gen_dir = p
            break
    
    if not gen_dir:
        logger.error(f"Generation not found: {gen_id}")
        logger.error(f"Searched in: {', '.join(str(p) for p in possible_paths)}")
        return 1
    
    print("=" * 80)
    print(f"ğŸ” éªŒè¯æ•°æ®: {gen_id}")
    print("=" * 80)
    print(f"ğŸ“‚ ä½ç½®: {gen_dir}")
    print()
    
    # æŸ¥æ‰¾ stage3.jsonl
    stage3_file = gen_dir / 'stage3.jsonl'
    if not stage3_file.exists():
        logger.error(f"stage3.jsonl not found in {gen_dir}")
        return 1
    
    # ç»Ÿè®¡æ ·æœ¬æ•°
    samples = []
    with open(stage3_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except:
                    pass
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(samples)}")
    print()
    
    # å¿«é€Ÿç»Ÿè®¡
    from collections import Counter
    lang_counter = Counter()
    op_counter = Counter()
    
    for sample in samples:
        class_info = sample.get('class', {})
        lang_counter[class_info.get('lang', 'unknown')] += 1
        
        for schema in sample.get('schema_list', []):
            op_counter[schema.get('op', 'unknown')] += 1
    
    print("ğŸ“ˆ æ•°æ®åˆ†å¸ƒ:")
    print(f"   è¯­è¨€: {dict(lang_counter)}")
    print(f"   æ“ä½œ (Top 5): {dict(op_counter.most_common(5))}")
    print()
    
    # è¿è¡Œæµ‹è¯•éªŒè¯
    if args.run_tests:
        print("ğŸ§ª è¿è¡Œæµ‹è¯•éªŒè¯...")
        print()
        
        # ä½¿ç”¨ mock æ¨¡å¼å¿«é€ŸéªŒè¯
        import tempfile
        import shutil
        
        with tempfile.TemporaryDirectory() as tmpdir:
            # åˆ›å»ºä¸´æ—¶ benchmark
            tmp_benchmark_dir = Path(tmpdir) / 'benchmark'
            tmp_benchmark_dir.mkdir()
            
            shutil.copy(stage3_file, tmp_benchmark_dir / 'benchmark.jsonl')
            
            metadata = {
                'total_samples': len(samples),
                'created_at': 'validation',
            }
            with open(tmp_benchmark_dir / 'metadata.json', 'w') as f:
                json.dump(metadata, f)
            
            # ä¸´æ—¶è¿è¡Œæµ‹è¯•
            from bench.core.simple_manager import Benchmark
            runner = SimpleTestRunner(mode='mock')
            
            original_dir = runner.benchmark.benchmark_dir
            runner.benchmark.benchmark_dir = tmp_benchmark_dir
            runner.benchmark.benchmark_file = tmp_benchmark_dir / 'benchmark.jsonl'
            runner.benchmark.metadata_file = tmp_benchmark_dir / 'metadata.json'
            
            try:
                result = runner.run(verbose=args.verbose)
                
                pass_rate = result.report['summary']['pass_rate']
                print()
                print(f"ğŸ“Š æµ‹è¯•é€šè¿‡ç‡: {pass_rate*100:.1f}%")
                print()
                
                if pass_rate >= 0.5:
                    print("ğŸ’¡ è´¨é‡è¯„ä¼°: è‰¯å¥½ï¼Œå¯ä»¥æå‡ä¸ºæ­£å¼ benchmark")
                    print()
                    print("ä¸‹ä¸€æ­¥:")
                    print(f"  ./bench-cli promote {gen_id}")
                else:
                    print("âš ï¸  è´¨é‡è¯„ä¼°: é€šè¿‡ç‡åä½ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")
                
            finally:
                runner.benchmark.benchmark_dir = original_dir
    else:
        print("ğŸ’¡ æç¤º: æ·»åŠ  --run-tests å‚æ•°å¯ä»¥è¿è¡Œå®Œæ•´æµ‹è¯•éªŒè¯")
        print()
        print("ä¸‹ä¸€æ­¥:")
        print(f"  ./bench-cli validate {gen_id} --run-tests  # è¿è¡Œæµ‹è¯•")
        print(f"  ./bench-cli promote {gen_id}               # æå‡ä¸º benchmark")
    
    return 0


def cmd_promote(args):
    """å°†ç”Ÿæˆçš„æ•°æ®æå‡ä¸ºæ­£å¼ benchmark"""
    gen_id = args.generation_id
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„æ•°æ®
    possible_paths = [
        Path(f'bench/data/generation/{gen_id}'),
        Path(f'bench/data/raw/{gen_id}'),
    ]
    
    gen_dir = None
    for p in possible_paths:
        if p.exists():
            gen_dir = p
            break
    
    if not gen_dir:
        logger.error(f"Generation not found: {gen_id}")
        return 1
    
    stage3_file = gen_dir / 'stage3.jsonl'
    if not stage3_file.exists():
        logger.error(f"stage3.jsonl not found in {gen_dir}")
        return 1
    
    print("=" * 80)
    print(f"â¬†ï¸  æå‡ä¸ºæ­£å¼ Benchmark: {gen_id}")
    print("=" * 80)
    print()
    
    # ç¡®è®¤
    if not args.yes:
        print("âš ï¸  è¿™å°†æ›¿æ¢å½“å‰çš„ benchmarkï¼")
        
        try:
            benchmark = Benchmark()
            print(f"   å½“å‰ benchmark: {benchmark.sample_count} samples")
        except:
            print(f"   å½“å‰ benchmark: ä¸å­˜åœ¨")
        
        with open(stage3_file, 'r') as f:
            new_count = sum(1 for _ in f if _.strip())
        print(f"   æ–° benchmark: {new_count} samples")
        print()
        
        confirm = input("ç¡®è®¤æ›¿æ¢? (yes/no): ")
        if confirm.lower() != 'yes':
            print("å·²å–æ¶ˆ")
            return 0
    
    # å¤‡ä»½å½“å‰ benchmark
    import shutil
    from datetime import datetime
    
    benchmark_dir = Path('bench/data/benchmark')
    backup_dir = Path(f'bench/data/archive/benchmark_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    backup_dir.parent.mkdir(parents=True, exist_ok=True)
    
    if benchmark_dir.exists():
        shutil.copytree(benchmark_dir, backup_dir)
        print(f"âœ“ å·²å¤‡ä»½å½“å‰ benchmark åˆ°: {backup_dir}")
        print()
    
    # è¯»å–å¹¶è¿‡æ»¤æ ·æœ¬
    print("ğŸ”„ å¤„ç†æ•°æ®...")
    
    samples = []
    with open(stage3_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except:
                    pass
    
    print(f"   åŸå§‹æ ·æœ¬: {len(samples)}")
    
    # è¿‡æ»¤æ— æ•ˆæ•°æ®
    ALLOWED_OPERATIONS = {
        'Encode', 'Retrieve', 'Update', 'Delete', 'Summarize', 'Label',
        'Promote', 'Demote', 'Expire', 'Lock', 'Merge', 'Split',
    }
    
    filtered_samples = []
    for sample in samples:
        schema_list = sample.get('schema_list', [])
        if not schema_list:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å« unknown
        sample_str = json.dumps(sample)
        if 'unknown' in sample_str.lower():
            continue
        
        # æ£€æŸ¥æ“ä½œæœ‰æ•ˆæ€§
        valid = True
        for schema in schema_list:
            if schema.get('op') not in ALLOWED_OPERATIONS:
                valid = False
                break
        
        if valid:
            filtered_samples.append(sample)
    
    print(f"   è¿‡æ»¤å: {len(filtered_samples)}")
    print()
    
    # ä¿å­˜åˆ° benchmark
    benchmark_dir.mkdir(parents=True, exist_ok=True)
    
    with open(benchmark_dir / 'benchmark.jsonl', 'w', encoding='utf-8') as f:
        for sample in filtered_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    # ç”Ÿæˆç»Ÿè®¡
    from collections import Counter
    
    lang_counter = Counter()
    op_counter = Counter()
    structure_counter = Counter()
    
    for sample in filtered_samples:
        class_info = sample.get('class', {})
        lang_counter[class_info.get('lang', 'unknown')] += 1
        structure_counter[class_info.get('structure', 'unknown')] += 1
        
        for schema in sample.get('schema_list', []):
            op_counter[schema.get('op', 'unknown')] += 1
    
    stats = {
        'total': len(filtered_samples),
        'distribution': {
            'languages': dict(lang_counter),
            'operations': dict(op_counter),
            'structures': dict(structure_counter),
        }
    }
    
    with open(benchmark_dir / 'stats.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜ metadata
    metadata = {
        'total_samples': len(filtered_samples),
        'created_at': datetime.now().isoformat(),
        'last_updated': datetime.now().isoformat(),
        'source': f'{gen_dir.parent.name}/{gen_id}',
        'notes': args.notes or f'Promoted from {gen_id}'
    }
    
    with open(benchmark_dir / 'metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print("=" * 80)
    print("âœ… Benchmark å·²æ›´æ–°ï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“Š æ–° Benchmark:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(filtered_samples)}")
    print(f"   è¯­è¨€åˆ†å¸ƒ: {dict(lang_counter)}")
    print(f"   æ“ä½œåˆ†å¸ƒ (Top 5): {dict(op_counter.most_common(5))}")
    print()
    print("ä¸‹ä¸€æ­¥:")
    print("  ./bench-cli run --mode ollama -v  # è¿è¡Œå®Œæ•´æµ‹è¯•")
    print()
    
    return 0


def cmd_list_results(args):
    """åˆ—å‡ºæµ‹è¯•ç»“æœ"""
    manager = ResultsManager()
    results = manager.list_results(limit=args.limit)
    
    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç»“æœ")
        return 0
    
    print()
    print("=" * 100)
    print(f"{'ID':<20} {'Mode':<10} {'Pass Rate':<12} {'Duration':<12} {'Timestamp':<20}")
    print("=" * 100)
    
    for result in results:
        config = result.config
        report = result.report
        
        mode = config.get('test_config', {}).get('mode', 'unknown')
        summary = report.get('summary', {})
        pass_rate = summary.get('pass_rate', 0.0)
        duration = summary.get('duration_seconds', 0.0)
        timestamp = config.get('timestamp', '')
        
        if timestamp and 'T' in timestamp:
            from datetime import datetime
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                timestamp = dt.strftime('%Y-%m-%d %H:%M')
            except:
                timestamp = timestamp[:16]
        
        print(f"{result.result_id:<20} {mode:<10} {pass_rate*100:>6.1f}%      {duration:>8.1f}s    {timestamp:<20}")
    
    print()
    return 0


def cmd_show_result(args):
    """æ˜¾ç¤ºæµ‹è¯•ç»“æœè¯¦æƒ…"""
    manager = ResultsManager()
    
    try:
        result = manager.get_result(args.result_id)
    except FileNotFoundError as e:
        logger.error(str(e))
        return 1
    
    config = result.config
    report = result.report
    
    print()
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {result.result_id}")
    print("=" * 80)
    print()
    
    test_config = config.get('test_config', {})
    print("âš™ï¸  é…ç½®:")
    print(f"  æ¨¡å¼: {test_config.get('mode', 'unknown')}")
    print(f"  æ ·æœ¬æ•°: {config.get('benchmark_samples', 0)}")
    
    filters = test_config.get('filters', {})
    if filters.get('filter_expr'):
        print(f"  è¿‡æ»¤: {filters['filter_expr']}")
    if filters.get('schema_filter'):
        print(f"  Schema è¿‡æ»¤: {', '.join(filters['schema_filter'])}")
    
    print()
    
    summary = report.get('summary', {})
    print("ğŸ“ˆ æ‘˜è¦:")
    print(f"  æ€»æ•°: {summary.get('total', 0)}")
    print(f"  é€šè¿‡: {summary.get('passed', 0)}")
    print(f"  å¤±è´¥: {summary.get('failed', 0)}")
    print(f"  é€šè¿‡ç‡: {summary.get('pass_rate', 0)*100:.1f}%")
    print(f"  è€—æ—¶: {summary.get('duration_seconds', 0):.1f}s")
    print()
    
    by_op = report.get('by_operation', {})
    if by_op:
        print("ğŸ“‹ æŒ‰æ“ä½œ:")
        sorted_ops = sorted(by_op.items(), key=lambda x: x[1]['total'], reverse=True)
        for op, stats in sorted_ops[:10]:
            print(f"  {op:<12} {stats['passed']:>4}/{stats['total']:<4} ({stats['pass_rate']*100:>5.1f}%)")
        print()
    
    by_lang = report.get('by_language', {})
    if by_lang:
        print("ğŸŒ æŒ‰è¯­è¨€:")
        for lang, stats in sorted(by_lang.items()):
            print(f"  {lang:<6} {stats['passed']:>4}/{stats['total']:<4} ({stats['pass_rate']*100:>5.1f}%)")
        print()
    
    if args.show_failed and result.failed_file.exists():
        print("âŒ å¤±è´¥æ ·æœ¬:")
        with open(result.failed_file, 'r', encoding='utf-8') as f:
            failed = [json.loads(line) for line in f if line.strip()]
        
        for item in failed[:10]:
            sample_id = item.get('sample_id', 'unknown')
            errors = item.get('errors', [])
            print(f"  {sample_id}")
            if errors:
                print(f"    {errors[0][:80]}...")
        
        if len(failed) > 10:
            print(f"  ... è¿˜æœ‰ {len(failed) - 10} ä¸ª")
        print()
    
    print(f"ğŸ“‚ ç»“æœç›®å½•: bench/data/results/{result.result_id}/")
    print()
    
    return 0


def cmd_compare(args):
    """å¯¹æ¯”ä¸¤ä¸ªæµ‹è¯•ç»“æœ"""
    manager = ResultsManager()
    
    try:
        result1 = manager.get_result(args.result_id1)
        result2 = manager.get_result(args.result_id2)
    except FileNotFoundError as e:
        logger.error(str(e))
        return 1
    
    report1 = result1.report
    report2 = result2.report
    
    summary1 = report1.get('summary', {})
    summary2 = report2.get('summary', {})
    
    print()
    print("=" * 80)
    print("ğŸ“Š ç»“æœå¯¹æ¯”")
    print("=" * 80)
    print()
    print(f"å·¦:  {result1.result_id}")
    print(f"å³: {result2.result_id}")
    print()
    
    print("ğŸ“ˆ æ‘˜è¦:")
    print(f"{'æŒ‡æ ‡':<20} {'å·¦':<15} {'å³':<15} {'å˜åŒ–':<15}")
    print("-" * 65)
    
    total1 = summary1.get('total', 0)
    total2 = summary2.get('total', 0)
    print(f"{'æ€»æ•°':<20} {total1:<15} {total2:<15} {total2 - total1:+15}")
    
    passed1 = summary1.get('passed', 0)
    passed2 = summary2.get('passed', 0)
    print(f"{'é€šè¿‡':<20} {passed1:<15} {passed2:<15} {passed2 - passed1:+15}")
    
    rate1 = summary1.get('pass_rate', 0)
    rate2 = summary2.get('pass_rate', 0)
    print(f"{'é€šè¿‡ç‡':<20} {rate1*100:.1f}%{'':<11} {rate2*100:.1f}%{'':<11} {(rate2-rate1)*100:+.1f}%")
    
    dur1 = summary1.get('duration_seconds', 0)
    dur2 = summary2.get('duration_seconds', 0)
    print(f"{'è€—æ—¶ (s)':<20} {dur1:.1f}{'':<11} {dur2:.1f}{'':<11} {dur2-dur1:+.1f}")
    
    print()
    return 0


def cmd_info(args):
    """æ˜¾ç¤º benchmark ä¿¡æ¯"""
    try:
        benchmark = Benchmark()
        print(benchmark.info())
        return 0
    except FileNotFoundError as e:
        logger.error(str(e))
        logger.error("Benchmark ä¸å­˜åœ¨ï¼Œè¯·å…ˆç”Ÿæˆæˆ–å¯¼å…¥ benchmark")
        return 1


def main():
    parser = argparse.ArgumentParser(
        description='Text2Mem Benchmark CLI - å®Œæ•´ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # run
    run_parser = subparsers.add_parser('run', help='è¿è¡Œ benchmark æµ‹è¯•')
    run_parser.add_argument('--mode', default='auto',
                           choices=['auto', 'mock', 'ollama', 'openai'],
                           help='æµ‹è¯•æ¨¡å¼')
    run_parser.add_argument('--filter', help='æ ·æœ¬è¿‡æ»¤ (å¦‚ "lang:zh")')
    run_parser.add_argument('--schema-filter', help='Schema è¿‡æ»¤ (å¦‚ "Encode,Retrieve")')
    run_parser.add_argument('--schema-indices', help='Schema ç´¢å¼• (å¦‚ "0,2")')
    run_parser.add_argument('--timeout', type=float, help='è¶…æ—¶ (ç§’)')
    run_parser.add_argument('--output-id', help='ç»“æœ ID (é»˜è®¤: æ—¶é—´æˆ³)')
    run_parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    # generate
    gen_parser = subparsers.add_parser('generate', help='ç”Ÿæˆæ–°çš„ benchmark å€™é€‰æ•°æ®')
    gen_parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    gen_parser.add_argument('--output-id', help='è¾“å‡º ID (é»˜è®¤: æ—¶é—´æˆ³)')
    gen_parser.add_argument('--use-generation-dir', action='store_true', 
                           help='ä½¿ç”¨ generation/ ç›®å½•è€Œä¸æ˜¯ raw/')
    
    # validate
    val_parser = subparsers.add_parser('validate', help='éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡')
    val_parser.add_argument('generation_id', help='Generation ID')
    val_parser.add_argument('--run-tests', action='store_true', help='è¿è¡Œæµ‹è¯•éªŒè¯')
    val_parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    # promote
    pro_parser = subparsers.add_parser('promote', help='æå‡ä¸ºæ­£å¼ benchmark')
    pro_parser.add_argument('generation_id', help='Generation ID')
    pro_parser.add_argument('--yes', '-y', action='store_true', help='è·³è¿‡ç¡®è®¤')
    pro_parser.add_argument('--notes', help='å¤‡æ³¨')
    
    # list-results
    list_parser = subparsers.add_parser('list-results', help='åˆ—å‡ºæµ‹è¯•ç»“æœ')
    list_parser.add_argument('--limit', type=int, default=20, help='é™åˆ¶æ•°é‡')
    
    # show-result
    show_parser = subparsers.add_parser('show-result', help='æ˜¾ç¤ºç»“æœè¯¦æƒ…')
    show_parser.add_argument('result_id', help='ç»“æœ ID (æˆ– "latest")')
    show_parser.add_argument('--show-failed', action='store_true', help='æ˜¾ç¤ºå¤±è´¥æ ·æœ¬')
    
    # compare
    cmp_parser = subparsers.add_parser('compare', help='å¯¹æ¯”ä¸¤ä¸ªç»“æœ')
    cmp_parser.add_argument('result_id1', help='ç»“æœ ID 1')
    cmp_parser.add_argument('result_id2', help='ç»“æœ ID 2')
    
    # info
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤º benchmark ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    commands = {
        'run': cmd_run,
        'generate': cmd_generate,
        'validate': cmd_validate,
        'promote': cmd_promote,
        'list-results': cmd_list_results,
        'show-result': cmd_show_result,
        'compare': cmd_compare,
        'info': cmd_info,
    }
    
    return commands[args.command](args)


if __name__ == '__main__':
    sys.exit(main())
